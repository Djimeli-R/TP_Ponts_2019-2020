{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **<center>Deep Convolutional Generative Adversarial Network (DCGAN) </center>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DCGAN is a Generative Adversarial Network (GAN) using CNN.\n",
    "\n",
    "* The generator tries to fool the discriminator by generating fake images.\n",
    "\n",
    "* The discriminator learns to discriminate real from fake images.\n",
    "\n",
    "* The generator + discriminator form an adversarial network.\n",
    "\n",
    "* DCGAN trains the discriminator and adversarial networks alternately.\n",
    "\n",
    "* During training, not only the discriminator learns to distinguish real from\n",
    "fake images, it also coaches the generator part of the adversarial on how\n",
    "to improve its ability to generate fake images.\n",
    "\n",
    "Paper of reference: [1] Radford, Alec, Luke Metz, and Soumith Chintala.\n",
    "\"Unsupervised representation learning with deep convolutional\n",
    "generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(inputs, image_size):\n",
    "    \"\"\"Build a Generator Model\n",
    "    Stack of BN-ReLU-Conv2DTranpose to generate fake images\n",
    "    Output activation is sigmoid instead of tanh in [1].\n",
    "    Sigmoid converges easily.\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the generator (the z-vector)\n",
    "        image_size: Target size of one side (assuming square image)\n",
    "    # Returns\n",
    "        Model: Generator Model\n",
    "    \"\"\"\n",
    "\n",
    "    image_resize = image_size // 4\n",
    "    # network parameters \n",
    "    kernel_size = 5\n",
    "    layer_filters = [128, 64, 32, 1]\n",
    "\n",
    "    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n",
    "    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n",
    "\n",
    "    for filters in layer_filters:\n",
    "        # first two convolution layers use strides = 2\n",
    "        # the last two use strides = 1\n",
    "        if filters > layer_filters[-2]:\n",
    "            strides = 2\n",
    "        else:\n",
    "            strides = 1\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            strides=strides,\n",
    "                            padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    generator = Model(inputs, x, name='generator')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(inputs):\n",
    "    \"\"\"Build a Discriminator Model\n",
    "    Stack of LeakyReLU-Conv2D to discriminate real from fake.\n",
    "    The network does not converge with BN so it is not used here\n",
    "    unlike in [1] or original paper.\n",
    "    # Arguments\n",
    "        inputs (Layer): Input layer of the discriminator (the image)\n",
    "    # Returns\n",
    "        Model: Discriminator Model\n",
    "    \"\"\"\n",
    "    kernel_size = 5\n",
    "    layer_filters = [32, 64, 128, 256]\n",
    "\n",
    "    x = inputs\n",
    "    for filters in layer_filters:\n",
    "        # first 3 convolution layers use strides = 2\n",
    "        # last one uses strides = 1\n",
    "        if filters == layer_filters[-1]:\n",
    "            strides = 1\n",
    "        else:\n",
    "            strides = 2\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    discriminator = Model(inputs, x, name='discriminator')\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"train.png\" alt=\"Drawing\" style=\"width: 800;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, x_train, params):\n",
    "    \"\"\"Train the Discriminator and Adversarial Networks\n",
    "    Alternately train Discriminator and Adversarial networks by batch.\n",
    "    Discriminator is trained first with properly real and fake images.\n",
    "    Adversarial is trained next with fake images pretending to be real\n",
    "    Generate sample images per save_interval.\n",
    "    # Arguments\n",
    "        models (list): Generator, Discriminator, Adversarial models\n",
    "        x_train (tensor): Train images\n",
    "        params (list) : Networks parameters\n",
    "    \"\"\"\n",
    "    # the GAN models\n",
    "    generator, discriminator, adversarial = models\n",
    "    # network parameters\n",
    "    batch_size, latent_size, train_steps, model_name = params\n",
    "    # noise vector to see how the generator output evolves during training\n",
    "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size])\n",
    "    # number of elements in train dataset\n",
    "    train_size = x_train.shape[0]\n",
    "    # save generated images after 500 steps\n",
    "    save_interval = 500\n",
    "    list_images = []\n",
    "    list_discriminator_loss = []\n",
    "    list_adversarial_loss = []\n",
    "    for i in range(train_steps):\n",
    "        # train the discriminator for 1 batch\n",
    "        # 1 batch of real (label=1.0) and fake images (label=0.0)\n",
    "        # randomly pick real images from dataset\n",
    "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
    "        real_images = x_train[rand_indexes]\n",
    "        # generate fake images from noise using generator \n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # generate fake images\n",
    "        fake_images = generator.predict(noise)\n",
    "        # real + fake images = 1 batch of train data\n",
    "        x = np.concatenate((real_images, fake_images))\n",
    "        # label real and fake images\n",
    "        # real images label is 1.0\n",
    "        y = np.ones([2 * batch_size, 1])\n",
    "        # fake images label is 0.0\n",
    "        y[batch_size:, :] = 0.0\n",
    "        # train discriminator network, log the loss and accuracy\n",
    "        loss_disc, acc = discriminator.train_on_batch(x, y)\n",
    "        list_discriminator_loss.append(loss_disc)\n",
    "        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss_disc, acc)\n",
    "\n",
    "        # train the adversarial network for 1 batch\n",
    "        # 1 batch of fake images with label=1.0\n",
    "        # since the discriminator weights are frozen in adversarial network\n",
    "        # only the generator is trained\n",
    "        # generate noise using uniform distribution\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
    "        # label fake images as real or 1.0\n",
    "        y = np.ones([batch_size, 1])\n",
    "        # train the adversarial network \n",
    "        # note that unlike in discriminator training, \n",
    "        # we do not save the fake images in a variable\n",
    "        # the fake images go to the discriminator input of the adversarial\n",
    "        # for classification\n",
    "        # log the loss and accuracy\n",
    "        loss_adv, acc = adversarial.train_on_batch(noise, y)\n",
    "        list_adversarial_loss.append(loss_adv)\n",
    "        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss_adv, acc)\n",
    "        print(log)\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "            images = generator.predict(noise_input)\n",
    "            list_images.append(images)\n",
    "            \n",
    "    # save the model after training the generator\n",
    "    # the trained generator can be reloaded for future MNIST digit generation        \n",
    "    generator.save(model_name + \".h5\")\n",
    " \n",
    "    return generator, list_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to plot generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    \"\"\"Generate fake images and plot them\n",
    "    For visualization purposes, generate fake images\n",
    "    then plot them in a square grid\n",
    "    # Arguments\n",
    "        images : 16 images generated at different steps of learning\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(3.5, 3.5))\n",
    "    num_images = images.shape[0]\n",
    "    image_size = images.shape[1]\n",
    "    rows = int(math.sqrt(noise_input.shape[0]))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(rows, rows, i + 1)\n",
    "        image = np.reshape(images[i], [image_size, image_size])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "(x_train, _), (_, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CNN as (28, 28, 1) and normalize\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dcgan_mnist\"\n",
    "# network parameters\n",
    "# the latent or z vector is 100-dim\n",
    "latent_size = 100\n",
    "batch_size = 64\n",
    "train_steps = 3000\n",
    "lr = 2e-4\n",
    "decay = 6e-8\n",
    "input_shape = (image_size, image_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=input_shape, name='discriminator_input')\n",
    "discriminator = build_discriminator(inputs)\n",
    "# [1] or original paper uses Adam, \n",
    "# but discriminator converges easily with RMSprop\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (latent_size, )\n",
    "inputs = Input(shape=input_shape, name='z_input')\n",
    "generator = build_generator(inputs, image_size)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the adversarial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dcgan_mnist\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 28, 28, 1)         1301505   \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 1)                 1080577   \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=lr * 0.5, decay=decay * 0.5)\n",
    "# freeze the weights of discriminator during adversarial training\n",
    "discriminator.trainable = False\n",
    "# adversarial = generator + discriminator\n",
    "adversarial = Model(inputs, \n",
    "                    discriminator(generator(inputs)),\n",
    "                    name=model_name)\n",
    "adversarial.compile(loss='binary_crossentropy',\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "adversarial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train discriminator and adversarial networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 4097      \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,080,577\n",
      "Trainable params: 1,080,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DT (None, 14, 14, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,301,505\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Model: \"dcgan_mnist\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_input (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 28, 28, 1)         1301505   \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 1)                 1080577   \n",
      "=================================================================\n",
      "Total params: 2,382,082\n",
      "Trainable params: 1,300,801\n",
      "Non-trainable params: 1,081,281\n",
      "_________________________________________________________________\n",
      "0: [discriminator loss: 0.691088, acc: 0.820312] [adversarial loss: 0.805298, acc: 0.000000]\n",
      "1: [discriminator loss: 0.608377, acc: 0.898438] [adversarial loss: 0.866776, acc: 0.000000]\n",
      "2: [discriminator loss: 0.480980, acc: 1.000000] [adversarial loss: 0.948807, acc: 0.000000]\n",
      "3: [discriminator loss: 0.333145, acc: 0.984375] [adversarial loss: 0.557108, acc: 1.000000]\n",
      "4: [discriminator loss: 0.214187, acc: 1.000000] [adversarial loss: 1.331007, acc: 0.000000]\n",
      "5: [discriminator loss: 0.244946, acc: 0.921875] [adversarial loss: 0.234694, acc: 1.000000]\n",
      "6: [discriminator loss: 0.100823, acc: 1.000000] [adversarial loss: 0.389000, acc: 1.000000]\n",
      "7: [discriminator loss: 0.060791, acc: 1.000000] [adversarial loss: 0.181457, acc: 1.000000]\n",
      "8: [discriminator loss: 0.037187, acc: 1.000000] [adversarial loss: 0.131410, acc: 1.000000]\n",
      "9: [discriminator loss: 0.029063, acc: 1.000000] [adversarial loss: 0.088673, acc: 1.000000]\n",
      "10: [discriminator loss: 0.027529, acc: 1.000000] [adversarial loss: 0.046633, acc: 1.000000]\n",
      "11: [discriminator loss: 0.017757, acc: 1.000000] [adversarial loss: 0.053608, acc: 1.000000]\n",
      "12: [discriminator loss: 0.014269, acc: 1.000000] [adversarial loss: 0.035820, acc: 1.000000]\n",
      "13: [discriminator loss: 0.010747, acc: 1.000000] [adversarial loss: 0.027175, acc: 1.000000]\n",
      "14: [discriminator loss: 0.011668, acc: 1.000000] [adversarial loss: 0.016856, acc: 1.000000]\n",
      "15: [discriminator loss: 0.009309, acc: 1.000000] [adversarial loss: 0.015048, acc: 1.000000]\n",
      "16: [discriminator loss: 0.006471, acc: 1.000000] [adversarial loss: 0.011651, acc: 1.000000]\n",
      "17: [discriminator loss: 0.006300, acc: 1.000000] [adversarial loss: 0.009122, acc: 1.000000]\n",
      "18: [discriminator loss: 0.005347, acc: 1.000000] [adversarial loss: 0.007326, acc: 1.000000]\n",
      "19: [discriminator loss: 0.004620, acc: 1.000000] [adversarial loss: 0.005918, acc: 1.000000]\n",
      "20: [discriminator loss: 0.003991, acc: 1.000000] [adversarial loss: 0.005241, acc: 1.000000]\n",
      "21: [discriminator loss: 0.002734, acc: 1.000000] [adversarial loss: 0.004994, acc: 1.000000]\n",
      "22: [discriminator loss: 0.003141, acc: 1.000000] [adversarial loss: 0.003660, acc: 1.000000]\n",
      "23: [discriminator loss: 0.002672, acc: 1.000000] [adversarial loss: 0.003564, acc: 1.000000]\n",
      "24: [discriminator loss: 0.002631, acc: 1.000000] [adversarial loss: 0.002829, acc: 1.000000]\n",
      "25: [discriminator loss: 0.002195, acc: 1.000000] [adversarial loss: 0.002403, acc: 1.000000]\n",
      "26: [discriminator loss: 0.002175, acc: 1.000000] [adversarial loss: 0.002094, acc: 1.000000]\n",
      "27: [discriminator loss: 0.002341, acc: 1.000000] [adversarial loss: 0.001333, acc: 1.000000]\n",
      "28: [discriminator loss: 0.002377, acc: 1.000000] [adversarial loss: 0.001058, acc: 1.000000]\n",
      "29: [discriminator loss: 0.002477, acc: 1.000000] [adversarial loss: 0.000558, acc: 1.000000]\n",
      "30: [discriminator loss: 0.001781, acc: 1.000000] [adversarial loss: 0.000896, acc: 1.000000]\n",
      "31: [discriminator loss: 0.001124, acc: 1.000000] [adversarial loss: 0.000806, acc: 1.000000]\n",
      "32: [discriminator loss: 0.000957, acc: 1.000000] [adversarial loss: 0.000777, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33: [discriminator loss: 0.000998, acc: 1.000000] [adversarial loss: 0.000598, acc: 1.000000]\n",
      "34: [discriminator loss: 0.000832, acc: 1.000000] [adversarial loss: 0.000597, acc: 1.000000]\n",
      "35: [discriminator loss: 0.000823, acc: 1.000000] [adversarial loss: 0.000494, acc: 1.000000]\n",
      "36: [discriminator loss: 0.000860, acc: 1.000000] [adversarial loss: 0.000439, acc: 1.000000]\n",
      "37: [discriminator loss: 0.000874, acc: 1.000000] [adversarial loss: 0.000312, acc: 1.000000]\n",
      "38: [discriminator loss: 0.000763, acc: 1.000000] [adversarial loss: 0.000343, acc: 1.000000]\n",
      "39: [discriminator loss: 0.000639, acc: 1.000000] [adversarial loss: 0.000307, acc: 1.000000]\n",
      "40: [discriminator loss: 0.000645, acc: 1.000000] [adversarial loss: 0.000262, acc: 1.000000]\n",
      "41: [discriminator loss: 0.000558, acc: 1.000000] [adversarial loss: 0.000247, acc: 1.000000]\n",
      "42: [discriminator loss: 0.000599, acc: 1.000000] [adversarial loss: 0.000193, acc: 1.000000]\n",
      "43: [discriminator loss: 0.000525, acc: 1.000000] [adversarial loss: 0.000187, acc: 1.000000]\n",
      "44: [discriminator loss: 0.000413, acc: 1.000000] [adversarial loss: 0.000169, acc: 1.000000]\n",
      "45: [discriminator loss: 0.000442, acc: 1.000000] [adversarial loss: 0.000143, acc: 1.000000]\n",
      "46: [discriminator loss: 0.000368, acc: 1.000000] [adversarial loss: 0.000146, acc: 1.000000]\n",
      "47: [discriminator loss: 0.000443, acc: 1.000000] [adversarial loss: 0.000108, acc: 1.000000]\n",
      "48: [discriminator loss: 0.000386, acc: 1.000000] [adversarial loss: 0.000100, acc: 1.000000]\n",
      "49: [discriminator loss: 0.000339, acc: 1.000000] [adversarial loss: 0.000092, acc: 1.000000]\n",
      "50: [discriminator loss: 0.000271, acc: 1.000000] [adversarial loss: 0.000095, acc: 1.000000]\n",
      "51: [discriminator loss: 0.000338, acc: 1.000000] [adversarial loss: 0.000070, acc: 1.000000]\n",
      "52: [discriminator loss: 0.000236, acc: 1.000000] [adversarial loss: 0.000072, acc: 1.000000]\n",
      "53: [discriminator loss: 0.000226, acc: 1.000000] [adversarial loss: 0.000071, acc: 1.000000]\n",
      "54: [discriminator loss: 0.000249, acc: 1.000000] [adversarial loss: 0.000056, acc: 1.000000]\n",
      "55: [discriminator loss: 0.000197, acc: 1.000000] [adversarial loss: 0.000054, acc: 1.000000]\n",
      "56: [discriminator loss: 0.000370, acc: 1.000000] [adversarial loss: 0.000015, acc: 1.000000]\n",
      "57: [discriminator loss: 0.000240, acc: 1.000000] [adversarial loss: 0.000039, acc: 1.000000]\n",
      "58: [discriminator loss: 0.000188, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "59: [discriminator loss: 0.000163, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "60: [discriminator loss: 0.000132, acc: 1.000000] [adversarial loss: 0.000030, acc: 1.000000]\n",
      "61: [discriminator loss: 0.000116, acc: 1.000000] [adversarial loss: 0.000031, acc: 1.000000]\n",
      "62: [discriminator loss: 0.000129, acc: 1.000000] [adversarial loss: 0.000028, acc: 1.000000]\n",
      "63: [discriminator loss: 0.000133, acc: 1.000000] [adversarial loss: 0.000023, acc: 1.000000]\n",
      "64: [discriminator loss: 0.000097, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "65: [discriminator loss: 0.000098, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "66: [discriminator loss: 0.000095, acc: 1.000000] [adversarial loss: 0.000022, acc: 1.000000]\n",
      "67: [discriminator loss: 0.000106, acc: 1.000000] [adversarial loss: 0.000016, acc: 1.000000]\n",
      "68: [discriminator loss: 0.000088, acc: 1.000000] [adversarial loss: 0.000015, acc: 1.000000]\n",
      "69: [discriminator loss: 0.000074, acc: 1.000000] [adversarial loss: 0.000018, acc: 1.000000]\n",
      "70: [discriminator loss: 0.000089, acc: 1.000000] [adversarial loss: 0.000013, acc: 1.000000]\n",
      "71: [discriminator loss: 0.000083, acc: 1.000000] [adversarial loss: 0.000011, acc: 1.000000]\n",
      "72: [discriminator loss: 0.000059, acc: 1.000000] [adversarial loss: 0.000014, acc: 1.000000]\n",
      "73: [discriminator loss: 0.000060, acc: 1.000000] [adversarial loss: 0.000013, acc: 1.000000]\n",
      "74: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.000012, acc: 1.000000]\n",
      "75: [discriminator loss: 0.000049, acc: 1.000000] [adversarial loss: 0.000013, acc: 1.000000]\n",
      "76: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "77: [discriminator loss: 0.000083, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "78: [discriminator loss: 0.000050, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "79: [discriminator loss: 0.000050, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "80: [discriminator loss: 0.000046, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "81: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "82: [discriminator loss: 0.000043, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "83: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "84: [discriminator loss: 0.000041, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "85: [discriminator loss: 0.000031, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "86: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "87: [discriminator loss: 0.000027, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "88: [discriminator loss: 0.000045, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "89: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "90: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "91: [discriminator loss: 0.000021, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "92: [discriminator loss: 0.000019, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "93: [discriminator loss: 0.000019, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "94: [discriminator loss: 0.000018, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "95: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "96: [discriminator loss: 0.000017, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "97: [discriminator loss: 0.000015, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "98: [discriminator loss: 0.000021, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "99: [discriminator loss: 0.000016, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "100: [discriminator loss: 0.000014, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "101: [discriminator loss: 0.000015, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "102: [discriminator loss: 0.000011, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "103: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "104: [discriminator loss: 0.000011, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "105: [discriminator loss: 0.000010, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "106: [discriminator loss: 0.000018, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "107: [discriminator loss: 0.000022, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "108: [discriminator loss: 0.000018, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "109: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "110: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "111: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "112: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "113: [discriminator loss: 0.000013, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "114: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "115: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "116: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "117: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "118: [discriminator loss: 0.000005, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "119: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "121: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000007, acc: 1.000000]\n",
      "122: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "123: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "124: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "125: [discriminator loss: 0.000006, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "126: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "127: [discriminator loss: 0.000004, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "128: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "129: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "130: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "131: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "132: [discriminator loss: 0.000002, acc: 1.000000] [adversarial loss: 0.000008, acc: 1.000000]\n",
      "133: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "134: [discriminator loss: 0.000003, acc: 1.000000] [adversarial loss: 0.000006, acc: 1.000000]\n",
      "135: [discriminator loss: 0.000012, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "136: [discriminator loss: 0.003769, acc: 1.000000] [adversarial loss: 8.038582, acc: 0.000000]\n",
      "137: [discriminator loss: 0.259973, acc: 0.929688] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "138: [discriminator loss: 0.002612, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "139: [discriminator loss: 0.001466, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "140: [discriminator loss: 0.001086, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "141: [discriminator loss: 0.000863, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "142: [discriminator loss: 0.000726, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "143: [discriminator loss: 0.000637, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "144: [discriminator loss: 0.000567, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "145: [discriminator loss: 0.000516, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "146: [discriminator loss: 0.000461, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "147: [discriminator loss: 0.000415, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "148: [discriminator loss: 0.000400, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "149: [discriminator loss: 0.000368, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "150: [discriminator loss: 0.000345, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "151: [discriminator loss: 0.000329, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "152: [discriminator loss: 0.000314, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "153: [discriminator loss: 0.000293, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "154: [discriminator loss: 0.000275, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "155: [discriminator loss: 0.000260, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "156: [discriminator loss: 0.000253, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "157: [discriminator loss: 0.000235, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "158: [discriminator loss: 0.000230, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "159: [discriminator loss: 0.000214, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "160: [discriminator loss: 0.000208, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "161: [discriminator loss: 0.000199, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "162: [discriminator loss: 0.000185, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "163: [discriminator loss: 0.000181, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "164: [discriminator loss: 0.000173, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "165: [discriminator loss: 0.000165, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "166: [discriminator loss: 0.000157, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "167: [discriminator loss: 0.000152, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "168: [discriminator loss: 0.000142, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "169: [discriminator loss: 0.000139, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "170: [discriminator loss: 0.000133, acc: 1.000000] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "171: [discriminator loss: 0.000125, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "172: [discriminator loss: 0.000123, acc: 1.000000] [adversarial loss: 0.000002, acc: 1.000000]\n",
      "173: [discriminator loss: 0.000115, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "174: [discriminator loss: 0.000113, acc: 1.000000] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "175: [discriminator loss: 0.000107, acc: 1.000000] [adversarial loss: 0.000004, acc: 1.000000]\n",
      "176: [discriminator loss: 0.000101, acc: 1.000000] [adversarial loss: 0.000005, acc: 1.000000]\n",
      "177: [discriminator loss: 0.000098, acc: 1.000000] [adversarial loss: 0.000007, acc: 1.000000]\n",
      "178: [discriminator loss: 0.000093, acc: 1.000000] [adversarial loss: 0.000009, acc: 1.000000]\n",
      "179: [discriminator loss: 0.000090, acc: 1.000000] [adversarial loss: 0.000011, acc: 1.000000]\n",
      "180: [discriminator loss: 0.000086, acc: 1.000000] [adversarial loss: 0.000015, acc: 1.000000]\n",
      "181: [discriminator loss: 0.000082, acc: 1.000000] [adversarial loss: 0.000018, acc: 1.000000]\n",
      "182: [discriminator loss: 0.000078, acc: 1.000000] [adversarial loss: 0.000024, acc: 1.000000]\n",
      "183: [discriminator loss: 0.000075, acc: 1.000000] [adversarial loss: 0.000029, acc: 1.000000]\n",
      "184: [discriminator loss: 0.000069, acc: 1.000000] [adversarial loss: 0.000035, acc: 1.000000]\n",
      "185: [discriminator loss: 0.000069, acc: 1.000000] [adversarial loss: 0.000045, acc: 1.000000]\n",
      "186: [discriminator loss: 0.000065, acc: 1.000000] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "187: [discriminator loss: 0.000063, acc: 1.000000] [adversarial loss: 0.000071, acc: 1.000000]\n",
      "188: [discriminator loss: 0.000061, acc: 1.000000] [adversarial loss: 0.000098, acc: 1.000000]\n",
      "189: [discriminator loss: 0.000057, acc: 1.000000] [adversarial loss: 0.000110, acc: 1.000000]\n",
      "190: [discriminator loss: 0.000055, acc: 1.000000] [adversarial loss: 0.000138, acc: 1.000000]\n",
      "191: [discriminator loss: 0.000053, acc: 1.000000] [adversarial loss: 0.000166, acc: 1.000000]\n",
      "192: [discriminator loss: 0.000050, acc: 1.000000] [adversarial loss: 0.000202, acc: 1.000000]\n",
      "193: [discriminator loss: 0.000051, acc: 1.000000] [adversarial loss: 0.000252, acc: 1.000000]\n",
      "194: [discriminator loss: 0.000047, acc: 1.000000] [adversarial loss: 0.000292, acc: 1.000000]\n",
      "195: [discriminator loss: 0.000046, acc: 1.000000] [adversarial loss: 0.000354, acc: 1.000000]\n",
      "196: [discriminator loss: 0.000045, acc: 1.000000] [adversarial loss: 0.000398, acc: 1.000000]\n",
      "197: [discriminator loss: 0.000044, acc: 1.000000] [adversarial loss: 0.000469, acc: 1.000000]\n",
      "198: [discriminator loss: 0.000041, acc: 1.000000] [adversarial loss: 0.000579, acc: 1.000000]\n",
      "199: [discriminator loss: 0.000038, acc: 1.000000] [adversarial loss: 0.000624, acc: 1.000000]\n",
      "200: [discriminator loss: 0.000039, acc: 1.000000] [adversarial loss: 0.000767, acc: 1.000000]\n",
      "201: [discriminator loss: 0.000036, acc: 1.000000] [adversarial loss: 0.000902, acc: 1.000000]\n",
      "202: [discriminator loss: 0.000034, acc: 1.000000] [adversarial loss: 0.000959, acc: 1.000000]\n",
      "203: [discriminator loss: 0.000032, acc: 1.000000] [adversarial loss: 0.001102, acc: 1.000000]\n",
      "204: [discriminator loss: 0.000030, acc: 1.000000] [adversarial loss: 0.001234, acc: 1.000000]\n",
      "205: [discriminator loss: 0.000028, acc: 1.000000] [adversarial loss: 0.001338, acc: 1.000000]\n",
      "206: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.001447, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207: [discriminator loss: 0.000025, acc: 1.000000] [adversarial loss: 0.001633, acc: 1.000000]\n",
      "208: [discriminator loss: 0.000024, acc: 1.000000] [adversarial loss: 0.001727, acc: 1.000000]\n",
      "209: [discriminator loss: 0.000023, acc: 1.000000] [adversarial loss: 0.001814, acc: 1.000000]\n",
      "210: [discriminator loss: 0.000023, acc: 1.000000] [adversarial loss: 0.001880, acc: 1.000000]\n",
      "211: [discriminator loss: 0.000024, acc: 1.000000] [adversarial loss: 0.001901, acc: 1.000000]\n",
      "212: [discriminator loss: 0.000024, acc: 1.000000] [adversarial loss: 0.002188, acc: 1.000000]\n",
      "213: [discriminator loss: 0.000026, acc: 1.000000] [adversarial loss: 0.002151, acc: 1.000000]\n",
      "214: [discriminator loss: 0.000028, acc: 1.000000] [adversarial loss: 0.002413, acc: 1.000000]\n",
      "215: [discriminator loss: 0.000030, acc: 1.000000] [adversarial loss: 0.002701, acc: 1.000000]\n",
      "216: [discriminator loss: 0.000037, acc: 1.000000] [adversarial loss: 0.002711, acc: 1.000000]\n",
      "217: [discriminator loss: 0.000046, acc: 1.000000] [adversarial loss: 0.002991, acc: 1.000000]\n",
      "218: [discriminator loss: 0.000056, acc: 1.000000] [adversarial loss: 0.002910, acc: 1.000000]\n",
      "219: [discriminator loss: 0.000071, acc: 1.000000] [adversarial loss: 0.004338, acc: 1.000000]\n",
      "220: [discriminator loss: 0.000063, acc: 1.000000] [adversarial loss: 0.006875, acc: 1.000000]\n",
      "221: [discriminator loss: 0.000049, acc: 1.000000] [adversarial loss: 0.006578, acc: 1.000000]\n",
      "222: [discriminator loss: 0.000946, acc: 1.000000] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "223: [discriminator loss: 1.452650, acc: 0.500000] [adversarial loss: 2.254936, acc: 0.000000]\n",
      "224: [discriminator loss: 0.043691, acc: 0.984375] [adversarial loss: 0.663425, acc: 0.640625]\n",
      "225: [discriminator loss: 0.000108, acc: 1.000000] [adversarial loss: 0.181995, acc: 1.000000]\n",
      "226: [discriminator loss: 0.002254, acc: 1.000000] [adversarial loss: 0.110496, acc: 1.000000]\n",
      "227: [discriminator loss: 0.000174, acc: 1.000000] [adversarial loss: 0.097764, acc: 1.000000]\n",
      "228: [discriminator loss: 0.000644, acc: 1.000000] [adversarial loss: 0.082112, acc: 1.000000]\n",
      "229: [discriminator loss: 0.000087, acc: 1.000000] [adversarial loss: 0.074869, acc: 1.000000]\n",
      "230: [discriminator loss: 0.000084, acc: 1.000000] [adversarial loss: 0.068361, acc: 1.000000]\n",
      "231: [discriminator loss: 0.000167, acc: 1.000000] [adversarial loss: 0.064091, acc: 1.000000]\n",
      "232: [discriminator loss: 0.000203, acc: 1.000000] [adversarial loss: 0.059735, acc: 1.000000]\n",
      "233: [discriminator loss: 0.000078, acc: 1.000000] [adversarial loss: 0.056348, acc: 1.000000]\n",
      "234: [discriminator loss: 0.000128, acc: 1.000000] [adversarial loss: 0.052578, acc: 1.000000]\n",
      "235: [discriminator loss: 0.000462, acc: 1.000000] [adversarial loss: 0.046003, acc: 1.000000]\n",
      "236: [discriminator loss: 0.000833, acc: 1.000000] [adversarial loss: 0.039163, acc: 1.000000]\n",
      "237: [discriminator loss: 0.000317, acc: 1.000000] [adversarial loss: 0.037076, acc: 1.000000]\n",
      "238: [discriminator loss: 0.000167, acc: 1.000000] [adversarial loss: 0.034461, acc: 1.000000]\n",
      "239: [discriminator loss: 0.000178, acc: 1.000000] [adversarial loss: 0.033548, acc: 1.000000]\n",
      "240: [discriminator loss: 0.000200, acc: 1.000000] [adversarial loss: 0.031990, acc: 1.000000]\n",
      "241: [discriminator loss: 0.000360, acc: 1.000000] [adversarial loss: 0.030088, acc: 1.000000]\n",
      "242: [discriminator loss: 0.000226, acc: 1.000000] [adversarial loss: 0.029993, acc: 1.000000]\n",
      "243: [discriminator loss: 0.001277, acc: 1.000000] [adversarial loss: 0.023421, acc: 1.000000]\n",
      "244: [discriminator loss: 0.000289, acc: 1.000000] [adversarial loss: 0.023609, acc: 1.000000]\n",
      "245: [discriminator loss: 0.000300, acc: 1.000000] [adversarial loss: 0.024007, acc: 1.000000]\n",
      "246: [discriminator loss: 0.003726, acc: 1.000000] [adversarial loss: 0.009625, acc: 1.000000]\n",
      "247: [discriminator loss: 0.000673, acc: 1.000000] [adversarial loss: 0.010526, acc: 1.000000]\n",
      "248: [discriminator loss: 0.000678, acc: 1.000000] [adversarial loss: 0.011854, acc: 1.000000]\n",
      "249: [discriminator loss: 0.000717, acc: 1.000000] [adversarial loss: 0.013523, acc: 1.000000]\n",
      "250: [discriminator loss: 0.000686, acc: 1.000000] [adversarial loss: 0.015549, acc: 1.000000]\n",
      "251: [discriminator loss: 0.000688, acc: 1.000000] [adversarial loss: 0.018247, acc: 1.000000]\n",
      "252: [discriminator loss: 0.000705, acc: 1.000000] [adversarial loss: 0.020925, acc: 1.000000]\n",
      "253: [discriminator loss: 0.000738, acc: 1.000000] [adversarial loss: 0.024354, acc: 1.000000]\n",
      "254: [discriminator loss: 0.000739, acc: 1.000000] [adversarial loss: 0.028659, acc: 1.000000]\n",
      "255: [discriminator loss: 0.000763, acc: 1.000000] [adversarial loss: 0.033544, acc: 1.000000]\n",
      "256: [discriminator loss: 0.000895, acc: 1.000000] [adversarial loss: 0.039927, acc: 1.000000]\n",
      "257: [discriminator loss: 0.000994, acc: 1.000000] [adversarial loss: 0.044792, acc: 1.000000]\n",
      "258: [discriminator loss: 0.001041, acc: 1.000000] [adversarial loss: 0.054076, acc: 1.000000]\n",
      "259: [discriminator loss: 0.001174, acc: 1.000000] [adversarial loss: 0.064068, acc: 1.000000]\n",
      "260: [discriminator loss: 0.001234, acc: 1.000000] [adversarial loss: 0.090197, acc: 1.000000]\n",
      "261: [discriminator loss: 0.002856, acc: 1.000000] [adversarial loss: 0.053450, acc: 1.000000]\n",
      "262: [discriminator loss: 0.002819, acc: 1.000000] [adversarial loss: 0.155384, acc: 1.000000]\n",
      "263: [discriminator loss: 0.003640, acc: 1.000000] [adversarial loss: 0.391226, acc: 1.000000]\n",
      "264: [discriminator loss: 0.009437, acc: 1.000000] [adversarial loss: 1.492743, acc: 0.000000]\n",
      "265: [discriminator loss: 0.011872, acc: 1.000000] [adversarial loss: 0.651479, acc: 0.687500]\n",
      "266: [discriminator loss: 0.019164, acc: 1.000000] [adversarial loss: 3.511227, acc: 0.000000]\n",
      "267: [discriminator loss: 0.040281, acc: 0.984375] [adversarial loss: 0.001063, acc: 1.000000]\n",
      "268: [discriminator loss: 0.263706, acc: 0.992188] [adversarial loss: 9.721970, acc: 0.000000]\n",
      "269: [discriminator loss: 0.679446, acc: 0.835938] [adversarial loss: 0.012673, acc: 1.000000]\n",
      "270: [discriminator loss: 0.308578, acc: 0.968750] [adversarial loss: 0.502884, acc: 0.843750]\n",
      "271: [discriminator loss: 0.031415, acc: 1.000000] [adversarial loss: 0.233198, acc: 1.000000]\n",
      "272: [discriminator loss: 0.022515, acc: 1.000000] [adversarial loss: 0.258322, acc: 1.000000]\n",
      "273: [discriminator loss: 0.018621, acc: 1.000000] [adversarial loss: 0.324419, acc: 1.000000]\n",
      "274: [discriminator loss: 0.015469, acc: 1.000000] [adversarial loss: 0.372689, acc: 1.000000]\n",
      "275: [discriminator loss: 0.014000, acc: 1.000000] [adversarial loss: 0.428442, acc: 1.000000]\n",
      "276: [discriminator loss: 0.013672, acc: 1.000000] [adversarial loss: 0.447960, acc: 1.000000]\n",
      "277: [discriminator loss: 0.015967, acc: 1.000000] [adversarial loss: 0.454441, acc: 1.000000]\n",
      "278: [discriminator loss: 0.033538, acc: 1.000000] [adversarial loss: 0.253460, acc: 1.000000]\n",
      "279: [discriminator loss: 0.301836, acc: 0.906250] [adversarial loss: 4.337841, acc: 0.000000]\n",
      "280: [discriminator loss: 0.936436, acc: 0.859375] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "281: [discriminator loss: 1.628641, acc: 0.500000] [adversarial loss: 0.703651, acc: 0.484375]\n",
      "282: [discriminator loss: 0.207627, acc: 0.960938] [adversarial loss: 0.005966, acc: 1.000000]\n",
      "283: [discriminator loss: 0.075834, acc: 1.000000] [adversarial loss: 0.021802, acc: 1.000000]\n",
      "284: [discriminator loss: 0.061685, acc: 0.992188] [adversarial loss: 0.023184, acc: 1.000000]\n",
      "285: [discriminator loss: 0.070695, acc: 0.976562] [adversarial loss: 0.007608, acc: 1.000000]\n",
      "286: [discriminator loss: 0.056694, acc: 1.000000] [adversarial loss: 0.038749, acc: 1.000000]\n",
      "287: [discriminator loss: 0.050755, acc: 0.992188] [adversarial loss: 0.026647, acc: 1.000000]\n",
      "288: [discriminator loss: 0.038159, acc: 1.000000] [adversarial loss: 0.045160, acc: 1.000000]\n",
      "289: [discriminator loss: 0.074757, acc: 0.984375] [adversarial loss: 0.016995, acc: 1.000000]\n",
      "290: [discriminator loss: 0.035895, acc: 1.000000] [adversarial loss: 0.064305, acc: 1.000000]\n",
      "291: [discriminator loss: 0.020717, acc: 1.000000] [adversarial loss: 0.112928, acc: 1.000000]\n",
      "292: [discriminator loss: 0.033034, acc: 0.992188] [adversarial loss: 0.038445, acc: 1.000000]\n",
      "293: [discriminator loss: 0.026855, acc: 1.000000] [adversarial loss: 0.052875, acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294: [discriminator loss: 0.038471, acc: 0.992188] [adversarial loss: 0.029630, acc: 1.000000]\n",
      "295: [discriminator loss: 0.025169, acc: 1.000000] [adversarial loss: 0.071919, acc: 1.000000]\n",
      "296: [discriminator loss: 0.033344, acc: 0.992188] [adversarial loss: 0.012890, acc: 1.000000]\n",
      "297: [discriminator loss: 0.030627, acc: 1.000000] [adversarial loss: 0.166897, acc: 1.000000]\n",
      "298: [discriminator loss: 0.020952, acc: 0.992188] [adversarial loss: 0.047800, acc: 1.000000]\n",
      "299: [discriminator loss: 0.015132, acc: 1.000000] [adversarial loss: 0.143511, acc: 1.000000]\n",
      "300: [discriminator loss: 0.012147, acc: 1.000000] [adversarial loss: 0.077714, acc: 1.000000]\n",
      "301: [discriminator loss: 0.011179, acc: 1.000000] [adversarial loss: 0.122616, acc: 1.000000]\n",
      "302: [discriminator loss: 0.013813, acc: 1.000000] [adversarial loss: 0.077366, acc: 1.000000]\n",
      "303: [discriminator loss: 0.012768, acc: 1.000000] [adversarial loss: 0.057186, acc: 1.000000]\n",
      "304: [discriminator loss: 0.020662, acc: 0.992188] [adversarial loss: 0.035398, acc: 1.000000]\n",
      "305: [discriminator loss: 0.011233, acc: 1.000000] [adversarial loss: 0.113350, acc: 1.000000]\n",
      "306: [discriminator loss: 0.012308, acc: 1.000000] [adversarial loss: 0.045545, acc: 1.000000]\n",
      "307: [discriminator loss: 0.010075, acc: 1.000000] [adversarial loss: 0.177078, acc: 1.000000]\n",
      "308: [discriminator loss: 0.012007, acc: 1.000000] [adversarial loss: 0.036872, acc: 1.000000]\n",
      "309: [discriminator loss: 0.010320, acc: 1.000000] [adversarial loss: 0.137872, acc: 1.000000]\n",
      "310: [discriminator loss: 0.011110, acc: 1.000000] [adversarial loss: 0.080460, acc: 1.000000]\n",
      "311: [discriminator loss: 0.008665, acc: 1.000000] [adversarial loss: 0.150546, acc: 1.000000]\n",
      "312: [discriminator loss: 0.015198, acc: 1.000000] [adversarial loss: 0.003291, acc: 1.000000]\n",
      "313: [discriminator loss: 0.023864, acc: 1.000000] [adversarial loss: 0.501475, acc: 0.906250]\n",
      "314: [discriminator loss: 0.041696, acc: 0.992188] [adversarial loss: 0.000081, acc: 1.000000]\n",
      "315: [discriminator loss: 0.066682, acc: 1.000000] [adversarial loss: 1.826060, acc: 0.000000]\n",
      "316: [discriminator loss: 0.172787, acc: 0.945312] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "317: [discriminator loss: 1.451000, acc: 0.500000] [adversarial loss: 0.351590, acc: 1.000000]\n",
      "318: [discriminator loss: 0.007355, acc: 1.000000] [adversarial loss: 0.089906, acc: 1.000000]\n",
      "319: [discriminator loss: 0.008000, acc: 1.000000] [adversarial loss: 0.081484, acc: 1.000000]\n",
      "320: [discriminator loss: 0.007306, acc: 1.000000] [adversarial loss: 0.080361, acc: 1.000000]\n",
      "321: [discriminator loss: 0.007172, acc: 1.000000] [adversarial loss: 0.089279, acc: 1.000000]\n",
      "322: [discriminator loss: 0.013914, acc: 0.992188] [adversarial loss: 0.028760, acc: 1.000000]\n",
      "323: [discriminator loss: 0.010977, acc: 1.000000] [adversarial loss: 0.049523, acc: 1.000000]\n",
      "324: [discriminator loss: 0.009298, acc: 1.000000] [adversarial loss: 0.075498, acc: 1.000000]\n",
      "325: [discriminator loss: 0.008262, acc: 1.000000] [adversarial loss: 0.086576, acc: 1.000000]\n",
      "326: [discriminator loss: 0.011913, acc: 1.000000] [adversarial loss: 0.033094, acc: 1.000000]\n",
      "327: [discriminator loss: 0.011214, acc: 1.000000] [adversarial loss: 0.082253, acc: 1.000000]\n",
      "328: [discriminator loss: 0.010980, acc: 1.000000] [adversarial loss: 0.086509, acc: 1.000000]\n",
      "329: [discriminator loss: 0.010957, acc: 1.000000] [adversarial loss: 0.173291, acc: 1.000000]\n",
      "330: [discriminator loss: 0.013712, acc: 1.000000] [adversarial loss: 0.203798, acc: 1.000000]\n",
      "331: [discriminator loss: 0.025851, acc: 1.000000] [adversarial loss: 0.010934, acc: 1.000000]\n",
      "332: [discriminator loss: 0.033053, acc: 1.000000] [adversarial loss: 1.190507, acc: 0.000000]\n",
      "333: [discriminator loss: 0.070546, acc: 0.984375] [adversarial loss: 0.000001, acc: 1.000000]\n",
      "334: [discriminator loss: 0.810370, acc: 0.531250] [adversarial loss: 7.654223, acc: 0.000000]\n",
      "335: [discriminator loss: 1.768765, acc: 0.523438] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "336: [discriminator loss: 0.768224, acc: 0.562500] [adversarial loss: 0.001304, acc: 1.000000]\n",
      "337: [discriminator loss: 0.068085, acc: 1.000000] [adversarial loss: 0.004330, acc: 1.000000]\n",
      "338: [discriminator loss: 0.044228, acc: 1.000000] [adversarial loss: 0.010595, acc: 1.000000]\n",
      "339: [discriminator loss: 0.036514, acc: 1.000000] [adversarial loss: 0.019963, acc: 1.000000]\n",
      "340: [discriminator loss: 0.028509, acc: 1.000000] [adversarial loss: 0.028636, acc: 1.000000]\n",
      "341: [discriminator loss: 0.027858, acc: 1.000000] [adversarial loss: 0.039449, acc: 1.000000]\n",
      "342: [discriminator loss: 0.023433, acc: 1.000000] [adversarial loss: 0.045281, acc: 1.000000]\n",
      "343: [discriminator loss: 0.025884, acc: 1.000000] [adversarial loss: 0.051848, acc: 1.000000]\n",
      "344: [discriminator loss: 0.025802, acc: 1.000000] [adversarial loss: 0.055639, acc: 1.000000]\n",
      "345: [discriminator loss: 0.030547, acc: 1.000000] [adversarial loss: 0.038396, acc: 1.000000]\n",
      "346: [discriminator loss: 0.034139, acc: 1.000000] [adversarial loss: 0.039819, acc: 1.000000]\n",
      "347: [discriminator loss: 0.041900, acc: 1.000000] [adversarial loss: 0.085864, acc: 1.000000]\n",
      "348: [discriminator loss: 0.050888, acc: 1.000000] [adversarial loss: 0.083595, acc: 1.000000]\n",
      "349: [discriminator loss: 0.047558, acc: 1.000000] [adversarial loss: 0.225559, acc: 1.000000]\n",
      "350: [discriminator loss: 0.084187, acc: 0.976562] [adversarial loss: 0.028179, acc: 1.000000]\n",
      "351: [discriminator loss: 0.080572, acc: 1.000000] [adversarial loss: 2.133774, acc: 0.000000]\n",
      "352: [discriminator loss: 0.075720, acc: 0.984375] [adversarial loss: 0.037223, acc: 1.000000]\n",
      "353: [discriminator loss: 0.059294, acc: 1.000000] [adversarial loss: 0.671210, acc: 0.671875]\n",
      "354: [discriminator loss: 0.041332, acc: 0.992188] [adversarial loss: 0.273938, acc: 1.000000]\n",
      "355: [discriminator loss: 0.056492, acc: 1.000000] [adversarial loss: 0.323759, acc: 1.000000]\n",
      "356: [discriminator loss: 0.131411, acc: 0.984375] [adversarial loss: 0.000912, acc: 1.000000]\n",
      "357: [discriminator loss: 0.256344, acc: 0.882812] [adversarial loss: 6.328544, acc: 0.000000]\n",
      "358: [discriminator loss: 1.001314, acc: 0.687500] [adversarial loss: 0.000059, acc: 1.000000]\n",
      "359: [discriminator loss: 0.496079, acc: 0.664062] [adversarial loss: 0.066943, acc: 1.000000]\n",
      "360: [discriminator loss: 0.039868, acc: 1.000000] [adversarial loss: 0.118172, acc: 1.000000]\n",
      "361: [discriminator loss: 0.037221, acc: 1.000000] [adversarial loss: 0.182477, acc: 1.000000]\n",
      "362: [discriminator loss: 0.038456, acc: 1.000000] [adversarial loss: 0.208782, acc: 1.000000]\n",
      "363: [discriminator loss: 0.089867, acc: 1.000000] [adversarial loss: 0.615827, acc: 0.828125]\n",
      "364: [discriminator loss: 0.126048, acc: 1.000000] [adversarial loss: 1.496460, acc: 0.000000]\n",
      "365: [discriminator loss: 0.191371, acc: 0.945312] [adversarial loss: 0.001799, acc: 1.000000]\n",
      "366: [discriminator loss: 0.253687, acc: 0.851562] [adversarial loss: 4.099234, acc: 0.000000]\n",
      "367: [discriminator loss: 0.854807, acc: 0.648438] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "368: [discriminator loss: 2.244697, acc: 0.500000] [adversarial loss: 0.068225, acc: 1.000000]\n",
      "369: [discriminator loss: 0.072890, acc: 0.992188] [adversarial loss: 0.063534, acc: 1.000000]\n",
      "370: [discriminator loss: 0.069170, acc: 1.000000] [adversarial loss: 0.141570, acc: 1.000000]\n",
      "371: [discriminator loss: 0.085037, acc: 0.984375] [adversarial loss: 0.143831, acc: 1.000000]\n",
      "372: [discriminator loss: 0.102510, acc: 0.992188] [adversarial loss: 0.228866, acc: 1.000000]\n",
      "373: [discriminator loss: 0.156782, acc: 0.976562] [adversarial loss: 0.271811, acc: 0.984375]\n",
      "374: [discriminator loss: 0.134254, acc: 0.992188] [adversarial loss: 2.018989, acc: 0.000000]\n",
      "375: [discriminator loss: 0.221412, acc: 0.945312] [adversarial loss: 0.003822, acc: 1.000000]\n",
      "376: [discriminator loss: 0.787634, acc: 0.523438] [adversarial loss: 14.340611, acc: 0.000000]\n",
      "377: [discriminator loss: 2.902922, acc: 0.500000] [adversarial loss: 0.152903, acc: 1.000000]\n",
      "378: [discriminator loss: 0.106719, acc: 0.984375] [adversarial loss: 0.606516, acc: 0.796875]\n",
      "379: [discriminator loss: 0.056764, acc: 1.000000] [adversarial loss: 0.722847, acc: 0.515625]\n",
      "380: [discriminator loss: 0.085094, acc: 1.000000] [adversarial loss: 1.230009, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381: [discriminator loss: 0.065257, acc: 1.000000] [adversarial loss: 1.152692, acc: 0.000000]\n",
      "382: [discriminator loss: 0.073726, acc: 1.000000] [adversarial loss: 1.153270, acc: 0.000000]\n",
      "383: [discriminator loss: 0.062952, acc: 1.000000] [adversarial loss: 1.489701, acc: 0.000000]\n",
      "384: [discriminator loss: 0.115581, acc: 0.984375] [adversarial loss: 0.300328, acc: 1.000000]\n",
      "385: [discriminator loss: 0.206077, acc: 0.984375] [adversarial loss: 3.296086, acc: 0.000000]\n",
      "386: [discriminator loss: 0.344037, acc: 0.867188] [adversarial loss: 0.009597, acc: 1.000000]\n",
      "387: [discriminator loss: 0.710769, acc: 0.515625] [adversarial loss: 5.090758, acc: 0.000000]\n",
      "388: [discriminator loss: 0.235210, acc: 0.914062] [adversarial loss: 0.664053, acc: 0.687500]\n",
      "389: [discriminator loss: 0.044301, acc: 1.000000] [adversarial loss: 0.755729, acc: 0.296875]\n",
      "390: [discriminator loss: 0.053393, acc: 0.992188] [adversarial loss: 0.656603, acc: 0.640625]\n",
      "391: [discriminator loss: 0.060601, acc: 1.000000] [adversarial loss: 0.273758, acc: 1.000000]\n",
      "392: [discriminator loss: 0.136560, acc: 0.984375] [adversarial loss: 0.571818, acc: 0.828125]\n",
      "393: [discriminator loss: 0.184666, acc: 0.976562] [adversarial loss: 0.573769, acc: 0.890625]\n",
      "394: [discriminator loss: 0.273604, acc: 0.937500] [adversarial loss: 0.125037, acc: 1.000000]\n",
      "395: [discriminator loss: 0.186532, acc: 0.960938] [adversarial loss: 2.441802, acc: 0.000000]\n",
      "396: [discriminator loss: 0.236831, acc: 0.937500] [adversarial loss: 0.027504, acc: 1.000000]\n",
      "397: [discriminator loss: 0.336894, acc: 0.796875] [adversarial loss: 4.355394, acc: 0.000000]\n",
      "398: [discriminator loss: 0.241310, acc: 0.906250] [adversarial loss: 0.113287, acc: 1.000000]\n",
      "399: [discriminator loss: 0.208012, acc: 0.929688] [adversarial loss: 2.623147, acc: 0.000000]\n",
      "400: [discriminator loss: 0.109849, acc: 0.992188] [adversarial loss: 1.602296, acc: 0.000000]\n",
      "401: [discriminator loss: 0.183543, acc: 0.976562] [adversarial loss: 1.840851, acc: 0.000000]\n",
      "402: [discriminator loss: 0.141647, acc: 0.960938] [adversarial loss: 0.153891, acc: 1.000000]\n",
      "403: [discriminator loss: 0.257493, acc: 0.875000] [adversarial loss: 6.722266, acc: 0.000000]\n",
      "404: [discriminator loss: 0.450866, acc: 0.757812] [adversarial loss: 0.030121, acc: 1.000000]\n",
      "405: [discriminator loss: 0.436688, acc: 0.695312] [adversarial loss: 4.388584, acc: 0.000000]\n",
      "406: [discriminator loss: 0.052711, acc: 0.984375] [adversarial loss: 1.856200, acc: 0.000000]\n",
      "407: [discriminator loss: 0.082834, acc: 1.000000] [adversarial loss: 1.835785, acc: 0.000000]\n",
      "408: [discriminator loss: 0.071546, acc: 0.992188] [adversarial loss: 1.522872, acc: 0.000000]\n",
      "409: [discriminator loss: 0.075414, acc: 1.000000] [adversarial loss: 1.638685, acc: 0.000000]\n",
      "410: [discriminator loss: 0.094199, acc: 0.968750] [adversarial loss: 1.216769, acc: 0.000000]\n",
      "411: [discriminator loss: 0.070006, acc: 0.992188] [adversarial loss: 1.691296, acc: 0.000000]\n",
      "412: [discriminator loss: 0.085043, acc: 0.976562] [adversarial loss: 0.969869, acc: 0.000000]\n",
      "413: [discriminator loss: 0.071867, acc: 0.984375] [adversarial loss: 2.140551, acc: 0.000000]\n",
      "414: [discriminator loss: 0.048267, acc: 0.984375] [adversarial loss: 1.616030, acc: 0.000000]\n",
      "415: [discriminator loss: 0.052452, acc: 1.000000] [adversarial loss: 1.713751, acc: 0.000000]\n",
      "416: [discriminator loss: 0.057308, acc: 1.000000] [adversarial loss: 2.038726, acc: 0.000000]\n",
      "417: [discriminator loss: 0.089053, acc: 0.984375] [adversarial loss: 2.627154, acc: 0.000000]\n",
      "418: [discriminator loss: 0.078627, acc: 0.984375] [adversarial loss: 2.447842, acc: 0.000000]\n",
      "419: [discriminator loss: 0.145702, acc: 0.960938] [adversarial loss: 0.401068, acc: 1.000000]\n",
      "420: [discriminator loss: 0.356257, acc: 0.773438] [adversarial loss: 12.381842, acc: 0.000000]\n",
      "421: [discriminator loss: 2.651172, acc: 0.523438] [adversarial loss: 0.003629, acc: 1.000000]\n",
      "422: [discriminator loss: 0.552238, acc: 0.671875] [adversarial loss: 1.843559, acc: 0.000000]\n",
      "423: [discriminator loss: 0.104993, acc: 1.000000] [adversarial loss: 1.710408, acc: 0.000000]\n",
      "424: [discriminator loss: 0.159818, acc: 0.976562] [adversarial loss: 3.053638, acc: 0.000000]\n",
      "425: [discriminator loss: 0.438064, acc: 0.789062] [adversarial loss: 4.475970, acc: 0.000000]\n",
      "426: [discriminator loss: 0.981207, acc: 0.593750] [adversarial loss: 0.000180, acc: 1.000000]\n",
      "427: [discriminator loss: 2.612724, acc: 0.492188] [adversarial loss: 3.975132, acc: 0.000000]\n",
      "428: [discriminator loss: 0.504395, acc: 0.757812] [adversarial loss: 0.183500, acc: 1.000000]\n",
      "429: [discriminator loss: 0.406646, acc: 0.734375] [adversarial loss: 3.448322, acc: 0.000000]\n",
      "430: [discriminator loss: 0.183340, acc: 0.914062] [adversarial loss: 1.104183, acc: 0.015625]\n",
      "431: [discriminator loss: 0.117874, acc: 1.000000] [adversarial loss: 2.132605, acc: 0.000000]\n",
      "432: [discriminator loss: 0.114180, acc: 0.968750] [adversarial loss: 1.515531, acc: 0.000000]\n",
      "433: [discriminator loss: 0.147654, acc: 0.960938] [adversarial loss: 1.760311, acc: 0.000000]\n",
      "434: [discriminator loss: 0.095091, acc: 0.992188] [adversarial loss: 2.098915, acc: 0.000000]\n",
      "435: [discriminator loss: 0.126116, acc: 0.960938] [adversarial loss: 1.115866, acc: 0.015625]\n",
      "436: [discriminator loss: 0.097820, acc: 1.000000] [adversarial loss: 3.468907, acc: 0.000000]\n",
      "437: [discriminator loss: 0.099524, acc: 0.960938] [adversarial loss: 1.223944, acc: 0.000000]\n",
      "438: [discriminator loss: 0.091912, acc: 0.992188] [adversarial loss: 3.260926, acc: 0.000000]\n",
      "439: [discriminator loss: 0.101719, acc: 0.968750] [adversarial loss: 1.103356, acc: 0.000000]\n",
      "440: [discriminator loss: 0.114078, acc: 0.992188] [adversarial loss: 3.527202, acc: 0.000000]\n",
      "441: [discriminator loss: 0.052694, acc: 0.992188] [adversarial loss: 1.846848, acc: 0.000000]\n",
      "442: [discriminator loss: 0.077567, acc: 0.984375] [adversarial loss: 2.349852, acc: 0.000000]\n",
      "443: [discriminator loss: 0.079611, acc: 0.992188] [adversarial loss: 1.257088, acc: 0.000000]\n",
      "444: [discriminator loss: 0.063172, acc: 1.000000] [adversarial loss: 2.145174, acc: 0.000000]\n",
      "445: [discriminator loss: 0.098873, acc: 0.968750] [adversarial loss: 0.281011, acc: 1.000000]\n",
      "446: [discriminator loss: 0.123353, acc: 1.000000] [adversarial loss: 3.675992, acc: 0.000000]\n",
      "447: [discriminator loss: 0.162363, acc: 0.921875] [adversarial loss: 0.009134, acc: 1.000000]\n",
      "448: [discriminator loss: 0.394535, acc: 0.750000] [adversarial loss: 5.262961, acc: 0.000000]\n",
      "449: [discriminator loss: 0.442725, acc: 0.765625] [adversarial loss: 0.002841, acc: 1.000000]\n",
      "450: [discriminator loss: 0.504140, acc: 0.671875] [adversarial loss: 1.783322, acc: 0.000000]\n",
      "451: [discriminator loss: 0.071175, acc: 0.976562] [adversarial loss: 0.934650, acc: 0.203125]\n",
      "452: [discriminator loss: 0.114144, acc: 0.992188] [adversarial loss: 1.255367, acc: 0.000000]\n",
      "453: [discriminator loss: 0.172096, acc: 0.968750] [adversarial loss: 1.937379, acc: 0.000000]\n",
      "454: [discriminator loss: 0.300553, acc: 0.898438] [adversarial loss: 0.141058, acc: 0.984375]\n",
      "455: [discriminator loss: 0.290577, acc: 0.835938] [adversarial loss: 5.778839, acc: 0.000000]\n",
      "456: [discriminator loss: 0.678136, acc: 0.718750] [adversarial loss: 0.009459, acc: 1.000000]\n",
      "457: [discriminator loss: 0.666145, acc: 0.617188] [adversarial loss: 4.221107, acc: 0.000000]\n",
      "458: [discriminator loss: 0.122567, acc: 0.945312] [adversarial loss: 2.694715, acc: 0.000000]\n",
      "459: [discriminator loss: 0.036490, acc: 0.992188] [adversarial loss: 2.552772, acc: 0.000000]\n",
      "460: [discriminator loss: 0.041889, acc: 0.992188] [adversarial loss: 2.437537, acc: 0.000000]\n",
      "461: [discriminator loss: 0.064587, acc: 0.984375] [adversarial loss: 2.419136, acc: 0.000000]\n",
      "462: [discriminator loss: 0.077342, acc: 0.984375] [adversarial loss: 2.476461, acc: 0.000000]\n",
      "463: [discriminator loss: 0.037787, acc: 0.992188] [adversarial loss: 2.667831, acc: 0.000000]\n",
      "464: [discriminator loss: 0.057358, acc: 0.984375] [adversarial loss: 2.472202, acc: 0.000000]\n",
      "465: [discriminator loss: 0.055023, acc: 0.984375] [adversarial loss: 2.269963, acc: 0.000000]\n",
      "466: [discriminator loss: 0.054949, acc: 1.000000] [adversarial loss: 3.161660, acc: 0.000000]\n",
      "467: [discriminator loss: 0.063424, acc: 0.984375] [adversarial loss: 1.866211, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468: [discriminator loss: 0.060190, acc: 0.984375] [adversarial loss: 2.287014, acc: 0.000000]\n",
      "469: [discriminator loss: 0.046150, acc: 0.992188] [adversarial loss: 2.437406, acc: 0.000000]\n",
      "470: [discriminator loss: 0.040908, acc: 0.984375] [adversarial loss: 2.591581, acc: 0.000000]\n",
      "471: [discriminator loss: 0.048787, acc: 0.992188] [adversarial loss: 2.626960, acc: 0.000000]\n",
      "472: [discriminator loss: 0.038304, acc: 1.000000] [adversarial loss: 3.180546, acc: 0.000000]\n",
      "473: [discriminator loss: 0.030438, acc: 0.992188] [adversarial loss: 3.260438, acc: 0.000000]\n",
      "474: [discriminator loss: 0.044122, acc: 0.984375] [adversarial loss: 3.452574, acc: 0.000000]\n",
      "475: [discriminator loss: 0.091627, acc: 0.992188] [adversarial loss: 4.768622, acc: 0.000000]\n",
      "476: [discriminator loss: 0.068633, acc: 0.976562] [adversarial loss: 0.621139, acc: 0.765625]\n",
      "477: [discriminator loss: 0.454464, acc: 0.640625] [adversarial loss: 15.418657, acc: 0.000000]\n",
      "478: [discriminator loss: 3.240179, acc: 0.500000] [adversarial loss: 2.372602, acc: 0.000000]\n",
      "479: [discriminator loss: 0.067550, acc: 1.000000] [adversarial loss: 3.642083, acc: 0.000000]\n",
      "480: [discriminator loss: 0.049697, acc: 0.992188] [adversarial loss: 2.952115, acc: 0.000000]\n",
      "481: [discriminator loss: 0.081929, acc: 0.992188] [adversarial loss: 4.233468, acc: 0.000000]\n",
      "482: [discriminator loss: 0.057029, acc: 0.992188] [adversarial loss: 2.997301, acc: 0.000000]\n",
      "483: [discriminator loss: 0.281809, acc: 0.859375] [adversarial loss: 5.831531, acc: 0.000000]\n",
      "484: [discriminator loss: 0.439999, acc: 0.789062] [adversarial loss: 0.010108, acc: 1.000000]\n",
      "485: [discriminator loss: 0.884582, acc: 0.554688] [adversarial loss: 6.772739, acc: 0.000000]\n",
      "486: [discriminator loss: 0.887712, acc: 0.601562] [adversarial loss: 0.011160, acc: 1.000000]\n",
      "487: [discriminator loss: 0.998659, acc: 0.539062] [adversarial loss: 5.394675, acc: 0.000000]\n",
      "488: [discriminator loss: 0.427437, acc: 0.765625] [adversarial loss: 0.622262, acc: 0.656250]\n",
      "489: [discriminator loss: 0.295536, acc: 0.875000] [adversarial loss: 4.118463, acc: 0.000000]\n",
      "490: [discriminator loss: 0.201840, acc: 0.921875] [adversarial loss: 1.100363, acc: 0.109375]\n",
      "491: [discriminator loss: 0.247434, acc: 0.960938] [adversarial loss: 4.141965, acc: 0.000000]\n",
      "492: [discriminator loss: 0.130512, acc: 0.953125] [adversarial loss: 1.835764, acc: 0.000000]\n",
      "493: [discriminator loss: 0.131157, acc: 0.984375] [adversarial loss: 2.787130, acc: 0.000000]\n",
      "494: [discriminator loss: 0.112005, acc: 0.968750] [adversarial loss: 1.777988, acc: 0.000000]\n",
      "495: [discriminator loss: 0.130755, acc: 0.984375] [adversarial loss: 3.231016, acc: 0.000000]\n",
      "496: [discriminator loss: 0.124707, acc: 0.968750] [adversarial loss: 2.160156, acc: 0.000000]\n",
      "497: [discriminator loss: 0.066724, acc: 1.000000] [adversarial loss: 3.843629, acc: 0.000000]\n",
      "498: [discriminator loss: 0.039088, acc: 0.992188] [adversarial loss: 3.574773, acc: 0.000000]\n",
      "499: [discriminator loss: 0.056780, acc: 0.992188] [adversarial loss: 3.182527, acc: 0.000000]\n",
      "500: [discriminator loss: 0.048055, acc: 0.992188] [adversarial loss: 3.331964, acc: 0.000000]\n",
      "501: [discriminator loss: 0.058054, acc: 0.984375] [adversarial loss: 2.387877, acc: 0.000000]\n",
      "502: [discriminator loss: 0.041806, acc: 1.000000] [adversarial loss: 1.299878, acc: 0.109375]\n",
      "503: [discriminator loss: 0.031147, acc: 1.000000] [adversarial loss: 0.643483, acc: 0.640625]\n",
      "504: [discriminator loss: 0.037108, acc: 1.000000] [adversarial loss: 0.655630, acc: 0.578125]\n",
      "505: [discriminator loss: 0.040302, acc: 1.000000] [adversarial loss: 0.777692, acc: 0.437500]\n",
      "506: [discriminator loss: 0.057448, acc: 0.984375] [adversarial loss: 0.572614, acc: 0.734375]\n",
      "507: [discriminator loss: 0.061060, acc: 0.984375] [adversarial loss: 1.567369, acc: 0.031250]\n",
      "508: [discriminator loss: 0.047963, acc: 0.992188] [adversarial loss: 0.750629, acc: 0.500000]\n",
      "509: [discriminator loss: 0.062046, acc: 0.992188] [adversarial loss: 1.923633, acc: 0.015625]\n",
      "510: [discriminator loss: 0.032052, acc: 1.000000] [adversarial loss: 2.269254, acc: 0.000000]\n",
      "511: [discriminator loss: 0.060433, acc: 0.976562] [adversarial loss: 1.042919, acc: 0.046875]\n",
      "512: [discriminator loss: 0.066396, acc: 0.992188] [adversarial loss: 3.304458, acc: 0.000000]\n",
      "513: [discriminator loss: 0.032281, acc: 0.992188] [adversarial loss: 1.748455, acc: 0.000000]\n",
      "514: [discriminator loss: 0.034204, acc: 0.992188] [adversarial loss: 1.075668, acc: 0.078125]\n",
      "515: [discriminator loss: 0.020803, acc: 1.000000] [adversarial loss: 0.914671, acc: 0.140625]\n",
      "516: [discriminator loss: 0.050111, acc: 0.984375] [adversarial loss: 0.852045, acc: 0.250000]\n",
      "517: [discriminator loss: 0.020970, acc: 1.000000] [adversarial loss: 1.287096, acc: 0.015625]\n",
      "518: [discriminator loss: 0.021547, acc: 1.000000] [adversarial loss: 0.647322, acc: 0.625000]\n",
      "519: [discriminator loss: 0.020222, acc: 0.992188] [adversarial loss: 0.778183, acc: 0.312500]\n",
      "520: [discriminator loss: 0.023867, acc: 0.992188] [adversarial loss: 0.876276, acc: 0.203125]\n",
      "521: [discriminator loss: 0.018520, acc: 1.000000] [adversarial loss: 0.832082, acc: 0.296875]\n",
      "522: [discriminator loss: 0.061819, acc: 0.984375] [adversarial loss: 0.706628, acc: 0.484375]\n",
      "523: [discriminator loss: 0.025053, acc: 1.000000] [adversarial loss: 2.054667, acc: 0.000000]\n",
      "524: [discriminator loss: 0.023522, acc: 0.992188] [adversarial loss: 0.688328, acc: 0.515625]\n",
      "525: [discriminator loss: 0.035791, acc: 0.992188] [adversarial loss: 0.129460, acc: 1.000000]\n",
      "526: [discriminator loss: 0.017885, acc: 1.000000] [adversarial loss: 0.136814, acc: 1.000000]\n",
      "527: [discriminator loss: 0.014224, acc: 1.000000] [adversarial loss: 0.334096, acc: 0.906250]\n",
      "528: [discriminator loss: 0.032350, acc: 0.984375] [adversarial loss: 0.026154, acc: 1.000000]\n",
      "529: [discriminator loss: 0.026263, acc: 0.992188] [adversarial loss: 0.148885, acc: 1.000000]\n",
      "530: [discriminator loss: 0.016520, acc: 1.000000] [adversarial loss: 0.036734, acc: 1.000000]\n",
      "531: [discriminator loss: 0.023376, acc: 0.992188] [adversarial loss: 0.031850, acc: 1.000000]\n",
      "532: [discriminator loss: 0.014130, acc: 1.000000] [adversarial loss: 0.262561, acc: 0.937500]\n",
      "533: [discriminator loss: 0.010880, acc: 1.000000] [adversarial loss: 0.397510, acc: 0.843750]\n",
      "534: [discriminator loss: 0.020842, acc: 1.000000] [adversarial loss: 0.652427, acc: 0.625000]\n",
      "535: [discriminator loss: 0.016845, acc: 1.000000] [adversarial loss: 0.222309, acc: 0.968750]\n",
      "536: [discriminator loss: 0.026014, acc: 0.992188] [adversarial loss: 0.110702, acc: 1.000000]\n",
      "537: [discriminator loss: 0.015356, acc: 1.000000] [adversarial loss: 0.933388, acc: 0.421875]\n",
      "538: [discriminator loss: 0.031690, acc: 0.984375] [adversarial loss: 0.001163, acc: 1.000000]\n",
      "539: [discriminator loss: 0.050841, acc: 0.984375] [adversarial loss: 2.812609, acc: 0.156250]\n",
      "540: [discriminator loss: 0.063126, acc: 0.968750] [adversarial loss: 0.000003, acc: 1.000000]\n",
      "541: [discriminator loss: 0.058262, acc: 0.984375] [adversarial loss: 0.005969, acc: 1.000000]\n",
      "542: [discriminator loss: 0.008305, acc: 1.000000] [adversarial loss: 0.003569, acc: 1.000000]\n",
      "543: [discriminator loss: 0.005586, acc: 1.000000] [adversarial loss: 0.003784, acc: 1.000000]\n",
      "544: [discriminator loss: 0.002206, acc: 1.000000] [adversarial loss: 0.004264, acc: 1.000000]\n",
      "545: [discriminator loss: 0.005853, acc: 1.000000] [adversarial loss: 0.004485, acc: 1.000000]\n",
      "546: [discriminator loss: 0.012172, acc: 1.000000] [adversarial loss: 0.070954, acc: 1.000000]\n",
      "547: [discriminator loss: 0.021989, acc: 1.000000] [adversarial loss: 0.004803, acc: 1.000000]\n",
      "548: [discriminator loss: 0.008929, acc: 1.000000] [adversarial loss: 0.055485, acc: 1.000000]\n",
      "549: [discriminator loss: 0.028156, acc: 0.984375] [adversarial loss: 0.000204, acc: 1.000000]\n",
      "550: [discriminator loss: 0.010657, acc: 1.000000] [adversarial loss: 0.005528, acc: 1.000000]\n",
      "551: [discriminator loss: 0.006700, acc: 1.000000] [adversarial loss: 0.006050, acc: 1.000000]\n",
      "552: [discriminator loss: 0.012616, acc: 1.000000] [adversarial loss: 0.015566, acc: 1.000000]\n",
      "553: [discriminator loss: 0.007040, acc: 1.000000] [adversarial loss: 0.146712, acc: 0.968750]\n",
      "554: [discriminator loss: 0.011830, acc: 1.000000] [adversarial loss: 0.561716, acc: 0.671875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555: [discriminator loss: 0.077576, acc: 0.960938] [adversarial loss: 6.487237, acc: 0.000000]\n",
      "556: [discriminator loss: 0.190959, acc: 0.945312] [adversarial loss: 0.000000, acc: 1.000000]\n",
      "557: [discriminator loss: 0.422930, acc: 0.843750] [adversarial loss: 0.586934, acc: 0.640625]\n",
      "558: [discriminator loss: 0.086539, acc: 0.960938] [adversarial loss: 0.049198, acc: 0.984375]\n",
      "559: [discriminator loss: 0.096785, acc: 0.960938] [adversarial loss: 1.191843, acc: 0.390625]\n",
      "560: [discriminator loss: 0.030975, acc: 0.992188] [adversarial loss: 0.318171, acc: 0.843750]\n",
      "561: [discriminator loss: 0.093274, acc: 0.968750] [adversarial loss: 0.001828, acc: 1.000000]\n",
      "562: [discriminator loss: 0.229589, acc: 0.921875] [adversarial loss: 10.135479, acc: 0.000000]\n",
      "563: [discriminator loss: 0.453462, acc: 0.843750] [adversarial loss: 0.000354, acc: 1.000000]\n",
      "564: [discriminator loss: 0.007077, acc: 0.992188] [adversarial loss: 0.000131, acc: 1.000000]\n",
      "565: [discriminator loss: 0.002389, acc: 1.000000] [adversarial loss: 0.000285, acc: 1.000000]\n",
      "566: [discriminator loss: 0.011830, acc: 1.000000] [adversarial loss: 0.000354, acc: 1.000000]\n",
      "567: [discriminator loss: 0.022896, acc: 0.984375] [adversarial loss: 0.000962, acc: 1.000000]\n",
      "568: [discriminator loss: 0.001418, acc: 1.000000] [adversarial loss: 0.002615, acc: 1.000000]\n",
      "569: [discriminator loss: 0.027410, acc: 0.992188] [adversarial loss: 0.001215, acc: 1.000000]\n",
      "570: [discriminator loss: 0.053232, acc: 0.976562] [adversarial loss: 0.000568, acc: 1.000000]\n",
      "571: [discriminator loss: 0.006661, acc: 1.000000] [adversarial loss: 0.000200, acc: 1.000000]\n",
      "572: [discriminator loss: 0.018835, acc: 0.984375] [adversarial loss: 0.003531, acc: 1.000000]\n",
      "573: [discriminator loss: 0.013833, acc: 0.992188] [adversarial loss: 0.007390, acc: 1.000000]\n",
      "574: [discriminator loss: 0.004496, acc: 1.000000] [adversarial loss: 0.002010, acc: 1.000000]\n",
      "575: [discriminator loss: 0.025952, acc: 0.984375] [adversarial loss: 0.007099, acc: 1.000000]\n",
      "576: [discriminator loss: 0.045749, acc: 0.976562] [adversarial loss: 0.003532, acc: 1.000000]\n",
      "577: [discriminator loss: 0.012474, acc: 1.000000] [adversarial loss: 0.008816, acc: 1.000000]\n",
      "578: [discriminator loss: 0.028334, acc: 0.992188] [adversarial loss: 0.007919, acc: 1.000000]\n",
      "579: [discriminator loss: 0.045440, acc: 0.992188] [adversarial loss: 0.022681, acc: 1.000000]\n",
      "580: [discriminator loss: 0.036284, acc: 0.976562] [adversarial loss: 0.026452, acc: 1.000000]\n",
      "581: [discriminator loss: 0.038591, acc: 0.984375] [adversarial loss: 0.033576, acc: 0.984375]\n",
      "582: [discriminator loss: 0.106354, acc: 0.984375] [adversarial loss: 0.128889, acc: 0.953125]\n",
      "583: [discriminator loss: 0.149976, acc: 0.945312] [adversarial loss: 2.384336, acc: 0.156250]\n",
      "584: [discriminator loss: 0.069793, acc: 0.960938] [adversarial loss: 0.010782, acc: 1.000000]\n",
      "585: [discriminator loss: 0.272442, acc: 0.906250] [adversarial loss: 7.825805, acc: 0.015625]\n",
      "586: [discriminator loss: 0.894642, acc: 0.750000] [adversarial loss: 0.000060, acc: 1.000000]\n",
      "587: [discriminator loss: 0.129784, acc: 0.968750] [adversarial loss: 0.000618, acc: 1.000000]\n",
      "588: [discriminator loss: 0.147339, acc: 0.929688] [adversarial loss: 0.014011, acc: 1.000000]\n",
      "589: [discriminator loss: 0.060582, acc: 0.984375] [adversarial loss: 0.009681, acc: 1.000000]\n",
      "590: [discriminator loss: 0.087134, acc: 0.953125] [adversarial loss: 0.033373, acc: 1.000000]\n",
      "591: [discriminator loss: 0.111978, acc: 0.960938] [adversarial loss: 0.008826, acc: 1.000000]\n",
      "592: [discriminator loss: 0.063041, acc: 0.976562] [adversarial loss: 0.064963, acc: 0.984375]\n",
      "593: [discriminator loss: 0.130726, acc: 0.976562] [adversarial loss: 0.160481, acc: 0.953125]\n",
      "594: [discriminator loss: 0.337106, acc: 0.890625] [adversarial loss: 3.602496, acc: 0.062500]\n",
      "595: [discriminator loss: 0.629823, acc: 0.843750] [adversarial loss: 0.117604, acc: 1.000000]\n",
      "596: [discriminator loss: 1.367494, acc: 0.546875] [adversarial loss: 13.843956, acc: 0.000000]\n",
      "597: [discriminator loss: 2.739304, acc: 0.515625] [adversarial loss: 2.603605, acc: 0.140625]\n",
      "598: [discriminator loss: 0.315317, acc: 0.859375] [adversarial loss: 1.131528, acc: 0.406250]\n",
      "599: [discriminator loss: 0.270270, acc: 0.906250] [adversarial loss: 2.221811, acc: 0.062500]\n",
      "600: [discriminator loss: 0.427779, acc: 0.875000] [adversarial loss: 2.622269, acc: 0.093750]\n",
      "601: [discriminator loss: 0.466363, acc: 0.820312] [adversarial loss: 2.448211, acc: 0.046875]\n",
      "602: [discriminator loss: 0.503061, acc: 0.820312] [adversarial loss: 1.145735, acc: 0.375000]\n",
      "603: [discriminator loss: 0.444044, acc: 0.820312] [adversarial loss: 3.651200, acc: 0.031250]\n",
      "604: [discriminator loss: 0.591506, acc: 0.757812] [adversarial loss: 0.431273, acc: 0.859375]\n",
      "605: [discriminator loss: 0.726176, acc: 0.648438] [adversarial loss: 5.059147, acc: 0.000000]\n",
      "606: [discriminator loss: 0.871286, acc: 0.695312] [adversarial loss: 0.487078, acc: 0.781250]\n",
      "607: [discriminator loss: 0.560081, acc: 0.687500] [adversarial loss: 3.861995, acc: 0.031250]\n",
      "608: [discriminator loss: 0.451184, acc: 0.789062] [adversarial loss: 1.311979, acc: 0.140625]\n",
      "609: [discriminator loss: 0.535501, acc: 0.812500] [adversarial loss: 4.489295, acc: 0.000000]\n",
      "610: [discriminator loss: 0.447993, acc: 0.812500] [adversarial loss: 0.942949, acc: 0.281250]\n",
      "611: [discriminator loss: 0.466370, acc: 0.828125] [adversarial loss: 3.337320, acc: 0.062500]\n",
      "612: [discriminator loss: 0.392039, acc: 0.835938] [adversarial loss: 0.977556, acc: 0.328125]\n",
      "613: [discriminator loss: 0.319282, acc: 0.898438] [adversarial loss: 2.108666, acc: 0.062500]\n",
      "614: [discriminator loss: 0.318159, acc: 0.882812] [adversarial loss: 1.191188, acc: 0.281250]\n",
      "615: [discriminator loss: 0.285854, acc: 0.890625] [adversarial loss: 1.529979, acc: 0.125000]\n",
      "616: [discriminator loss: 0.290900, acc: 0.890625] [adversarial loss: 0.480858, acc: 0.765625]\n",
      "617: [discriminator loss: 0.387329, acc: 0.804688] [adversarial loss: 2.720765, acc: 0.015625]\n",
      "618: [discriminator loss: 0.298611, acc: 0.875000] [adversarial loss: 0.475038, acc: 0.796875]\n",
      "619: [discriminator loss: 0.391670, acc: 0.859375] [adversarial loss: 2.573992, acc: 0.031250]\n",
      "620: [discriminator loss: 0.305091, acc: 0.851562] [adversarial loss: 0.414598, acc: 0.796875]\n",
      "621: [discriminator loss: 0.308155, acc: 0.859375] [adversarial loss: 1.752439, acc: 0.125000]\n",
      "622: [discriminator loss: 0.246335, acc: 0.859375] [adversarial loss: 0.363104, acc: 0.875000]\n",
      "623: [discriminator loss: 0.271148, acc: 0.906250] [adversarial loss: 2.482198, acc: 0.093750]\n",
      "624: [discriminator loss: 0.433987, acc: 0.835938] [adversarial loss: 0.175091, acc: 0.984375]\n",
      "625: [discriminator loss: 0.354662, acc: 0.851562] [adversarial loss: 2.727125, acc: 0.031250]\n",
      "626: [discriminator loss: 0.411643, acc: 0.796875] [adversarial loss: 0.142500, acc: 1.000000]\n",
      "627: [discriminator loss: 0.230422, acc: 0.906250] [adversarial loss: 1.390095, acc: 0.218750]\n",
      "628: [discriminator loss: 0.213849, acc: 0.914062] [adversarial loss: 0.664856, acc: 0.546875]\n",
      "629: [discriminator loss: 0.219289, acc: 0.937500] [adversarial loss: 1.119157, acc: 0.296875]\n",
      "630: [discriminator loss: 0.205968, acc: 0.921875] [adversarial loss: 0.810140, acc: 0.406250]\n",
      "631: [discriminator loss: 0.258635, acc: 0.851562] [adversarial loss: 0.463170, acc: 0.718750]\n",
      "632: [discriminator loss: 0.184027, acc: 0.968750] [adversarial loss: 0.494477, acc: 0.734375]\n",
      "633: [discriminator loss: 0.164560, acc: 0.921875] [adversarial loss: 0.857768, acc: 0.531250]\n",
      "634: [discriminator loss: 0.256846, acc: 0.875000] [adversarial loss: 0.034589, acc: 1.000000]\n",
      "635: [discriminator loss: 0.174316, acc: 0.960938] [adversarial loss: 0.694940, acc: 0.578125]\n",
      "636: [discriminator loss: 0.154504, acc: 0.929688] [adversarial loss: 0.120621, acc: 1.000000]\n",
      "637: [discriminator loss: 0.187443, acc: 0.914062] [adversarial loss: 0.808337, acc: 0.500000]\n",
      "638: [discriminator loss: 0.177003, acc: 0.929688] [adversarial loss: 0.045393, acc: 1.000000]\n",
      "639: [discriminator loss: 0.174332, acc: 0.937500] [adversarial loss: 0.554270, acc: 0.687500]\n",
      "640: [discriminator loss: 0.139252, acc: 0.960938] [adversarial loss: 0.057599, acc: 1.000000]\n",
      "641: [discriminator loss: 0.166590, acc: 0.945312] [adversarial loss: 0.744668, acc: 0.625000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642: [discriminator loss: 0.163224, acc: 0.945312] [adversarial loss: 0.013014, acc: 1.000000]\n",
      "643: [discriminator loss: 0.172892, acc: 0.937500] [adversarial loss: 0.175371, acc: 0.968750]\n",
      "644: [discriminator loss: 0.111973, acc: 0.968750] [adversarial loss: 0.258387, acc: 0.921875]\n",
      "645: [discriminator loss: 0.203084, acc: 0.921875] [adversarial loss: 0.432226, acc: 0.843750]\n",
      "646: [discriminator loss: 0.173242, acc: 0.953125] [adversarial loss: 0.064774, acc: 1.000000]\n",
      "647: [discriminator loss: 0.196164, acc: 0.929688] [adversarial loss: 2.602017, acc: 0.078125]\n",
      "648: [discriminator loss: 0.399562, acc: 0.820312] [adversarial loss: 0.000397, acc: 1.000000]\n",
      "649: [discriminator loss: 0.593941, acc: 0.726562] [adversarial loss: 1.199471, acc: 0.328125]\n",
      "650: [discriminator loss: 0.342314, acc: 0.859375] [adversarial loss: 0.798626, acc: 0.546875]\n",
      "651: [discriminator loss: 0.413610, acc: 0.835938] [adversarial loss: 3.147089, acc: 0.000000]\n",
      "652: [discriminator loss: 0.328154, acc: 0.859375] [adversarial loss: 0.079140, acc: 1.000000]\n",
      "653: [discriminator loss: 0.424378, acc: 0.765625] [adversarial loss: 3.458793, acc: 0.000000]\n",
      "654: [discriminator loss: 0.562728, acc: 0.757812] [adversarial loss: 0.145097, acc: 1.000000]\n",
      "655: [discriminator loss: 0.818579, acc: 0.625000] [adversarial loss: 4.825200, acc: 0.000000]\n",
      "656: [discriminator loss: 0.638512, acc: 0.718750] [adversarial loss: 1.198398, acc: 0.265625]\n",
      "657: [discriminator loss: 0.415427, acc: 0.781250] [adversarial loss: 3.533004, acc: 0.015625]\n",
      "658: [discriminator loss: 0.416281, acc: 0.804688] [adversarial loss: 1.526290, acc: 0.171875]\n",
      "659: [discriminator loss: 0.367420, acc: 0.835938] [adversarial loss: 3.798333, acc: 0.015625]\n",
      "660: [discriminator loss: 0.348223, acc: 0.804688] [adversarial loss: 1.608419, acc: 0.125000]\n",
      "661: [discriminator loss: 0.340831, acc: 0.859375] [adversarial loss: 2.948842, acc: 0.000000]\n",
      "662: [discriminator loss: 0.416870, acc: 0.804688] [adversarial loss: 0.997931, acc: 0.359375]\n",
      "663: [discriminator loss: 0.518731, acc: 0.765625] [adversarial loss: 4.082693, acc: 0.000000]\n",
      "664: [discriminator loss: 0.535151, acc: 0.750000] [adversarial loss: 1.097797, acc: 0.281250]\n",
      "665: [discriminator loss: 0.295872, acc: 0.890625] [adversarial loss: 2.276075, acc: 0.062500]\n",
      "666: [discriminator loss: 0.306080, acc: 0.890625] [adversarial loss: 1.248565, acc: 0.234375]\n",
      "667: [discriminator loss: 0.314418, acc: 0.867188] [adversarial loss: 2.756048, acc: 0.000000]\n",
      "668: [discriminator loss: 0.362118, acc: 0.804688] [adversarial loss: 0.920951, acc: 0.531250]\n",
      "669: [discriminator loss: 0.411631, acc: 0.781250] [adversarial loss: 2.577404, acc: 0.031250]\n",
      "670: [discriminator loss: 0.238722, acc: 0.921875] [adversarial loss: 1.525572, acc: 0.140625]\n",
      "671: [discriminator loss: 0.270417, acc: 0.906250] [adversarial loss: 1.947901, acc: 0.109375]\n",
      "672: [discriminator loss: 0.271808, acc: 0.914062] [adversarial loss: 1.494748, acc: 0.187500]\n",
      "673: [discriminator loss: 0.290821, acc: 0.906250] [adversarial loss: 1.395509, acc: 0.250000]\n",
      "674: [discriminator loss: 0.285230, acc: 0.914062] [adversarial loss: 2.082870, acc: 0.078125]\n",
      "675: [discriminator loss: 0.271666, acc: 0.875000] [adversarial loss: 1.337920, acc: 0.171875]\n",
      "676: [discriminator loss: 0.306474, acc: 0.882812] [adversarial loss: 2.290959, acc: 0.031250]\n",
      "677: [discriminator loss: 0.241638, acc: 0.906250] [adversarial loss: 1.188994, acc: 0.312500]\n",
      "678: [discriminator loss: 0.344321, acc: 0.828125] [adversarial loss: 3.359657, acc: 0.015625]\n",
      "679: [discriminator loss: 0.423914, acc: 0.781250] [adversarial loss: 0.557734, acc: 0.718750]\n",
      "680: [discriminator loss: 0.617166, acc: 0.656250] [adversarial loss: 3.622486, acc: 0.000000]\n",
      "681: [discriminator loss: 0.521721, acc: 0.734375] [adversarial loss: 1.041330, acc: 0.328125]\n",
      "682: [discriminator loss: 0.387685, acc: 0.820312] [adversarial loss: 2.391332, acc: 0.031250]\n",
      "683: [discriminator loss: 0.405824, acc: 0.781250] [adversarial loss: 1.241102, acc: 0.265625]\n",
      "684: [discriminator loss: 0.419187, acc: 0.781250] [adversarial loss: 2.340343, acc: 0.031250]\n",
      "685: [discriminator loss: 0.392167, acc: 0.843750] [adversarial loss: 1.080036, acc: 0.328125]\n",
      "686: [discriminator loss: 0.552389, acc: 0.726562] [adversarial loss: 2.901098, acc: 0.000000]\n",
      "687: [discriminator loss: 0.698801, acc: 0.710938] [adversarial loss: 0.685790, acc: 0.640625]\n",
      "688: [discriminator loss: 0.539439, acc: 0.726562] [adversarial loss: 1.767802, acc: 0.078125]\n",
      "689: [discriminator loss: 0.476472, acc: 0.812500] [adversarial loss: 1.218977, acc: 0.312500]\n",
      "690: [discriminator loss: 0.447617, acc: 0.796875] [adversarial loss: 1.563293, acc: 0.171875]\n",
      "691: [discriminator loss: 0.512758, acc: 0.789062] [adversarial loss: 1.442439, acc: 0.218750]\n",
      "692: [discriminator loss: 0.431092, acc: 0.828125] [adversarial loss: 1.771302, acc: 0.062500]\n",
      "693: [discriminator loss: 0.488473, acc: 0.726562] [adversarial loss: 0.752413, acc: 0.593750]\n",
      "694: [discriminator loss: 0.569446, acc: 0.726562] [adversarial loss: 2.650148, acc: 0.015625]\n",
      "695: [discriminator loss: 0.714330, acc: 0.601562] [adversarial loss: 0.491507, acc: 0.703125]\n",
      "696: [discriminator loss: 0.856825, acc: 0.609375] [adversarial loss: 2.174379, acc: 0.015625]\n",
      "697: [discriminator loss: 0.565754, acc: 0.703125] [adversarial loss: 0.669004, acc: 0.609375]\n",
      "698: [discriminator loss: 0.563283, acc: 0.648438] [adversarial loss: 1.822742, acc: 0.031250]\n",
      "699: [discriminator loss: 0.501109, acc: 0.757812] [adversarial loss: 0.907892, acc: 0.296875]\n",
      "700: [discriminator loss: 0.513630, acc: 0.765625] [adversarial loss: 1.607335, acc: 0.046875]\n",
      "701: [discriminator loss: 0.506842, acc: 0.757812] [adversarial loss: 1.244743, acc: 0.140625]\n",
      "702: [discriminator loss: 0.467947, acc: 0.781250] [adversarial loss: 1.437526, acc: 0.156250]\n",
      "703: [discriminator loss: 0.467938, acc: 0.789062] [adversarial loss: 1.223025, acc: 0.187500]\n",
      "704: [discriminator loss: 0.464701, acc: 0.750000] [adversarial loss: 1.107333, acc: 0.281250]\n",
      "705: [discriminator loss: 0.509724, acc: 0.734375] [adversarial loss: 1.626958, acc: 0.015625]\n",
      "706: [discriminator loss: 0.449900, acc: 0.781250] [adversarial loss: 0.517724, acc: 0.703125]\n",
      "707: [discriminator loss: 0.653951, acc: 0.640625] [adversarial loss: 2.391881, acc: 0.000000]\n",
      "708: [discriminator loss: 0.603535, acc: 0.656250] [adversarial loss: 0.435129, acc: 0.750000]\n",
      "709: [discriminator loss: 0.595486, acc: 0.726562] [adversarial loss: 1.712998, acc: 0.031250]\n",
      "710: [discriminator loss: 0.536161, acc: 0.679688] [adversarial loss: 0.690673, acc: 0.531250]\n",
      "711: [discriminator loss: 0.530217, acc: 0.703125] [adversarial loss: 1.796669, acc: 0.000000]\n",
      "712: [discriminator loss: 0.458729, acc: 0.757812] [adversarial loss: 0.829131, acc: 0.437500]\n",
      "713: [discriminator loss: 0.423261, acc: 0.867188] [adversarial loss: 1.528489, acc: 0.078125]\n",
      "714: [discriminator loss: 0.428977, acc: 0.812500] [adversarial loss: 0.776919, acc: 0.468750]\n",
      "715: [discriminator loss: 0.404260, acc: 0.851562] [adversarial loss: 1.317821, acc: 0.125000]\n",
      "716: [discriminator loss: 0.441959, acc: 0.789062] [adversarial loss: 0.658181, acc: 0.578125]\n",
      "717: [discriminator loss: 0.457762, acc: 0.789062] [adversarial loss: 1.570110, acc: 0.031250]\n",
      "718: [discriminator loss: 0.481452, acc: 0.757812] [adversarial loss: 0.394531, acc: 0.843750]\n",
      "719: [discriminator loss: 0.460121, acc: 0.734375] [adversarial loss: 1.079060, acc: 0.234375]\n",
      "720: [discriminator loss: 0.455068, acc: 0.789062] [adversarial loss: 0.847525, acc: 0.437500]\n",
      "721: [discriminator loss: 0.466016, acc: 0.804688] [adversarial loss: 1.106216, acc: 0.187500]\n",
      "722: [discriminator loss: 0.408413, acc: 0.843750] [adversarial loss: 0.741906, acc: 0.453125]\n",
      "723: [discriminator loss: 0.415158, acc: 0.851562] [adversarial loss: 1.107955, acc: 0.296875]\n",
      "724: [discriminator loss: 0.389291, acc: 0.859375] [adversarial loss: 1.456200, acc: 0.031250]\n",
      "725: [discriminator loss: 0.446375, acc: 0.804688] [adversarial loss: 0.538533, acc: 0.703125]\n",
      "726: [discriminator loss: 0.613665, acc: 0.617188] [adversarial loss: 3.296140, acc: 0.000000]\n",
      "727: [discriminator loss: 0.948043, acc: 0.507812] [adversarial loss: 0.381014, acc: 0.875000]\n",
      "728: [discriminator loss: 0.541476, acc: 0.695312] [adversarial loss: 1.427475, acc: 0.109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729: [discriminator loss: 0.535673, acc: 0.742188] [adversarial loss: 0.862031, acc: 0.390625]\n",
      "730: [discriminator loss: 0.520656, acc: 0.734375] [adversarial loss: 1.328178, acc: 0.093750]\n",
      "731: [discriminator loss: 0.403647, acc: 0.875000] [adversarial loss: 0.938861, acc: 0.328125]\n",
      "732: [discriminator loss: 0.470001, acc: 0.804688] [adversarial loss: 1.576409, acc: 0.046875]\n",
      "733: [discriminator loss: 0.461027, acc: 0.796875] [adversarial loss: 0.947863, acc: 0.312500]\n",
      "734: [discriminator loss: 0.471182, acc: 0.750000] [adversarial loss: 1.853453, acc: 0.031250]\n",
      "735: [discriminator loss: 0.474026, acc: 0.757812] [adversarial loss: 0.537394, acc: 0.703125]\n",
      "736: [discriminator loss: 0.561533, acc: 0.703125] [adversarial loss: 2.289829, acc: 0.000000]\n",
      "737: [discriminator loss: 0.623714, acc: 0.625000] [adversarial loss: 0.512449, acc: 0.796875]\n",
      "738: [discriminator loss: 0.665038, acc: 0.632812] [adversarial loss: 2.021529, acc: 0.000000]\n",
      "739: [discriminator loss: 0.647657, acc: 0.625000] [adversarial loss: 0.707093, acc: 0.578125]\n",
      "740: [discriminator loss: 0.603453, acc: 0.648438] [adversarial loss: 1.855478, acc: 0.015625]\n",
      "741: [discriminator loss: 0.535874, acc: 0.710938] [adversarial loss: 0.807165, acc: 0.453125]\n",
      "742: [discriminator loss: 0.475159, acc: 0.773438] [adversarial loss: 1.355580, acc: 0.046875]\n",
      "743: [discriminator loss: 0.541762, acc: 0.765625] [adversarial loss: 0.859388, acc: 0.421875]\n",
      "744: [discriminator loss: 0.498900, acc: 0.765625] [adversarial loss: 0.936662, acc: 0.250000]\n",
      "745: [discriminator loss: 0.460657, acc: 0.781250] [adversarial loss: 1.018667, acc: 0.250000]\n",
      "746: [discriminator loss: 0.521506, acc: 0.750000] [adversarial loss: 0.796860, acc: 0.453125]\n",
      "747: [discriminator loss: 0.583594, acc: 0.671875] [adversarial loss: 1.858257, acc: 0.015625]\n",
      "748: [discriminator loss: 0.624636, acc: 0.679688] [adversarial loss: 0.478052, acc: 0.750000]\n",
      "749: [discriminator loss: 0.516709, acc: 0.742188] [adversarial loss: 1.563690, acc: 0.015625]\n",
      "750: [discriminator loss: 0.567870, acc: 0.664062] [adversarial loss: 0.502119, acc: 0.812500]\n",
      "751: [discriminator loss: 0.505656, acc: 0.726562] [adversarial loss: 1.570801, acc: 0.000000]\n",
      "752: [discriminator loss: 0.541099, acc: 0.710938] [adversarial loss: 0.586309, acc: 0.671875]\n",
      "753: [discriminator loss: 0.595309, acc: 0.664062] [adversarial loss: 1.700871, acc: 0.031250]\n",
      "754: [discriminator loss: 0.581880, acc: 0.671875] [adversarial loss: 0.659428, acc: 0.546875]\n",
      "755: [discriminator loss: 0.527855, acc: 0.710938] [adversarial loss: 1.606433, acc: 0.031250]\n",
      "756: [discriminator loss: 0.508800, acc: 0.734375] [adversarial loss: 0.521386, acc: 0.734375]\n",
      "757: [discriminator loss: 0.466095, acc: 0.765625] [adversarial loss: 1.035196, acc: 0.250000]\n",
      "758: [discriminator loss: 0.454075, acc: 0.796875] [adversarial loss: 0.509161, acc: 0.765625]\n",
      "759: [discriminator loss: 0.474777, acc: 0.789062] [adversarial loss: 1.237760, acc: 0.109375]\n",
      "760: [discriminator loss: 0.528163, acc: 0.789062] [adversarial loss: 0.598712, acc: 0.671875]\n",
      "761: [discriminator loss: 0.559391, acc: 0.710938] [adversarial loss: 1.486942, acc: 0.015625]\n",
      "762: [discriminator loss: 0.482115, acc: 0.781250] [adversarial loss: 0.576428, acc: 0.687500]\n",
      "763: [discriminator loss: 0.509449, acc: 0.804688] [adversarial loss: 1.475339, acc: 0.046875]\n",
      "764: [discriminator loss: 0.554051, acc: 0.742188] [adversarial loss: 0.561659, acc: 0.765625]\n",
      "765: [discriminator loss: 0.587053, acc: 0.632812] [adversarial loss: 2.157143, acc: 0.000000]\n",
      "766: [discriminator loss: 0.678703, acc: 0.585938] [adversarial loss: 0.445842, acc: 0.781250]\n",
      "767: [discriminator loss: 0.570869, acc: 0.664062] [adversarial loss: 1.377811, acc: 0.046875]\n",
      "768: [discriminator loss: 0.569181, acc: 0.750000] [adversarial loss: 0.758916, acc: 0.484375]\n",
      "769: [discriminator loss: 0.543001, acc: 0.687500] [adversarial loss: 1.159106, acc: 0.140625]\n",
      "770: [discriminator loss: 0.513873, acc: 0.789062] [adversarial loss: 1.085491, acc: 0.203125]\n",
      "771: [discriminator loss: 0.549906, acc: 0.718750] [adversarial loss: 0.841107, acc: 0.406250]\n",
      "772: [discriminator loss: 0.494028, acc: 0.757812] [adversarial loss: 1.101634, acc: 0.093750]\n",
      "773: [discriminator loss: 0.520756, acc: 0.773438] [adversarial loss: 0.771638, acc: 0.484375]\n",
      "774: [discriminator loss: 0.523285, acc: 0.726562] [adversarial loss: 1.114830, acc: 0.187500]\n",
      "775: [discriminator loss: 0.455051, acc: 0.804688] [adversarial loss: 0.433123, acc: 0.859375]\n",
      "776: [discriminator loss: 0.491384, acc: 0.757812] [adversarial loss: 1.233516, acc: 0.062500]\n",
      "777: [discriminator loss: 0.551889, acc: 0.757812] [adversarial loss: 0.581838, acc: 0.671875]\n",
      "778: [discriminator loss: 0.600603, acc: 0.625000] [adversarial loss: 1.983019, acc: 0.000000]\n",
      "779: [discriminator loss: 0.601411, acc: 0.617188] [adversarial loss: 0.427307, acc: 0.812500]\n",
      "780: [discriminator loss: 0.636456, acc: 0.617188] [adversarial loss: 2.096445, acc: 0.000000]\n",
      "781: [discriminator loss: 0.687418, acc: 0.570312] [adversarial loss: 0.680652, acc: 0.593750]\n",
      "782: [discriminator loss: 0.420794, acc: 0.828125] [adversarial loss: 0.738021, acc: 0.531250]\n",
      "783: [discriminator loss: 0.453003, acc: 0.828125] [adversarial loss: 0.833968, acc: 0.390625]\n",
      "784: [discriminator loss: 0.477816, acc: 0.796875] [adversarial loss: 0.534653, acc: 0.671875]\n",
      "785: [discriminator loss: 0.462365, acc: 0.804688] [adversarial loss: 0.983204, acc: 0.234375]\n",
      "786: [discriminator loss: 0.487767, acc: 0.789062] [adversarial loss: 0.499044, acc: 0.812500]\n",
      "787: [discriminator loss: 0.522659, acc: 0.726562] [adversarial loss: 1.287417, acc: 0.109375]\n",
      "788: [discriminator loss: 0.594992, acc: 0.664062] [adversarial loss: 0.391254, acc: 0.921875]\n",
      "789: [discriminator loss: 0.628143, acc: 0.601562] [adversarial loss: 1.874058, acc: 0.000000]\n",
      "790: [discriminator loss: 0.667695, acc: 0.546875] [adversarial loss: 0.592317, acc: 0.671875]\n",
      "791: [discriminator loss: 0.603559, acc: 0.671875] [adversarial loss: 1.238279, acc: 0.078125]\n",
      "792: [discriminator loss: 0.579423, acc: 0.679688] [adversarial loss: 0.558852, acc: 0.734375]\n",
      "793: [discriminator loss: 0.536331, acc: 0.703125] [adversarial loss: 1.287521, acc: 0.031250]\n",
      "794: [discriminator loss: 0.509679, acc: 0.757812] [adversarial loss: 0.645182, acc: 0.609375]\n",
      "795: [discriminator loss: 0.534011, acc: 0.703125] [adversarial loss: 1.064139, acc: 0.187500]\n",
      "796: [discriminator loss: 0.528865, acc: 0.765625] [adversarial loss: 0.522905, acc: 0.812500]\n",
      "797: [discriminator loss: 0.608027, acc: 0.632812] [adversarial loss: 1.338694, acc: 0.015625]\n",
      "798: [discriminator loss: 0.575762, acc: 0.687500] [adversarial loss: 0.539747, acc: 0.750000]\n",
      "799: [discriminator loss: 0.648586, acc: 0.625000] [adversarial loss: 1.622154, acc: 0.031250]\n",
      "800: [discriminator loss: 0.574293, acc: 0.632812] [adversarial loss: 0.374048, acc: 0.890625]\n",
      "801: [discriminator loss: 0.622235, acc: 0.664062] [adversarial loss: 1.377952, acc: 0.062500]\n",
      "802: [discriminator loss: 0.530443, acc: 0.742188] [adversarial loss: 0.688464, acc: 0.593750]\n",
      "803: [discriminator loss: 0.531597, acc: 0.703125] [adversarial loss: 1.196551, acc: 0.062500]\n",
      "804: [discriminator loss: 0.543495, acc: 0.742188] [adversarial loss: 0.645827, acc: 0.609375]\n",
      "805: [discriminator loss: 0.559566, acc: 0.710938] [adversarial loss: 1.159301, acc: 0.093750]\n",
      "806: [discriminator loss: 0.526975, acc: 0.718750] [adversarial loss: 0.439655, acc: 0.843750]\n",
      "807: [discriminator loss: 0.540115, acc: 0.695312] [adversarial loss: 0.954635, acc: 0.265625]\n",
      "808: [discriminator loss: 0.607881, acc: 0.640625] [adversarial loss: 0.658320, acc: 0.625000]\n",
      "809: [discriminator loss: 0.565727, acc: 0.695312] [adversarial loss: 1.424027, acc: 0.015625]\n",
      "810: [discriminator loss: 0.572112, acc: 0.710938] [adversarial loss: 0.489978, acc: 0.796875]\n",
      "811: [discriminator loss: 0.572855, acc: 0.679688] [adversarial loss: 1.311350, acc: 0.078125]\n",
      "812: [discriminator loss: 0.678326, acc: 0.585938] [adversarial loss: 0.455075, acc: 0.828125]\n",
      "813: [discriminator loss: 0.604402, acc: 0.546875] [adversarial loss: 1.619325, acc: 0.000000]\n",
      "814: [discriminator loss: 0.649973, acc: 0.578125] [adversarial loss: 0.407459, acc: 0.937500]\n",
      "815: [discriminator loss: 0.608991, acc: 0.632812] [adversarial loss: 1.163219, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816: [discriminator loss: 0.507167, acc: 0.781250] [adversarial loss: 0.734378, acc: 0.578125]\n",
      "817: [discriminator loss: 0.591095, acc: 0.632812] [adversarial loss: 1.374349, acc: 0.000000]\n",
      "818: [discriminator loss: 0.585234, acc: 0.687500] [adversarial loss: 0.614275, acc: 0.718750]\n",
      "819: [discriminator loss: 0.540078, acc: 0.726562] [adversarial loss: 0.834790, acc: 0.406250]\n",
      "820: [discriminator loss: 0.501774, acc: 0.820312] [adversarial loss: 1.063077, acc: 0.203125]\n",
      "821: [discriminator loss: 0.544668, acc: 0.750000] [adversarial loss: 0.735334, acc: 0.453125]\n",
      "822: [discriminator loss: 0.485062, acc: 0.796875] [adversarial loss: 0.971611, acc: 0.187500]\n",
      "823: [discriminator loss: 0.591061, acc: 0.703125] [adversarial loss: 0.633444, acc: 0.640625]\n",
      "824: [discriminator loss: 0.561569, acc: 0.726562] [adversarial loss: 1.358964, acc: 0.000000]\n",
      "825: [discriminator loss: 0.526953, acc: 0.687500] [adversarial loss: 0.263623, acc: 0.968750]\n",
      "826: [discriminator loss: 0.616062, acc: 0.625000] [adversarial loss: 1.330291, acc: 0.046875]\n",
      "827: [discriminator loss: 0.567423, acc: 0.687500] [adversarial loss: 0.498104, acc: 0.796875]\n",
      "828: [discriminator loss: 0.640271, acc: 0.601562] [adversarial loss: 1.743443, acc: 0.000000]\n",
      "829: [discriminator loss: 0.616604, acc: 0.625000] [adversarial loss: 0.532607, acc: 0.828125]\n",
      "830: [discriminator loss: 0.607263, acc: 0.648438] [adversarial loss: 1.168971, acc: 0.109375]\n",
      "831: [discriminator loss: 0.564567, acc: 0.687500] [adversarial loss: 0.573827, acc: 0.765625]\n",
      "832: [discriminator loss: 0.559509, acc: 0.687500] [adversarial loss: 1.227762, acc: 0.078125]\n",
      "833: [discriminator loss: 0.525630, acc: 0.781250] [adversarial loss: 0.794075, acc: 0.281250]\n",
      "834: [discriminator loss: 0.513512, acc: 0.757812] [adversarial loss: 0.944303, acc: 0.312500]\n",
      "835: [discriminator loss: 0.508573, acc: 0.789062] [adversarial loss: 0.636208, acc: 0.625000]\n",
      "836: [discriminator loss: 0.533504, acc: 0.742188] [adversarial loss: 1.166122, acc: 0.125000]\n",
      "837: [discriminator loss: 0.529902, acc: 0.734375] [adversarial loss: 0.469054, acc: 0.843750]\n",
      "838: [discriminator loss: 0.569047, acc: 0.671875] [adversarial loss: 1.518967, acc: 0.031250]\n",
      "839: [discriminator loss: 0.585706, acc: 0.648438] [adversarial loss: 0.291084, acc: 0.968750]\n",
      "840: [discriminator loss: 0.563854, acc: 0.609375] [adversarial loss: 1.144109, acc: 0.062500]\n",
      "841: [discriminator loss: 0.571277, acc: 0.703125] [adversarial loss: 0.571966, acc: 0.703125]\n",
      "842: [discriminator loss: 0.596913, acc: 0.632812] [adversarial loss: 1.800376, acc: 0.000000]\n",
      "843: [discriminator loss: 0.612974, acc: 0.593750] [adversarial loss: 0.503769, acc: 0.781250]\n",
      "844: [discriminator loss: 0.609723, acc: 0.601562] [adversarial loss: 1.257147, acc: 0.062500]\n",
      "845: [discriminator loss: 0.574301, acc: 0.687500] [adversarial loss: 0.747139, acc: 0.484375]\n",
      "846: [discriminator loss: 0.568009, acc: 0.695312] [adversarial loss: 1.574080, acc: 0.015625]\n",
      "847: [discriminator loss: 0.592563, acc: 0.679688] [adversarial loss: 0.727586, acc: 0.562500]\n",
      "848: [discriminator loss: 0.582592, acc: 0.617188] [adversarial loss: 1.557619, acc: 0.015625]\n",
      "849: [discriminator loss: 0.547877, acc: 0.718750] [adversarial loss: 0.780290, acc: 0.406250]\n",
      "850: [discriminator loss: 0.479731, acc: 0.812500] [adversarial loss: 1.129802, acc: 0.156250]\n",
      "851: [discriminator loss: 0.545037, acc: 0.750000] [adversarial loss: 0.876234, acc: 0.250000]\n",
      "852: [discriminator loss: 0.478240, acc: 0.835938] [adversarial loss: 1.054998, acc: 0.125000]\n",
      "853: [discriminator loss: 0.542044, acc: 0.726562] [adversarial loss: 0.627916, acc: 0.671875]\n",
      "854: [discriminator loss: 0.549173, acc: 0.726562] [adversarial loss: 1.296260, acc: 0.078125]\n",
      "855: [discriminator loss: 0.552621, acc: 0.781250] [adversarial loss: 0.798431, acc: 0.421875]\n",
      "856: [discriminator loss: 0.594857, acc: 0.664062] [adversarial loss: 1.422096, acc: 0.031250]\n",
      "857: [discriminator loss: 0.539298, acc: 0.734375] [adversarial loss: 0.540719, acc: 0.781250]\n",
      "858: [discriminator loss: 0.712785, acc: 0.609375] [adversarial loss: 1.698773, acc: 0.000000]\n",
      "859: [discriminator loss: 0.645445, acc: 0.648438] [adversarial loss: 0.613132, acc: 0.703125]\n",
      "860: [discriminator loss: 0.607814, acc: 0.625000] [adversarial loss: 1.548168, acc: 0.015625]\n",
      "861: [discriminator loss: 0.625251, acc: 0.593750] [adversarial loss: 0.594886, acc: 0.703125]\n",
      "862: [discriminator loss: 0.644315, acc: 0.601562] [adversarial loss: 1.314599, acc: 0.031250]\n",
      "863: [discriminator loss: 0.625139, acc: 0.703125] [adversarial loss: 0.786000, acc: 0.437500]\n",
      "864: [discriminator loss: 0.569114, acc: 0.710938] [adversarial loss: 1.152999, acc: 0.093750]\n",
      "865: [discriminator loss: 0.521045, acc: 0.726562] [adversarial loss: 0.684569, acc: 0.562500]\n",
      "866: [discriminator loss: 0.498995, acc: 0.781250] [adversarial loss: 0.742098, acc: 0.515625]\n",
      "867: [discriminator loss: 0.564461, acc: 0.750000] [adversarial loss: 0.727576, acc: 0.531250]\n",
      "868: [discriminator loss: 0.571675, acc: 0.695312] [adversarial loss: 1.122612, acc: 0.187500]\n",
      "869: [discriminator loss: 0.574013, acc: 0.664062] [adversarial loss: 0.518088, acc: 0.812500]\n",
      "870: [discriminator loss: 0.591523, acc: 0.664062] [adversarial loss: 1.358358, acc: 0.031250]\n",
      "871: [discriminator loss: 0.560560, acc: 0.687500] [adversarial loss: 0.342590, acc: 0.937500]\n",
      "872: [discriminator loss: 0.706234, acc: 0.585938] [adversarial loss: 1.517833, acc: 0.031250]\n",
      "873: [discriminator loss: 0.666693, acc: 0.617188] [adversarial loss: 0.649382, acc: 0.578125]\n",
      "874: [discriminator loss: 0.567509, acc: 0.648438] [adversarial loss: 1.327795, acc: 0.031250]\n",
      "875: [discriminator loss: 0.568778, acc: 0.710938] [adversarial loss: 0.791138, acc: 0.531250]\n",
      "876: [discriminator loss: 0.539265, acc: 0.734375] [adversarial loss: 1.147611, acc: 0.156250]\n",
      "877: [discriminator loss: 0.545790, acc: 0.718750] [adversarial loss: 0.742201, acc: 0.484375]\n",
      "878: [discriminator loss: 0.542917, acc: 0.750000] [adversarial loss: 1.288160, acc: 0.093750]\n",
      "879: [discriminator loss: 0.591024, acc: 0.648438] [adversarial loss: 0.632038, acc: 0.625000]\n",
      "880: [discriminator loss: 0.623053, acc: 0.640625] [adversarial loss: 1.744952, acc: 0.000000]\n",
      "881: [discriminator loss: 0.608454, acc: 0.632812] [adversarial loss: 0.676258, acc: 0.546875]\n",
      "882: [discriminator loss: 0.586194, acc: 0.718750] [adversarial loss: 0.963716, acc: 0.234375]\n",
      "883: [discriminator loss: 0.603976, acc: 0.664062] [adversarial loss: 0.955709, acc: 0.187500]\n",
      "884: [discriminator loss: 0.578820, acc: 0.718750] [adversarial loss: 0.838023, acc: 0.359375]\n",
      "885: [discriminator loss: 0.546348, acc: 0.742188] [adversarial loss: 1.221596, acc: 0.062500]\n",
      "886: [discriminator loss: 0.623032, acc: 0.679688] [adversarial loss: 0.745639, acc: 0.453125]\n",
      "887: [discriminator loss: 0.612093, acc: 0.625000] [adversarial loss: 1.344574, acc: 0.031250]\n",
      "888: [discriminator loss: 0.586946, acc: 0.671875] [adversarial loss: 0.656198, acc: 0.625000]\n",
      "889: [discriminator loss: 0.684624, acc: 0.601562] [adversarial loss: 1.661233, acc: 0.000000]\n",
      "890: [discriminator loss: 0.734869, acc: 0.562500] [adversarial loss: 0.486567, acc: 0.828125]\n",
      "891: [discriminator loss: 0.608086, acc: 0.617188] [adversarial loss: 1.244722, acc: 0.093750]\n",
      "892: [discriminator loss: 0.602276, acc: 0.648438] [adversarial loss: 0.768902, acc: 0.515625]\n",
      "893: [discriminator loss: 0.609994, acc: 0.640625] [adversarial loss: 1.176339, acc: 0.031250]\n",
      "894: [discriminator loss: 0.593656, acc: 0.671875] [adversarial loss: 0.694243, acc: 0.515625]\n",
      "895: [discriminator loss: 0.618730, acc: 0.648438] [adversarial loss: 0.998963, acc: 0.187500]\n",
      "896: [discriminator loss: 0.586723, acc: 0.664062] [adversarial loss: 0.875917, acc: 0.250000]\n",
      "897: [discriminator loss: 0.602054, acc: 0.710938] [adversarial loss: 0.942838, acc: 0.187500]\n",
      "898: [discriminator loss: 0.539302, acc: 0.781250] [adversarial loss: 1.079296, acc: 0.156250]\n",
      "899: [discriminator loss: 0.535877, acc: 0.773438] [adversarial loss: 1.093647, acc: 0.156250]\n",
      "900: [discriminator loss: 0.500078, acc: 0.742188] [adversarial loss: 0.540225, acc: 0.703125]\n",
      "901: [discriminator loss: 0.616205, acc: 0.609375] [adversarial loss: 1.103446, acc: 0.046875]\n",
      "902: [discriminator loss: 0.592291, acc: 0.695312] [adversarial loss: 0.542379, acc: 0.828125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903: [discriminator loss: 0.613418, acc: 0.648438] [adversarial loss: 1.295566, acc: 0.031250]\n",
      "904: [discriminator loss: 0.619476, acc: 0.648438] [adversarial loss: 0.634822, acc: 0.640625]\n",
      "905: [discriminator loss: 0.637324, acc: 0.578125] [adversarial loss: 1.853277, acc: 0.000000]\n",
      "906: [discriminator loss: 0.685574, acc: 0.562500] [adversarial loss: 0.519440, acc: 0.765625]\n",
      "907: [discriminator loss: 0.645859, acc: 0.546875] [adversarial loss: 1.135439, acc: 0.078125]\n",
      "908: [discriminator loss: 0.575122, acc: 0.671875] [adversarial loss: 0.741466, acc: 0.468750]\n",
      "909: [discriminator loss: 0.610882, acc: 0.632812] [adversarial loss: 1.221792, acc: 0.031250]\n",
      "910: [discriminator loss: 0.574140, acc: 0.703125] [adversarial loss: 0.733766, acc: 0.515625]\n",
      "911: [discriminator loss: 0.642938, acc: 0.632812] [adversarial loss: 1.172090, acc: 0.078125]\n",
      "912: [discriminator loss: 0.592676, acc: 0.695312] [adversarial loss: 0.695139, acc: 0.468750]\n",
      "913: [discriminator loss: 0.588726, acc: 0.648438] [adversarial loss: 1.342635, acc: 0.046875]\n",
      "914: [discriminator loss: 0.627913, acc: 0.632812] [adversarial loss: 0.714959, acc: 0.468750]\n",
      "915: [discriminator loss: 0.573289, acc: 0.679688] [adversarial loss: 1.261925, acc: 0.031250]\n",
      "916: [discriminator loss: 0.626221, acc: 0.625000] [adversarial loss: 0.559009, acc: 0.796875]\n",
      "917: [discriminator loss: 0.600237, acc: 0.601562] [adversarial loss: 1.231394, acc: 0.015625]\n",
      "918: [discriminator loss: 0.603015, acc: 0.664062] [adversarial loss: 0.857582, acc: 0.281250]\n",
      "919: [discriminator loss: 0.561674, acc: 0.757812] [adversarial loss: 1.324976, acc: 0.015625]\n",
      "920: [discriminator loss: 0.576298, acc: 0.703125] [adversarial loss: 0.715328, acc: 0.500000]\n",
      "921: [discriminator loss: 0.541662, acc: 0.750000] [adversarial loss: 1.123617, acc: 0.093750]\n",
      "922: [discriminator loss: 0.518517, acc: 0.757812] [adversarial loss: 0.613756, acc: 0.734375]\n",
      "923: [discriminator loss: 0.623351, acc: 0.625000] [adversarial loss: 1.166688, acc: 0.078125]\n",
      "924: [discriminator loss: 0.641664, acc: 0.640625] [adversarial loss: 0.693372, acc: 0.593750]\n",
      "925: [discriminator loss: 0.582647, acc: 0.695312] [adversarial loss: 1.651440, acc: 0.015625]\n",
      "926: [discriminator loss: 0.624313, acc: 0.648438] [adversarial loss: 0.512118, acc: 0.765625]\n",
      "927: [discriminator loss: 0.576813, acc: 0.671875] [adversarial loss: 0.941325, acc: 0.203125]\n",
      "928: [discriminator loss: 0.560273, acc: 0.734375] [adversarial loss: 0.828478, acc: 0.375000]\n",
      "929: [discriminator loss: 0.636908, acc: 0.625000] [adversarial loss: 1.206101, acc: 0.046875]\n",
      "930: [discriminator loss: 0.619054, acc: 0.671875] [adversarial loss: 0.779818, acc: 0.312500]\n",
      "931: [discriminator loss: 0.544435, acc: 0.789062] [adversarial loss: 1.003882, acc: 0.171875]\n",
      "932: [discriminator loss: 0.557258, acc: 0.726562] [adversarial loss: 0.820064, acc: 0.390625]\n",
      "933: [discriminator loss: 0.529164, acc: 0.742188] [adversarial loss: 1.144025, acc: 0.109375]\n",
      "934: [discriminator loss: 0.566385, acc: 0.718750] [adversarial loss: 0.846408, acc: 0.265625]\n",
      "935: [discriminator loss: 0.564391, acc: 0.734375] [adversarial loss: 1.221697, acc: 0.062500]\n",
      "936: [discriminator loss: 0.564711, acc: 0.757812] [adversarial loss: 0.508385, acc: 0.828125]\n",
      "937: [discriminator loss: 0.638980, acc: 0.570312] [adversarial loss: 1.680580, acc: 0.000000]\n",
      "938: [discriminator loss: 0.798275, acc: 0.554688] [adversarial loss: 0.493136, acc: 0.828125]\n",
      "939: [discriminator loss: 0.674412, acc: 0.531250] [adversarial loss: 1.775855, acc: 0.000000]\n",
      "940: [discriminator loss: 0.655530, acc: 0.632812] [adversarial loss: 0.688439, acc: 0.609375]\n",
      "941: [discriminator loss: 0.576769, acc: 0.679688] [adversarial loss: 0.978725, acc: 0.187500]\n",
      "942: [discriminator loss: 0.546971, acc: 0.703125] [adversarial loss: 0.837577, acc: 0.328125]\n",
      "943: [discriminator loss: 0.531371, acc: 0.796875] [adversarial loss: 0.890040, acc: 0.328125]\n",
      "944: [discriminator loss: 0.621312, acc: 0.648438] [adversarial loss: 1.119535, acc: 0.078125]\n",
      "945: [discriminator loss: 0.528587, acc: 0.804688] [adversarial loss: 1.045560, acc: 0.109375]\n",
      "946: [discriminator loss: 0.556496, acc: 0.679688] [adversarial loss: 0.957697, acc: 0.250000]\n",
      "947: [discriminator loss: 0.582022, acc: 0.718750] [adversarial loss: 0.842944, acc: 0.312500]\n",
      "948: [discriminator loss: 0.539383, acc: 0.773438] [adversarial loss: 1.059468, acc: 0.125000]\n",
      "949: [discriminator loss: 0.534837, acc: 0.750000] [adversarial loss: 0.933654, acc: 0.265625]\n",
      "950: [discriminator loss: 0.576144, acc: 0.695312] [adversarial loss: 1.117714, acc: 0.078125]\n",
      "951: [discriminator loss: 0.543190, acc: 0.710938] [adversarial loss: 0.676483, acc: 0.593750]\n",
      "952: [discriminator loss: 0.503009, acc: 0.789062] [adversarial loss: 1.460898, acc: 0.062500]\n",
      "953: [discriminator loss: 0.608426, acc: 0.664062] [adversarial loss: 0.479776, acc: 0.906250]\n",
      "954: [discriminator loss: 0.657133, acc: 0.539062] [adversarial loss: 2.013440, acc: 0.000000]\n",
      "955: [discriminator loss: 0.681679, acc: 0.593750] [adversarial loss: 0.612905, acc: 0.656250]\n",
      "956: [discriminator loss: 0.530238, acc: 0.687500] [adversarial loss: 1.031204, acc: 0.156250]\n",
      "957: [discriminator loss: 0.524992, acc: 0.796875] [adversarial loss: 0.700280, acc: 0.562500]\n",
      "958: [discriminator loss: 0.506822, acc: 0.789062] [adversarial loss: 0.740997, acc: 0.484375]\n",
      "959: [discriminator loss: 0.565420, acc: 0.765625] [adversarial loss: 0.926401, acc: 0.265625]\n",
      "960: [discriminator loss: 0.640690, acc: 0.664062] [adversarial loss: 1.061167, acc: 0.109375]\n",
      "961: [discriminator loss: 0.498591, acc: 0.820312] [adversarial loss: 0.797199, acc: 0.375000]\n",
      "962: [discriminator loss: 0.566077, acc: 0.710938] [adversarial loss: 1.093099, acc: 0.171875]\n",
      "963: [discriminator loss: 0.587023, acc: 0.695312] [adversarial loss: 0.550972, acc: 0.765625]\n",
      "964: [discriminator loss: 0.573113, acc: 0.664062] [adversarial loss: 1.664701, acc: 0.000000]\n",
      "965: [discriminator loss: 0.594257, acc: 0.640625] [adversarial loss: 0.547767, acc: 0.781250]\n",
      "966: [discriminator loss: 0.629343, acc: 0.656250] [adversarial loss: 1.686954, acc: 0.000000]\n",
      "967: [discriminator loss: 0.617096, acc: 0.601562] [adversarial loss: 0.659673, acc: 0.515625]\n",
      "968: [discriminator loss: 0.576267, acc: 0.671875] [adversarial loss: 1.229495, acc: 0.062500]\n",
      "969: [discriminator loss: 0.559446, acc: 0.718750] [adversarial loss: 0.810873, acc: 0.390625]\n",
      "970: [discriminator loss: 0.581001, acc: 0.671875] [adversarial loss: 1.187334, acc: 0.046875]\n",
      "971: [discriminator loss: 0.512501, acc: 0.734375] [adversarial loss: 0.557717, acc: 0.765625]\n",
      "972: [discriminator loss: 0.599921, acc: 0.625000] [adversarial loss: 1.280863, acc: 0.093750]\n",
      "973: [discriminator loss: 0.545408, acc: 0.750000] [adversarial loss: 0.878282, acc: 0.296875]\n",
      "974: [discriminator loss: 0.630237, acc: 0.656250] [adversarial loss: 1.170210, acc: 0.093750]\n",
      "975: [discriminator loss: 0.546701, acc: 0.703125] [adversarial loss: 0.872047, acc: 0.250000]\n",
      "976: [discriminator loss: 0.537073, acc: 0.757812] [adversarial loss: 1.295013, acc: 0.046875]\n",
      "977: [discriminator loss: 0.582003, acc: 0.726562] [adversarial loss: 0.784659, acc: 0.437500]\n",
      "978: [discriminator loss: 0.527042, acc: 0.734375] [adversarial loss: 1.038515, acc: 0.140625]\n",
      "979: [discriminator loss: 0.534373, acc: 0.718750] [adversarial loss: 0.779302, acc: 0.453125]\n",
      "980: [discriminator loss: 0.529715, acc: 0.757812] [adversarial loss: 1.222790, acc: 0.046875]\n",
      "981: [discriminator loss: 0.538015, acc: 0.750000] [adversarial loss: 0.711085, acc: 0.609375]\n",
      "982: [discriminator loss: 0.596696, acc: 0.632812] [adversarial loss: 1.660833, acc: 0.000000]\n",
      "983: [discriminator loss: 0.584786, acc: 0.656250] [adversarial loss: 0.593430, acc: 0.765625]\n",
      "984: [discriminator loss: 0.555066, acc: 0.710938] [adversarial loss: 1.405491, acc: 0.000000]\n",
      "985: [discriminator loss: 0.596693, acc: 0.671875] [adversarial loss: 0.558309, acc: 0.750000]\n",
      "986: [discriminator loss: 0.606126, acc: 0.617188] [adversarial loss: 1.490168, acc: 0.031250]\n",
      "987: [discriminator loss: 0.596476, acc: 0.718750] [adversarial loss: 0.738085, acc: 0.468750]\n",
      "988: [discriminator loss: 0.509833, acc: 0.765625] [adversarial loss: 1.434457, acc: 0.015625]\n",
      "989: [discriminator loss: 0.538558, acc: 0.726562] [adversarial loss: 0.668674, acc: 0.578125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990: [discriminator loss: 0.519385, acc: 0.757812] [adversarial loss: 0.833006, acc: 0.390625]\n",
      "991: [discriminator loss: 0.479536, acc: 0.804688] [adversarial loss: 0.978473, acc: 0.218750]\n",
      "992: [discriminator loss: 0.528719, acc: 0.750000] [adversarial loss: 0.693198, acc: 0.515625]\n",
      "993: [discriminator loss: 0.659349, acc: 0.562500] [adversarial loss: 1.722408, acc: 0.000000]\n",
      "994: [discriminator loss: 0.649800, acc: 0.609375] [adversarial loss: 0.683154, acc: 0.593750]\n",
      "995: [discriminator loss: 0.647393, acc: 0.617188] [adversarial loss: 1.580950, acc: 0.015625]\n",
      "996: [discriminator loss: 0.618502, acc: 0.632812] [adversarial loss: 0.733315, acc: 0.500000]\n",
      "997: [discriminator loss: 0.549791, acc: 0.695312] [adversarial loss: 1.378215, acc: 0.015625]\n",
      "998: [discriminator loss: 0.564738, acc: 0.671875] [adversarial loss: 0.760235, acc: 0.359375]\n",
      "999: [discriminator loss: 0.567675, acc: 0.687500] [adversarial loss: 1.451881, acc: 0.015625]\n",
      "1000: [discriminator loss: 0.531874, acc: 0.710938] [adversarial loss: 0.843418, acc: 0.390625]\n",
      "1001: [discriminator loss: 0.599542, acc: 0.632812] [adversarial loss: 1.229177, acc: 0.156250]\n",
      "1002: [discriminator loss: 0.544053, acc: 0.710938] [adversarial loss: 0.815823, acc: 0.453125]\n",
      "1003: [discriminator loss: 0.545263, acc: 0.687500] [adversarial loss: 1.038029, acc: 0.171875]\n",
      "1004: [discriminator loss: 0.499041, acc: 0.804688] [adversarial loss: 0.856115, acc: 0.312500]\n",
      "1005: [discriminator loss: 0.552940, acc: 0.726562] [adversarial loss: 1.446632, acc: 0.000000]\n",
      "1006: [discriminator loss: 0.565062, acc: 0.664062] [adversarial loss: 0.698924, acc: 0.484375]\n",
      "1007: [discriminator loss: 0.583549, acc: 0.679688] [adversarial loss: 1.523693, acc: 0.015625]\n",
      "1008: [discriminator loss: 0.588467, acc: 0.703125] [adversarial loss: 0.897901, acc: 0.218750]\n",
      "1009: [discriminator loss: 0.490939, acc: 0.757812] [adversarial loss: 0.947502, acc: 0.234375]\n",
      "1010: [discriminator loss: 0.525930, acc: 0.789062] [adversarial loss: 1.118738, acc: 0.062500]\n",
      "1011: [discriminator loss: 0.580044, acc: 0.710938] [adversarial loss: 0.906513, acc: 0.312500]\n",
      "1012: [discriminator loss: 0.561465, acc: 0.726562] [adversarial loss: 1.410215, acc: 0.000000]\n",
      "1013: [discriminator loss: 0.585786, acc: 0.734375] [adversarial loss: 0.660974, acc: 0.578125]\n",
      "1014: [discriminator loss: 0.605633, acc: 0.640625] [adversarial loss: 1.797282, acc: 0.015625]\n",
      "1015: [discriminator loss: 0.655501, acc: 0.601562] [adversarial loss: 0.596397, acc: 0.718750]\n",
      "1016: [discriminator loss: 0.700563, acc: 0.539062] [adversarial loss: 1.761491, acc: 0.000000]\n",
      "1017: [discriminator loss: 0.627259, acc: 0.664062] [adversarial loss: 0.851140, acc: 0.328125]\n",
      "1018: [discriminator loss: 0.532106, acc: 0.718750] [adversarial loss: 1.236917, acc: 0.046875]\n",
      "1019: [discriminator loss: 0.530003, acc: 0.757812] [adversarial loss: 0.845885, acc: 0.265625]\n",
      "1020: [discriminator loss: 0.527718, acc: 0.734375] [adversarial loss: 1.448131, acc: 0.031250]\n",
      "1021: [discriminator loss: 0.526356, acc: 0.765625] [adversarial loss: 0.905262, acc: 0.234375]\n",
      "1022: [discriminator loss: 0.549313, acc: 0.695312] [adversarial loss: 1.233966, acc: 0.093750]\n",
      "1023: [discriminator loss: 0.549245, acc: 0.734375] [adversarial loss: 0.753572, acc: 0.421875]\n",
      "1024: [discriminator loss: 0.572508, acc: 0.664062] [adversarial loss: 1.252995, acc: 0.031250]\n",
      "1025: [discriminator loss: 0.463148, acc: 0.859375] [adversarial loss: 0.845244, acc: 0.375000]\n",
      "1026: [discriminator loss: 0.537147, acc: 0.750000] [adversarial loss: 1.196264, acc: 0.046875]\n",
      "1027: [discriminator loss: 0.549643, acc: 0.734375] [adversarial loss: 0.905039, acc: 0.296875]\n",
      "1028: [discriminator loss: 0.526274, acc: 0.726562] [adversarial loss: 1.335040, acc: 0.046875]\n",
      "1029: [discriminator loss: 0.496171, acc: 0.718750] [adversarial loss: 0.846418, acc: 0.375000]\n",
      "1030: [discriminator loss: 0.542878, acc: 0.695312] [adversarial loss: 1.202953, acc: 0.046875]\n",
      "1031: [discriminator loss: 0.502723, acc: 0.757812] [adversarial loss: 0.677948, acc: 0.562500]\n",
      "1032: [discriminator loss: 0.583179, acc: 0.640625] [adversarial loss: 1.946117, acc: 0.000000]\n",
      "1033: [discriminator loss: 0.687945, acc: 0.617188] [adversarial loss: 0.511357, acc: 0.765625]\n",
      "1034: [discriminator loss: 0.614771, acc: 0.554688] [adversarial loss: 1.780972, acc: 0.000000]\n",
      "1035: [discriminator loss: 0.646009, acc: 0.625000] [adversarial loss: 0.808315, acc: 0.375000]\n",
      "1036: [discriminator loss: 0.549041, acc: 0.695312] [adversarial loss: 1.153866, acc: 0.109375]\n",
      "1037: [discriminator loss: 0.535136, acc: 0.742188] [adversarial loss: 0.786809, acc: 0.421875]\n",
      "1038: [discriminator loss: 0.507727, acc: 0.820312] [adversarial loss: 1.249870, acc: 0.031250]\n",
      "1039: [discriminator loss: 0.528098, acc: 0.734375] [adversarial loss: 0.915311, acc: 0.203125]\n",
      "1040: [discriminator loss: 0.511358, acc: 0.757812] [adversarial loss: 1.284049, acc: 0.109375]\n",
      "1041: [discriminator loss: 0.558841, acc: 0.710938] [adversarial loss: 0.746768, acc: 0.531250]\n",
      "1042: [discriminator loss: 0.553902, acc: 0.718750] [adversarial loss: 1.601339, acc: 0.046875]\n",
      "1043: [discriminator loss: 0.587100, acc: 0.695312] [adversarial loss: 0.828698, acc: 0.359375]\n",
      "1044: [discriminator loss: 0.499182, acc: 0.804688] [adversarial loss: 1.256549, acc: 0.046875]\n",
      "1045: [discriminator loss: 0.540906, acc: 0.695312] [adversarial loss: 0.583746, acc: 0.750000]\n",
      "1046: [discriminator loss: 0.631934, acc: 0.617188] [adversarial loss: 1.552046, acc: 0.046875]\n",
      "1047: [discriminator loss: 0.631635, acc: 0.617188] [adversarial loss: 0.729631, acc: 0.531250]\n",
      "1048: [discriminator loss: 0.545280, acc: 0.703125] [adversarial loss: 1.168843, acc: 0.093750]\n",
      "1049: [discriminator loss: 0.500451, acc: 0.757812] [adversarial loss: 1.072236, acc: 0.109375]\n",
      "1050: [discriminator loss: 0.516173, acc: 0.796875] [adversarial loss: 0.951612, acc: 0.250000]\n",
      "1051: [discriminator loss: 0.545304, acc: 0.750000] [adversarial loss: 1.159502, acc: 0.078125]\n",
      "1052: [discriminator loss: 0.468391, acc: 0.781250] [adversarial loss: 0.894810, acc: 0.343750]\n",
      "1053: [discriminator loss: 0.532192, acc: 0.718750] [adversarial loss: 1.744404, acc: 0.046875]\n",
      "1054: [discriminator loss: 0.576532, acc: 0.648438] [adversarial loss: 0.687362, acc: 0.578125]\n",
      "1055: [discriminator loss: 0.606060, acc: 0.632812] [adversarial loss: 1.673177, acc: 0.078125]\n",
      "1056: [discriminator loss: 0.605047, acc: 0.648438] [adversarial loss: 0.804507, acc: 0.296875]\n",
      "1057: [discriminator loss: 0.553693, acc: 0.695312] [adversarial loss: 1.563834, acc: 0.062500]\n",
      "1058: [discriminator loss: 0.588689, acc: 0.671875] [adversarial loss: 0.948723, acc: 0.218750]\n",
      "1059: [discriminator loss: 0.450940, acc: 0.875000] [adversarial loss: 1.194536, acc: 0.156250]\n",
      "1060: [discriminator loss: 0.488690, acc: 0.765625] [adversarial loss: 0.854794, acc: 0.312500]\n",
      "1061: [discriminator loss: 0.534986, acc: 0.726562] [adversarial loss: 1.159279, acc: 0.109375]\n",
      "1062: [discriminator loss: 0.531080, acc: 0.773438] [adversarial loss: 1.118387, acc: 0.140625]\n",
      "1063: [discriminator loss: 0.535461, acc: 0.718750] [adversarial loss: 1.486616, acc: 0.000000]\n",
      "1064: [discriminator loss: 0.507870, acc: 0.750000] [adversarial loss: 0.725517, acc: 0.500000]\n",
      "1065: [discriminator loss: 0.556389, acc: 0.703125] [adversarial loss: 1.665956, acc: 0.031250]\n",
      "1066: [discriminator loss: 0.550014, acc: 0.671875] [adversarial loss: 0.573401, acc: 0.750000]\n",
      "1067: [discriminator loss: 0.618542, acc: 0.664062] [adversarial loss: 1.869700, acc: 0.015625]\n",
      "1068: [discriminator loss: 0.684780, acc: 0.609375] [adversarial loss: 0.743939, acc: 0.515625]\n",
      "1069: [discriminator loss: 0.536479, acc: 0.687500] [adversarial loss: 1.237293, acc: 0.062500]\n",
      "1070: [discriminator loss: 0.537366, acc: 0.742188] [adversarial loss: 0.960217, acc: 0.265625]\n",
      "1071: [discriminator loss: 0.487929, acc: 0.804688] [adversarial loss: 1.534127, acc: 0.015625]\n",
      "1072: [discriminator loss: 0.501290, acc: 0.742188] [adversarial loss: 0.792992, acc: 0.421875]\n",
      "1073: [discriminator loss: 0.592252, acc: 0.671875] [adversarial loss: 1.648679, acc: 0.000000]\n",
      "1074: [discriminator loss: 0.570709, acc: 0.687500] [adversarial loss: 0.782423, acc: 0.421875]\n",
      "1075: [discriminator loss: 0.507843, acc: 0.734375] [adversarial loss: 1.166630, acc: 0.109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076: [discriminator loss: 0.512070, acc: 0.757812] [adversarial loss: 0.945208, acc: 0.203125]\n",
      "1077: [discriminator loss: 0.487846, acc: 0.812500] [adversarial loss: 1.214699, acc: 0.140625]\n",
      "1078: [discriminator loss: 0.442884, acc: 0.843750] [adversarial loss: 1.066800, acc: 0.203125]\n",
      "1079: [discriminator loss: 0.525672, acc: 0.765625] [adversarial loss: 1.143579, acc: 0.234375]\n",
      "1080: [discriminator loss: 0.467912, acc: 0.765625] [adversarial loss: 0.862839, acc: 0.375000]\n",
      "1081: [discriminator loss: 0.527062, acc: 0.703125] [adversarial loss: 1.808022, acc: 0.031250]\n",
      "1082: [discriminator loss: 0.554921, acc: 0.687500] [adversarial loss: 0.562416, acc: 0.765625]\n",
      "1083: [discriminator loss: 0.620233, acc: 0.570312] [adversarial loss: 1.939296, acc: 0.015625]\n",
      "1084: [discriminator loss: 0.597163, acc: 0.671875] [adversarial loss: 0.814552, acc: 0.359375]\n",
      "1085: [discriminator loss: 0.523790, acc: 0.757812] [adversarial loss: 1.391756, acc: 0.046875]\n",
      "1086: [discriminator loss: 0.565331, acc: 0.695312] [adversarial loss: 0.727003, acc: 0.484375]\n",
      "1087: [discriminator loss: 0.497456, acc: 0.734375] [adversarial loss: 1.406643, acc: 0.046875]\n",
      "1088: [discriminator loss: 0.535655, acc: 0.703125] [adversarial loss: 0.803173, acc: 0.375000]\n",
      "1089: [discriminator loss: 0.492097, acc: 0.781250] [adversarial loss: 1.417392, acc: 0.062500]\n",
      "1090: [discriminator loss: 0.545624, acc: 0.710938] [adversarial loss: 0.673796, acc: 0.562500]\n",
      "1091: [discriminator loss: 0.547250, acc: 0.750000] [adversarial loss: 1.538220, acc: 0.046875]\n",
      "1092: [discriminator loss: 0.592001, acc: 0.671875] [adversarial loss: 0.653193, acc: 0.578125]\n",
      "1093: [discriminator loss: 0.572358, acc: 0.695312] [adversarial loss: 1.439264, acc: 0.078125]\n",
      "1094: [discriminator loss: 0.585449, acc: 0.656250] [adversarial loss: 0.830130, acc: 0.390625]\n",
      "1095: [discriminator loss: 0.530045, acc: 0.710938] [adversarial loss: 1.496260, acc: 0.015625]\n",
      "1096: [discriminator loss: 0.571081, acc: 0.656250] [adversarial loss: 0.880581, acc: 0.343750]\n",
      "1097: [discriminator loss: 0.566438, acc: 0.703125] [adversarial loss: 1.479635, acc: 0.046875]\n",
      "1098: [discriminator loss: 0.495666, acc: 0.703125] [adversarial loss: 0.895025, acc: 0.406250]\n",
      "1099: [discriminator loss: 0.562857, acc: 0.671875] [adversarial loss: 1.603558, acc: 0.031250]\n",
      "1100: [discriminator loss: 0.511123, acc: 0.734375] [adversarial loss: 0.646485, acc: 0.578125]\n",
      "1101: [discriminator loss: 0.519785, acc: 0.726562] [adversarial loss: 1.418287, acc: 0.062500]\n",
      "1102: [discriminator loss: 0.518287, acc: 0.773438] [adversarial loss: 0.912335, acc: 0.343750]\n",
      "1103: [discriminator loss: 0.454987, acc: 0.820312] [adversarial loss: 1.246044, acc: 0.109375]\n",
      "1104: [discriminator loss: 0.527447, acc: 0.718750] [adversarial loss: 0.844605, acc: 0.359375]\n",
      "1105: [discriminator loss: 0.510027, acc: 0.734375] [adversarial loss: 1.183296, acc: 0.171875]\n",
      "1106: [discriminator loss: 0.553183, acc: 0.671875] [adversarial loss: 1.160420, acc: 0.171875]\n",
      "1107: [discriminator loss: 0.492475, acc: 0.796875] [adversarial loss: 0.813365, acc: 0.343750]\n",
      "1108: [discriminator loss: 0.507639, acc: 0.757812] [adversarial loss: 1.683434, acc: 0.062500]\n",
      "1109: [discriminator loss: 0.450131, acc: 0.789062] [adversarial loss: 0.698249, acc: 0.562500]\n",
      "1110: [discriminator loss: 0.586598, acc: 0.687500] [adversarial loss: 1.945391, acc: 0.000000]\n",
      "1111: [discriminator loss: 0.527123, acc: 0.718750] [adversarial loss: 0.817673, acc: 0.390625]\n",
      "1112: [discriminator loss: 0.531588, acc: 0.742188] [adversarial loss: 1.660365, acc: 0.031250]\n",
      "1113: [discriminator loss: 0.510600, acc: 0.710938] [adversarial loss: 0.719608, acc: 0.531250]\n",
      "1114: [discriminator loss: 0.501660, acc: 0.734375] [adversarial loss: 1.551295, acc: 0.046875]\n",
      "1115: [discriminator loss: 0.529339, acc: 0.695312] [adversarial loss: 0.773482, acc: 0.484375]\n",
      "1116: [discriminator loss: 0.501948, acc: 0.718750] [adversarial loss: 1.377127, acc: 0.156250]\n",
      "1117: [discriminator loss: 0.529189, acc: 0.679688] [adversarial loss: 1.167257, acc: 0.093750]\n",
      "1118: [discriminator loss: 0.484928, acc: 0.781250] [adversarial loss: 1.077039, acc: 0.250000]\n",
      "1119: [discriminator loss: 0.440952, acc: 0.828125] [adversarial loss: 1.446668, acc: 0.031250]\n",
      "1120: [discriminator loss: 0.457086, acc: 0.765625] [adversarial loss: 1.010350, acc: 0.218750]\n",
      "1121: [discriminator loss: 0.455333, acc: 0.812500] [adversarial loss: 2.026134, acc: 0.000000]\n",
      "1122: [discriminator loss: 0.587651, acc: 0.703125] [adversarial loss: 0.640099, acc: 0.671875]\n",
      "1123: [discriminator loss: 0.551639, acc: 0.679688] [adversarial loss: 1.893617, acc: 0.000000]\n",
      "1124: [discriminator loss: 0.549277, acc: 0.710938] [adversarial loss: 0.891147, acc: 0.312500]\n",
      "1125: [discriminator loss: 0.489267, acc: 0.781250] [adversarial loss: 1.668943, acc: 0.015625]\n",
      "1126: [discriminator loss: 0.504106, acc: 0.750000] [adversarial loss: 0.958736, acc: 0.281250]\n",
      "1127: [discriminator loss: 0.435936, acc: 0.867188] [adversarial loss: 1.236002, acc: 0.078125]\n",
      "1128: [discriminator loss: 0.481348, acc: 0.765625] [adversarial loss: 0.880371, acc: 0.406250]\n",
      "1129: [discriminator loss: 0.494670, acc: 0.750000] [adversarial loss: 1.491858, acc: 0.062500]\n",
      "1130: [discriminator loss: 0.487822, acc: 0.718750] [adversarial loss: 1.100501, acc: 0.171875]\n",
      "1131: [discriminator loss: 0.471704, acc: 0.781250] [adversarial loss: 1.349651, acc: 0.062500]\n",
      "1132: [discriminator loss: 0.465465, acc: 0.773438] [adversarial loss: 0.884485, acc: 0.328125]\n",
      "1133: [discriminator loss: 0.450269, acc: 0.812500] [adversarial loss: 1.499886, acc: 0.062500]\n",
      "1134: [discriminator loss: 0.521764, acc: 0.710938] [adversarial loss: 0.492670, acc: 0.765625]\n",
      "1135: [discriminator loss: 0.646670, acc: 0.632812] [adversarial loss: 1.882542, acc: 0.015625]\n",
      "1136: [discriminator loss: 0.661027, acc: 0.656250] [adversarial loss: 0.688945, acc: 0.531250]\n",
      "1137: [discriminator loss: 0.562183, acc: 0.664062] [adversarial loss: 1.559184, acc: 0.062500]\n",
      "1138: [discriminator loss: 0.511907, acc: 0.750000] [adversarial loss: 0.948167, acc: 0.312500]\n",
      "1139: [discriminator loss: 0.494485, acc: 0.789062] [adversarial loss: 1.418516, acc: 0.031250]\n",
      "1140: [discriminator loss: 0.437799, acc: 0.820312] [adversarial loss: 0.889689, acc: 0.312500]\n",
      "1141: [discriminator loss: 0.428963, acc: 0.804688] [adversarial loss: 1.207079, acc: 0.156250]\n",
      "1142: [discriminator loss: 0.449391, acc: 0.804688] [adversarial loss: 0.911280, acc: 0.312500]\n",
      "1143: [discriminator loss: 0.458552, acc: 0.781250] [adversarial loss: 1.600965, acc: 0.031250]\n",
      "1144: [discriminator loss: 0.524700, acc: 0.734375] [adversarial loss: 0.563274, acc: 0.750000]\n",
      "1145: [discriminator loss: 0.618317, acc: 0.601562] [adversarial loss: 2.250348, acc: 0.000000]\n",
      "1146: [discriminator loss: 0.683803, acc: 0.632812] [adversarial loss: 0.756473, acc: 0.453125]\n",
      "1147: [discriminator loss: 0.529797, acc: 0.687500] [adversarial loss: 1.650719, acc: 0.015625]\n",
      "1148: [discriminator loss: 0.572048, acc: 0.710938] [adversarial loss: 0.927042, acc: 0.328125]\n",
      "1149: [discriminator loss: 0.512600, acc: 0.742188] [adversarial loss: 1.361792, acc: 0.078125]\n",
      "1150: [discriminator loss: 0.411560, acc: 0.867188] [adversarial loss: 1.023148, acc: 0.234375]\n",
      "1151: [discriminator loss: 0.454027, acc: 0.804688] [adversarial loss: 1.103603, acc: 0.187500]\n",
      "1152: [discriminator loss: 0.427961, acc: 0.859375] [adversarial loss: 1.222895, acc: 0.187500]\n",
      "1153: [discriminator loss: 0.479641, acc: 0.781250] [adversarial loss: 1.047762, acc: 0.250000]\n",
      "1154: [discriminator loss: 0.449802, acc: 0.789062] [adversarial loss: 1.412415, acc: 0.015625]\n",
      "1155: [discriminator loss: 0.427075, acc: 0.867188] [adversarial loss: 0.779701, acc: 0.437500]\n",
      "1156: [discriminator loss: 0.544693, acc: 0.710938] [adversarial loss: 1.858422, acc: 0.046875]\n",
      "1157: [discriminator loss: 0.563529, acc: 0.679688] [adversarial loss: 0.874869, acc: 0.421875]\n",
      "1158: [discriminator loss: 0.523489, acc: 0.679688] [adversarial loss: 1.735729, acc: 0.000000]\n",
      "1159: [discriminator loss: 0.511878, acc: 0.734375] [adversarial loss: 1.012243, acc: 0.265625]\n",
      "1160: [discriminator loss: 0.433429, acc: 0.804688] [adversarial loss: 1.308067, acc: 0.062500]\n",
      "1161: [discriminator loss: 0.475954, acc: 0.812500] [adversarial loss: 1.043864, acc: 0.203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162: [discriminator loss: 0.442189, acc: 0.812500] [adversarial loss: 1.297743, acc: 0.140625]\n",
      "1163: [discriminator loss: 0.448709, acc: 0.804688] [adversarial loss: 1.173178, acc: 0.203125]\n",
      "1164: [discriminator loss: 0.487187, acc: 0.765625] [adversarial loss: 1.460850, acc: 0.062500]\n",
      "1165: [discriminator loss: 0.493337, acc: 0.750000] [adversarial loss: 0.869435, acc: 0.375000]\n",
      "1166: [discriminator loss: 0.501260, acc: 0.757812] [adversarial loss: 1.741899, acc: 0.062500]\n",
      "1167: [discriminator loss: 0.532516, acc: 0.718750] [adversarial loss: 0.752219, acc: 0.640625]\n",
      "1168: [discriminator loss: 0.610246, acc: 0.617188] [adversarial loss: 2.195321, acc: 0.000000]\n",
      "1169: [discriminator loss: 0.611211, acc: 0.710938] [adversarial loss: 0.815010, acc: 0.468750]\n",
      "1170: [discriminator loss: 0.489851, acc: 0.742188] [adversarial loss: 1.837702, acc: 0.015625]\n",
      "1171: [discriminator loss: 0.487219, acc: 0.742188] [adversarial loss: 0.857963, acc: 0.375000]\n",
      "1172: [discriminator loss: 0.464628, acc: 0.765625] [adversarial loss: 1.709602, acc: 0.046875]\n",
      "1173: [discriminator loss: 0.522371, acc: 0.734375] [adversarial loss: 0.876338, acc: 0.421875]\n",
      "1174: [discriminator loss: 0.531332, acc: 0.781250] [adversarial loss: 1.364610, acc: 0.140625]\n",
      "1175: [discriminator loss: 0.492390, acc: 0.757812] [adversarial loss: 0.859940, acc: 0.406250]\n",
      "1176: [discriminator loss: 0.493853, acc: 0.781250] [adversarial loss: 1.730518, acc: 0.046875]\n",
      "1177: [discriminator loss: 0.522333, acc: 0.750000] [adversarial loss: 0.818940, acc: 0.375000]\n",
      "1178: [discriminator loss: 0.549406, acc: 0.710938] [adversarial loss: 1.799339, acc: 0.031250]\n",
      "1179: [discriminator loss: 0.559000, acc: 0.687500] [adversarial loss: 0.811806, acc: 0.453125]\n",
      "1180: [discriminator loss: 0.513843, acc: 0.695312] [adversarial loss: 1.460051, acc: 0.078125]\n",
      "1181: [discriminator loss: 0.496566, acc: 0.773438] [adversarial loss: 0.832890, acc: 0.375000]\n",
      "1182: [discriminator loss: 0.548138, acc: 0.687500] [adversarial loss: 1.617484, acc: 0.031250]\n",
      "1183: [discriminator loss: 0.549713, acc: 0.703125] [adversarial loss: 0.732157, acc: 0.468750]\n",
      "1184: [discriminator loss: 0.512167, acc: 0.750000] [adversarial loss: 1.581665, acc: 0.031250]\n",
      "1185: [discriminator loss: 0.568260, acc: 0.656250] [adversarial loss: 0.840491, acc: 0.437500]\n",
      "1186: [discriminator loss: 0.449608, acc: 0.812500] [adversarial loss: 1.560691, acc: 0.031250]\n",
      "1187: [discriminator loss: 0.553188, acc: 0.679688] [adversarial loss: 0.874531, acc: 0.437500]\n",
      "1188: [discriminator loss: 0.471282, acc: 0.781250] [adversarial loss: 1.636542, acc: 0.015625]\n",
      "1189: [discriminator loss: 0.512609, acc: 0.710938] [adversarial loss: 0.813224, acc: 0.468750]\n",
      "1190: [discriminator loss: 0.499315, acc: 0.765625] [adversarial loss: 1.612852, acc: 0.062500]\n",
      "1191: [discriminator loss: 0.530796, acc: 0.703125] [adversarial loss: 0.930169, acc: 0.281250]\n",
      "1192: [discriminator loss: 0.486993, acc: 0.757812] [adversarial loss: 1.393678, acc: 0.062500]\n",
      "1193: [discriminator loss: 0.446251, acc: 0.804688] [adversarial loss: 1.132502, acc: 0.093750]\n",
      "1194: [discriminator loss: 0.404411, acc: 0.867188] [adversarial loss: 1.072526, acc: 0.156250]\n",
      "1195: [discriminator loss: 0.451265, acc: 0.804688] [adversarial loss: 0.983575, acc: 0.359375]\n",
      "1196: [discriminator loss: 0.431006, acc: 0.812500] [adversarial loss: 1.064809, acc: 0.250000]\n",
      "1197: [discriminator loss: 0.461336, acc: 0.812500] [adversarial loss: 1.296289, acc: 0.218750]\n",
      "1198: [discriminator loss: 0.491331, acc: 0.781250] [adversarial loss: 1.147769, acc: 0.218750]\n",
      "1199: [discriminator loss: 0.538565, acc: 0.773438] [adversarial loss: 1.342951, acc: 0.187500]\n",
      "1200: [discriminator loss: 0.440898, acc: 0.796875] [adversarial loss: 0.791875, acc: 0.468750]\n",
      "1201: [discriminator loss: 0.511414, acc: 0.742188] [adversarial loss: 1.246475, acc: 0.156250]\n",
      "1202: [discriminator loss: 0.514332, acc: 0.765625] [adversarial loss: 1.262445, acc: 0.109375]\n",
      "1203: [discriminator loss: 0.470393, acc: 0.765625] [adversarial loss: 0.862575, acc: 0.437500]\n",
      "1204: [discriminator loss: 0.543086, acc: 0.734375] [adversarial loss: 2.539492, acc: 0.000000]\n",
      "1205: [discriminator loss: 0.770324, acc: 0.601562] [adversarial loss: 0.575060, acc: 0.687500]\n",
      "1206: [discriminator loss: 0.643133, acc: 0.609375] [adversarial loss: 1.934837, acc: 0.031250]\n",
      "1207: [discriminator loss: 0.600259, acc: 0.671875] [adversarial loss: 0.889253, acc: 0.296875]\n",
      "1208: [discriminator loss: 0.470641, acc: 0.796875] [adversarial loss: 1.694781, acc: 0.015625]\n",
      "1209: [discriminator loss: 0.498750, acc: 0.796875] [adversarial loss: 0.861443, acc: 0.406250]\n",
      "1210: [discriminator loss: 0.468483, acc: 0.804688] [adversarial loss: 1.070736, acc: 0.203125]\n",
      "1211: [discriminator loss: 0.451721, acc: 0.796875] [adversarial loss: 1.064532, acc: 0.218750]\n",
      "1212: [discriminator loss: 0.451305, acc: 0.765625] [adversarial loss: 1.293348, acc: 0.093750]\n",
      "1213: [discriminator loss: 0.460820, acc: 0.789062] [adversarial loss: 0.970538, acc: 0.281250]\n",
      "1214: [discriminator loss: 0.454682, acc: 0.796875] [adversarial loss: 1.571598, acc: 0.031250]\n",
      "1215: [discriminator loss: 0.469465, acc: 0.734375] [adversarial loss: 0.947195, acc: 0.296875]\n",
      "1216: [discriminator loss: 0.458828, acc: 0.781250] [adversarial loss: 1.519327, acc: 0.109375]\n",
      "1217: [discriminator loss: 0.531471, acc: 0.742188] [adversarial loss: 0.760299, acc: 0.593750]\n",
      "1218: [discriminator loss: 0.574577, acc: 0.703125] [adversarial loss: 2.067021, acc: 0.015625]\n",
      "1219: [discriminator loss: 0.660580, acc: 0.593750] [adversarial loss: 0.774251, acc: 0.484375]\n",
      "1220: [discriminator loss: 0.535521, acc: 0.703125] [adversarial loss: 1.692434, acc: 0.046875]\n",
      "1221: [discriminator loss: 0.567171, acc: 0.718750] [adversarial loss: 0.768170, acc: 0.531250]\n",
      "1222: [discriminator loss: 0.514533, acc: 0.710938] [adversarial loss: 1.801105, acc: 0.046875]\n",
      "1223: [discriminator loss: 0.545940, acc: 0.703125] [adversarial loss: 1.077038, acc: 0.187500]\n",
      "1224: [discriminator loss: 0.519388, acc: 0.796875] [adversarial loss: 1.305974, acc: 0.140625]\n",
      "1225: [discriminator loss: 0.495915, acc: 0.750000] [adversarial loss: 0.804981, acc: 0.484375]\n",
      "1226: [discriminator loss: 0.525078, acc: 0.750000] [adversarial loss: 1.400383, acc: 0.093750]\n",
      "1227: [discriminator loss: 0.549435, acc: 0.687500] [adversarial loss: 0.925817, acc: 0.312500]\n",
      "1228: [discriminator loss: 0.486454, acc: 0.781250] [adversarial loss: 1.375167, acc: 0.140625]\n",
      "1229: [discriminator loss: 0.479915, acc: 0.742188] [adversarial loss: 0.833486, acc: 0.453125]\n",
      "1230: [discriminator loss: 0.492580, acc: 0.750000] [adversarial loss: 1.925858, acc: 0.015625]\n",
      "1231: [discriminator loss: 0.601186, acc: 0.695312] [adversarial loss: 0.758500, acc: 0.421875]\n",
      "1232: [discriminator loss: 0.532575, acc: 0.726562] [adversarial loss: 1.911770, acc: 0.062500]\n",
      "1233: [discriminator loss: 0.532844, acc: 0.703125] [adversarial loss: 0.743028, acc: 0.437500]\n",
      "1234: [discriminator loss: 0.495212, acc: 0.718750] [adversarial loss: 1.661659, acc: 0.031250]\n",
      "1235: [discriminator loss: 0.519599, acc: 0.687500] [adversarial loss: 0.698509, acc: 0.593750]\n",
      "1236: [discriminator loss: 0.511211, acc: 0.773438] [adversarial loss: 1.262635, acc: 0.125000]\n",
      "1237: [discriminator loss: 0.467100, acc: 0.789062] [adversarial loss: 1.084841, acc: 0.171875]\n",
      "1238: [discriminator loss: 0.463897, acc: 0.757812] [adversarial loss: 0.930534, acc: 0.421875]\n",
      "1239: [discriminator loss: 0.435915, acc: 0.851562] [adversarial loss: 1.453290, acc: 0.078125]\n",
      "1240: [discriminator loss: 0.480062, acc: 0.765625] [adversarial loss: 0.678026, acc: 0.656250]\n",
      "1241: [discriminator loss: 0.534467, acc: 0.726562] [adversarial loss: 1.926734, acc: 0.031250]\n",
      "1242: [discriminator loss: 0.554662, acc: 0.718750] [adversarial loss: 0.795892, acc: 0.421875]\n",
      "1243: [discriminator loss: 0.508663, acc: 0.718750] [adversarial loss: 1.753249, acc: 0.031250]\n",
      "1244: [discriminator loss: 0.600838, acc: 0.679688] [adversarial loss: 0.697831, acc: 0.625000]\n",
      "1245: [discriminator loss: 0.500615, acc: 0.796875] [adversarial loss: 1.442837, acc: 0.125000]\n",
      "1246: [discriminator loss: 0.531278, acc: 0.718750] [adversarial loss: 0.914149, acc: 0.328125]\n",
      "1247: [discriminator loss: 0.509612, acc: 0.781250] [adversarial loss: 1.650771, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248: [discriminator loss: 0.539321, acc: 0.718750] [adversarial loss: 0.915824, acc: 0.312500]\n",
      "1249: [discriminator loss: 0.471806, acc: 0.750000] [adversarial loss: 1.452579, acc: 0.109375]\n",
      "1250: [discriminator loss: 0.534153, acc: 0.695312] [adversarial loss: 0.772517, acc: 0.500000]\n",
      "1251: [discriminator loss: 0.557979, acc: 0.734375] [adversarial loss: 1.470271, acc: 0.140625]\n",
      "1252: [discriminator loss: 0.503235, acc: 0.695312] [adversarial loss: 0.847911, acc: 0.328125]\n",
      "1253: [discriminator loss: 0.520892, acc: 0.789062] [adversarial loss: 1.339248, acc: 0.046875]\n",
      "1254: [discriminator loss: 0.546629, acc: 0.703125] [adversarial loss: 0.910427, acc: 0.328125]\n",
      "1255: [discriminator loss: 0.559725, acc: 0.757812] [adversarial loss: 1.660857, acc: 0.031250]\n",
      "1256: [discriminator loss: 0.607473, acc: 0.671875] [adversarial loss: 0.790033, acc: 0.406250]\n",
      "1257: [discriminator loss: 0.432912, acc: 0.851562] [adversarial loss: 1.308726, acc: 0.140625]\n",
      "1258: [discriminator loss: 0.495251, acc: 0.781250] [adversarial loss: 1.101676, acc: 0.250000]\n",
      "1259: [discriminator loss: 0.466738, acc: 0.781250] [adversarial loss: 1.197461, acc: 0.203125]\n",
      "1260: [discriminator loss: 0.540146, acc: 0.757812] [adversarial loss: 0.894299, acc: 0.312500]\n",
      "1261: [discriminator loss: 0.509209, acc: 0.750000] [adversarial loss: 1.594788, acc: 0.093750]\n",
      "1262: [discriminator loss: 0.587587, acc: 0.679688] [adversarial loss: 0.620546, acc: 0.687500]\n",
      "1263: [discriminator loss: 0.655470, acc: 0.578125] [adversarial loss: 1.884878, acc: 0.015625]\n",
      "1264: [discriminator loss: 0.697776, acc: 0.625000] [adversarial loss: 0.769958, acc: 0.484375]\n",
      "1265: [discriminator loss: 0.564649, acc: 0.695312] [adversarial loss: 1.518036, acc: 0.062500]\n",
      "1266: [discriminator loss: 0.517663, acc: 0.734375] [adversarial loss: 0.822595, acc: 0.390625]\n",
      "1267: [discriminator loss: 0.535119, acc: 0.679688] [adversarial loss: 1.476636, acc: 0.062500]\n",
      "1268: [discriminator loss: 0.502125, acc: 0.757812] [adversarial loss: 0.804910, acc: 0.390625]\n",
      "1269: [discriminator loss: 0.497468, acc: 0.765625] [adversarial loss: 1.408388, acc: 0.046875]\n",
      "1270: [discriminator loss: 0.524699, acc: 0.750000] [adversarial loss: 0.867746, acc: 0.390625]\n",
      "1271: [discriminator loss: 0.520129, acc: 0.742188] [adversarial loss: 1.468274, acc: 0.062500]\n",
      "1272: [discriminator loss: 0.522948, acc: 0.718750] [adversarial loss: 0.816518, acc: 0.375000]\n",
      "1273: [discriminator loss: 0.535430, acc: 0.734375] [adversarial loss: 1.726645, acc: 0.031250]\n",
      "1274: [discriminator loss: 0.490942, acc: 0.750000] [adversarial loss: 0.891985, acc: 0.421875]\n",
      "1275: [discriminator loss: 0.526905, acc: 0.718750] [adversarial loss: 1.639718, acc: 0.046875]\n",
      "1276: [discriminator loss: 0.490782, acc: 0.726562] [adversarial loss: 0.930742, acc: 0.328125]\n",
      "1277: [discriminator loss: 0.594901, acc: 0.679688] [adversarial loss: 1.428756, acc: 0.109375]\n",
      "1278: [discriminator loss: 0.564287, acc: 0.679688] [adversarial loss: 0.786007, acc: 0.421875]\n",
      "1279: [discriminator loss: 0.563823, acc: 0.734375] [adversarial loss: 1.517357, acc: 0.046875]\n",
      "1280: [discriminator loss: 0.488628, acc: 0.773438] [adversarial loss: 0.790133, acc: 0.406250]\n",
      "1281: [discriminator loss: 0.561589, acc: 0.742188] [adversarial loss: 1.705265, acc: 0.031250]\n",
      "1282: [discriminator loss: 0.519039, acc: 0.687500] [adversarial loss: 1.008735, acc: 0.296875]\n",
      "1283: [discriminator loss: 0.527029, acc: 0.734375] [adversarial loss: 1.336238, acc: 0.140625]\n",
      "1284: [discriminator loss: 0.510399, acc: 0.789062] [adversarial loss: 0.865477, acc: 0.375000]\n",
      "1285: [discriminator loss: 0.505478, acc: 0.750000] [adversarial loss: 1.437494, acc: 0.031250]\n",
      "1286: [discriminator loss: 0.490678, acc: 0.750000] [adversarial loss: 0.910884, acc: 0.453125]\n",
      "1287: [discriminator loss: 0.536415, acc: 0.734375] [adversarial loss: 1.654805, acc: 0.046875]\n",
      "1288: [discriminator loss: 0.657478, acc: 0.671875] [adversarial loss: 0.516978, acc: 0.750000]\n",
      "1289: [discriminator loss: 0.595170, acc: 0.632812] [adversarial loss: 1.750558, acc: 0.031250]\n",
      "1290: [discriminator loss: 0.613862, acc: 0.687500] [adversarial loss: 0.838112, acc: 0.421875]\n",
      "1291: [discriminator loss: 0.491687, acc: 0.718750] [adversarial loss: 1.266373, acc: 0.140625]\n",
      "1292: [discriminator loss: 0.465123, acc: 0.765625] [adversarial loss: 1.026573, acc: 0.234375]\n",
      "1293: [discriminator loss: 0.455128, acc: 0.820312] [adversarial loss: 1.389114, acc: 0.031250]\n",
      "1294: [discriminator loss: 0.546957, acc: 0.687500] [adversarial loss: 0.960486, acc: 0.281250]\n",
      "1295: [discriminator loss: 0.535605, acc: 0.710938] [adversarial loss: 1.410115, acc: 0.046875]\n",
      "1296: [discriminator loss: 0.560353, acc: 0.687500] [adversarial loss: 0.874322, acc: 0.375000]\n",
      "1297: [discriminator loss: 0.599832, acc: 0.664062] [adversarial loss: 1.696377, acc: 0.046875]\n",
      "1298: [discriminator loss: 0.561109, acc: 0.679688] [adversarial loss: 0.884994, acc: 0.328125]\n",
      "1299: [discriminator loss: 0.508353, acc: 0.757812] [adversarial loss: 1.476777, acc: 0.062500]\n",
      "1300: [discriminator loss: 0.506836, acc: 0.710938] [adversarial loss: 1.016134, acc: 0.234375]\n",
      "1301: [discriminator loss: 0.496637, acc: 0.718750] [adversarial loss: 1.241368, acc: 0.109375]\n",
      "1302: [discriminator loss: 0.486580, acc: 0.710938] [adversarial loss: 1.175124, acc: 0.203125]\n",
      "1303: [discriminator loss: 0.540742, acc: 0.757812] [adversarial loss: 1.317783, acc: 0.093750]\n",
      "1304: [discriminator loss: 0.534977, acc: 0.773438] [adversarial loss: 0.955352, acc: 0.187500]\n",
      "1305: [discriminator loss: 0.528489, acc: 0.742188] [adversarial loss: 1.382523, acc: 0.078125]\n",
      "1306: [discriminator loss: 0.536750, acc: 0.726562] [adversarial loss: 0.795245, acc: 0.500000]\n",
      "1307: [discriminator loss: 0.599747, acc: 0.640625] [adversarial loss: 1.849805, acc: 0.015625]\n",
      "1308: [discriminator loss: 0.597899, acc: 0.703125] [adversarial loss: 0.756521, acc: 0.484375]\n",
      "1309: [discriminator loss: 0.551120, acc: 0.695312] [adversarial loss: 1.523161, acc: 0.062500]\n",
      "1310: [discriminator loss: 0.566664, acc: 0.726562] [adversarial loss: 0.714053, acc: 0.562500]\n",
      "1311: [discriminator loss: 0.488199, acc: 0.773438] [adversarial loss: 1.384338, acc: 0.109375]\n",
      "1312: [discriminator loss: 0.510880, acc: 0.765625] [adversarial loss: 0.826577, acc: 0.343750]\n",
      "1313: [discriminator loss: 0.566622, acc: 0.718750] [adversarial loss: 1.811588, acc: 0.062500]\n",
      "1314: [discriminator loss: 0.589037, acc: 0.648438] [adversarial loss: 0.777650, acc: 0.500000]\n",
      "1315: [discriminator loss: 0.479709, acc: 0.765625] [adversarial loss: 1.580980, acc: 0.015625]\n",
      "1316: [discriminator loss: 0.513887, acc: 0.773438] [adversarial loss: 0.840620, acc: 0.406250]\n",
      "1317: [discriminator loss: 0.519880, acc: 0.750000] [adversarial loss: 1.225267, acc: 0.171875]\n",
      "1318: [discriminator loss: 0.487282, acc: 0.796875] [adversarial loss: 0.930062, acc: 0.296875]\n",
      "1319: [discriminator loss: 0.528316, acc: 0.726562] [adversarial loss: 1.179754, acc: 0.171875]\n",
      "1320: [discriminator loss: 0.473750, acc: 0.773438] [adversarial loss: 1.155889, acc: 0.125000]\n",
      "1321: [discriminator loss: 0.510114, acc: 0.734375] [adversarial loss: 1.191998, acc: 0.218750]\n",
      "1322: [discriminator loss: 0.484814, acc: 0.781250] [adversarial loss: 1.153340, acc: 0.171875]\n",
      "1323: [discriminator loss: 0.474555, acc: 0.789062] [adversarial loss: 1.568439, acc: 0.046875]\n",
      "1324: [discriminator loss: 0.509433, acc: 0.695312] [adversarial loss: 0.826396, acc: 0.390625]\n",
      "1325: [discriminator loss: 0.555732, acc: 0.718750] [adversarial loss: 2.051875, acc: 0.046875]\n",
      "1326: [discriminator loss: 0.648877, acc: 0.656250] [adversarial loss: 0.634317, acc: 0.609375]\n",
      "1327: [discriminator loss: 0.676968, acc: 0.593750] [adversarial loss: 1.739609, acc: 0.031250]\n",
      "1328: [discriminator loss: 0.588140, acc: 0.648438] [adversarial loss: 1.110575, acc: 0.218750]\n",
      "1329: [discriminator loss: 0.494343, acc: 0.773438] [adversarial loss: 1.182875, acc: 0.109375]\n",
      "1330: [discriminator loss: 0.466820, acc: 0.781250] [adversarial loss: 0.960399, acc: 0.343750]\n",
      "1331: [discriminator loss: 0.474703, acc: 0.773438] [adversarial loss: 1.192026, acc: 0.156250]\n",
      "1332: [discriminator loss: 0.499594, acc: 0.773438] [adversarial loss: 0.977299, acc: 0.218750]\n",
      "1333: [discriminator loss: 0.556292, acc: 0.734375] [adversarial loss: 1.581403, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334: [discriminator loss: 0.481274, acc: 0.750000] [adversarial loss: 0.852459, acc: 0.484375]\n",
      "1335: [discriminator loss: 0.517165, acc: 0.703125] [adversarial loss: 1.649911, acc: 0.046875]\n",
      "1336: [discriminator loss: 0.546384, acc: 0.648438] [adversarial loss: 0.862320, acc: 0.421875]\n",
      "1337: [discriminator loss: 0.526704, acc: 0.734375] [adversarial loss: 1.236144, acc: 0.125000]\n",
      "1338: [discriminator loss: 0.521165, acc: 0.804688] [adversarial loss: 0.931292, acc: 0.343750]\n",
      "1339: [discriminator loss: 0.573429, acc: 0.695312] [adversarial loss: 1.549001, acc: 0.125000]\n",
      "1340: [discriminator loss: 0.509781, acc: 0.710938] [adversarial loss: 0.976535, acc: 0.203125]\n",
      "1341: [discriminator loss: 0.547139, acc: 0.726562] [adversarial loss: 1.454083, acc: 0.125000]\n",
      "1342: [discriminator loss: 0.531677, acc: 0.718750] [adversarial loss: 0.927024, acc: 0.312500]\n",
      "1343: [discriminator loss: 0.535893, acc: 0.679688] [adversarial loss: 1.267983, acc: 0.156250]\n",
      "1344: [discriminator loss: 0.545104, acc: 0.710938] [adversarial loss: 0.845718, acc: 0.359375]\n",
      "1345: [discriminator loss: 0.470298, acc: 0.781250] [adversarial loss: 1.443104, acc: 0.140625]\n",
      "1346: [discriminator loss: 0.537592, acc: 0.679688] [adversarial loss: 0.979519, acc: 0.281250]\n",
      "1347: [discriminator loss: 0.442431, acc: 0.789062] [adversarial loss: 1.351950, acc: 0.125000]\n",
      "1348: [discriminator loss: 0.441387, acc: 0.796875] [adversarial loss: 0.905647, acc: 0.406250]\n",
      "1349: [discriminator loss: 0.615688, acc: 0.687500] [adversarial loss: 1.549190, acc: 0.109375]\n",
      "1350: [discriminator loss: 0.619590, acc: 0.640625] [adversarial loss: 0.647563, acc: 0.609375]\n",
      "1351: [discriminator loss: 0.630572, acc: 0.570312] [adversarial loss: 1.881003, acc: 0.015625]\n",
      "1352: [discriminator loss: 0.625539, acc: 0.671875] [adversarial loss: 0.765647, acc: 0.484375]\n",
      "1353: [discriminator loss: 0.601517, acc: 0.664062] [adversarial loss: 1.208395, acc: 0.140625]\n",
      "1354: [discriminator loss: 0.438850, acc: 0.812500] [adversarial loss: 0.972217, acc: 0.296875]\n",
      "1355: [discriminator loss: 0.470481, acc: 0.804688] [adversarial loss: 1.125835, acc: 0.218750]\n",
      "1356: [discriminator loss: 0.487928, acc: 0.742188] [adversarial loss: 1.040879, acc: 0.234375]\n",
      "1357: [discriminator loss: 0.569117, acc: 0.656250] [adversarial loss: 1.172185, acc: 0.250000]\n",
      "1358: [discriminator loss: 0.523440, acc: 0.742188] [adversarial loss: 0.896994, acc: 0.406250]\n",
      "1359: [discriminator loss: 0.543378, acc: 0.750000] [adversarial loss: 1.396332, acc: 0.078125]\n",
      "1360: [discriminator loss: 0.535929, acc: 0.742188] [adversarial loss: 0.765005, acc: 0.500000]\n",
      "1361: [discriminator loss: 0.579611, acc: 0.695312] [adversarial loss: 1.897196, acc: 0.015625]\n",
      "1362: [discriminator loss: 0.606013, acc: 0.625000] [adversarial loss: 0.647160, acc: 0.578125]\n",
      "1363: [discriminator loss: 0.537208, acc: 0.718750] [adversarial loss: 1.434920, acc: 0.078125]\n",
      "1364: [discriminator loss: 0.533671, acc: 0.765625] [adversarial loss: 0.881299, acc: 0.375000]\n",
      "1365: [discriminator loss: 0.487978, acc: 0.781250] [adversarial loss: 1.161180, acc: 0.187500]\n",
      "1366: [discriminator loss: 0.512292, acc: 0.750000] [adversarial loss: 0.768371, acc: 0.468750]\n",
      "1367: [discriminator loss: 0.591998, acc: 0.710938] [adversarial loss: 1.352525, acc: 0.171875]\n",
      "1368: [discriminator loss: 0.555715, acc: 0.726562] [adversarial loss: 0.799797, acc: 0.421875]\n",
      "1369: [discriminator loss: 0.564047, acc: 0.703125] [adversarial loss: 1.627067, acc: 0.046875]\n",
      "1370: [discriminator loss: 0.626323, acc: 0.617188] [adversarial loss: 0.541537, acc: 0.765625]\n",
      "1371: [discriminator loss: 0.674413, acc: 0.601562] [adversarial loss: 1.743959, acc: 0.046875]\n",
      "1372: [discriminator loss: 0.657197, acc: 0.648438] [adversarial loss: 0.815091, acc: 0.500000]\n",
      "1373: [discriminator loss: 0.541140, acc: 0.726562] [adversarial loss: 1.219171, acc: 0.125000]\n",
      "1374: [discriminator loss: 0.460868, acc: 0.796875] [adversarial loss: 1.070968, acc: 0.250000]\n",
      "1375: [discriminator loss: 0.547713, acc: 0.718750] [adversarial loss: 1.104918, acc: 0.234375]\n",
      "1376: [discriminator loss: 0.534478, acc: 0.750000] [adversarial loss: 0.774917, acc: 0.468750]\n",
      "1377: [discriminator loss: 0.518139, acc: 0.773438] [adversarial loss: 1.298927, acc: 0.062500]\n",
      "1378: [discriminator loss: 0.489573, acc: 0.734375] [adversarial loss: 0.841894, acc: 0.343750]\n",
      "1379: [discriminator loss: 0.545547, acc: 0.703125] [adversarial loss: 1.391357, acc: 0.171875]\n",
      "1380: [discriminator loss: 0.625629, acc: 0.648438] [adversarial loss: 0.736612, acc: 0.546875]\n",
      "1381: [discriminator loss: 0.496253, acc: 0.750000] [adversarial loss: 1.483065, acc: 0.078125]\n",
      "1382: [discriminator loss: 0.580571, acc: 0.679688] [adversarial loss: 0.986018, acc: 0.250000]\n",
      "1383: [discriminator loss: 0.507628, acc: 0.765625] [adversarial loss: 1.647681, acc: 0.062500]\n",
      "1384: [discriminator loss: 0.645191, acc: 0.632812] [adversarial loss: 0.765889, acc: 0.406250]\n",
      "1385: [discriminator loss: 0.544672, acc: 0.703125] [adversarial loss: 1.201778, acc: 0.171875]\n",
      "1386: [discriminator loss: 0.548181, acc: 0.718750] [adversarial loss: 0.829220, acc: 0.375000]\n",
      "1387: [discriminator loss: 0.486678, acc: 0.796875] [adversarial loss: 1.009786, acc: 0.296875]\n",
      "1388: [discriminator loss: 0.462393, acc: 0.796875] [adversarial loss: 1.075423, acc: 0.203125]\n",
      "1389: [discriminator loss: 0.560610, acc: 0.710938] [adversarial loss: 1.206603, acc: 0.203125]\n",
      "1390: [discriminator loss: 0.512609, acc: 0.796875] [adversarial loss: 1.004742, acc: 0.296875]\n",
      "1391: [discriminator loss: 0.505161, acc: 0.765625] [adversarial loss: 1.756884, acc: 0.000000]\n",
      "1392: [discriminator loss: 0.530939, acc: 0.718750] [adversarial loss: 0.701016, acc: 0.546875]\n",
      "1393: [discriminator loss: 0.564128, acc: 0.703125] [adversarial loss: 1.820900, acc: 0.015625]\n",
      "1394: [discriminator loss: 0.611942, acc: 0.640625] [adversarial loss: 0.664765, acc: 0.531250]\n",
      "1395: [discriminator loss: 0.626924, acc: 0.656250] [adversarial loss: 1.495295, acc: 0.031250]\n",
      "1396: [discriminator loss: 0.602853, acc: 0.656250] [adversarial loss: 0.917094, acc: 0.296875]\n",
      "1397: [discriminator loss: 0.534820, acc: 0.742188] [adversarial loss: 1.194086, acc: 0.125000]\n",
      "1398: [discriminator loss: 0.502432, acc: 0.734375] [adversarial loss: 1.188257, acc: 0.125000]\n",
      "1399: [discriminator loss: 0.551681, acc: 0.718750] [adversarial loss: 1.248707, acc: 0.125000]\n",
      "1400: [discriminator loss: 0.522002, acc: 0.757812] [adversarial loss: 0.962250, acc: 0.359375]\n",
      "1401: [discriminator loss: 0.538373, acc: 0.726562] [adversarial loss: 1.401633, acc: 0.062500]\n",
      "1402: [discriminator loss: 0.510727, acc: 0.742188] [adversarial loss: 0.714495, acc: 0.546875]\n",
      "1403: [discriminator loss: 0.619430, acc: 0.625000] [adversarial loss: 1.895858, acc: 0.062500]\n",
      "1404: [discriminator loss: 0.607842, acc: 0.679688] [adversarial loss: 0.804444, acc: 0.390625]\n",
      "1405: [discriminator loss: 0.513792, acc: 0.742188] [adversarial loss: 1.368395, acc: 0.062500]\n",
      "1406: [discriminator loss: 0.530702, acc: 0.710938] [adversarial loss: 0.909236, acc: 0.312500]\n",
      "1407: [discriminator loss: 0.532573, acc: 0.765625] [adversarial loss: 1.373870, acc: 0.125000]\n",
      "1408: [discriminator loss: 0.551622, acc: 0.734375] [adversarial loss: 1.054769, acc: 0.281250]\n",
      "1409: [discriminator loss: 0.485880, acc: 0.781250] [adversarial loss: 0.982448, acc: 0.265625]\n",
      "1410: [discriminator loss: 0.514842, acc: 0.750000] [adversarial loss: 1.150596, acc: 0.156250]\n",
      "1411: [discriminator loss: 0.468371, acc: 0.781250] [adversarial loss: 0.802747, acc: 0.468750]\n",
      "1412: [discriminator loss: 0.485745, acc: 0.773438] [adversarial loss: 1.510838, acc: 0.046875]\n",
      "1413: [discriminator loss: 0.505275, acc: 0.757812] [adversarial loss: 0.778126, acc: 0.515625]\n",
      "1414: [discriminator loss: 0.533356, acc: 0.726562] [adversarial loss: 1.701204, acc: 0.093750]\n",
      "1415: [discriminator loss: 0.685097, acc: 0.625000] [adversarial loss: 0.814913, acc: 0.406250]\n",
      "1416: [discriminator loss: 0.533904, acc: 0.687500] [adversarial loss: 1.583561, acc: 0.031250]\n",
      "1417: [discriminator loss: 0.560804, acc: 0.671875] [adversarial loss: 0.707923, acc: 0.562500]\n",
      "1418: [discriminator loss: 0.578911, acc: 0.726562] [adversarial loss: 1.462746, acc: 0.015625]\n",
      "1419: [discriminator loss: 0.466066, acc: 0.773438] [adversarial loss: 0.915409, acc: 0.312500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420: [discriminator loss: 0.546192, acc: 0.726562] [adversarial loss: 1.344130, acc: 0.125000]\n",
      "1421: [discriminator loss: 0.446400, acc: 0.820312] [adversarial loss: 0.981381, acc: 0.328125]\n",
      "1422: [discriminator loss: 0.460314, acc: 0.789062] [adversarial loss: 1.308311, acc: 0.125000]\n",
      "1423: [discriminator loss: 0.557227, acc: 0.718750] [adversarial loss: 1.094415, acc: 0.234375]\n",
      "1424: [discriminator loss: 0.471191, acc: 0.781250] [adversarial loss: 1.544626, acc: 0.031250]\n",
      "1425: [discriminator loss: 0.593649, acc: 0.640625] [adversarial loss: 0.614326, acc: 0.687500]\n",
      "1426: [discriminator loss: 0.597053, acc: 0.656250] [adversarial loss: 1.912434, acc: 0.031250]\n",
      "1427: [discriminator loss: 0.638566, acc: 0.695312] [adversarial loss: 0.609177, acc: 0.671875]\n",
      "1428: [discriminator loss: 0.583821, acc: 0.710938] [adversarial loss: 1.542470, acc: 0.078125]\n",
      "1429: [discriminator loss: 0.491547, acc: 0.773438] [adversarial loss: 0.794957, acc: 0.437500]\n",
      "1430: [discriminator loss: 0.491003, acc: 0.789062] [adversarial loss: 1.362629, acc: 0.109375]\n",
      "1431: [discriminator loss: 0.469068, acc: 0.789062] [adversarial loss: 0.909303, acc: 0.359375]\n",
      "1432: [discriminator loss: 0.523036, acc: 0.757812] [adversarial loss: 1.687240, acc: 0.046875]\n",
      "1433: [discriminator loss: 0.543774, acc: 0.734375] [adversarial loss: 0.940249, acc: 0.343750]\n",
      "1434: [discriminator loss: 0.493142, acc: 0.742188] [adversarial loss: 1.284358, acc: 0.140625]\n",
      "1435: [discriminator loss: 0.534837, acc: 0.718750] [adversarial loss: 0.831600, acc: 0.468750]\n",
      "1436: [discriminator loss: 0.522365, acc: 0.734375] [adversarial loss: 1.250885, acc: 0.171875]\n",
      "1437: [discriminator loss: 0.511951, acc: 0.742188] [adversarial loss: 0.982126, acc: 0.343750]\n",
      "1438: [discriminator loss: 0.549782, acc: 0.773438] [adversarial loss: 1.240706, acc: 0.109375]\n",
      "1439: [discriminator loss: 0.530036, acc: 0.757812] [adversarial loss: 1.148572, acc: 0.250000]\n",
      "1440: [discriminator loss: 0.578563, acc: 0.671875] [adversarial loss: 1.027546, acc: 0.312500]\n",
      "1441: [discriminator loss: 0.450549, acc: 0.820312] [adversarial loss: 1.170318, acc: 0.125000]\n",
      "1442: [discriminator loss: 0.517821, acc: 0.765625] [adversarial loss: 1.113985, acc: 0.218750]\n",
      "1443: [discriminator loss: 0.556500, acc: 0.726562] [adversarial loss: 1.610586, acc: 0.078125]\n",
      "1444: [discriminator loss: 0.589969, acc: 0.687500] [adversarial loss: 0.743021, acc: 0.453125]\n",
      "1445: [discriminator loss: 0.589694, acc: 0.656250] [adversarial loss: 1.838951, acc: 0.062500]\n",
      "1446: [discriminator loss: 0.614673, acc: 0.640625] [adversarial loss: 0.529668, acc: 0.718750]\n",
      "1447: [discriminator loss: 0.615778, acc: 0.632812] [adversarial loss: 1.544512, acc: 0.015625]\n",
      "1448: [discriminator loss: 0.555119, acc: 0.687500] [adversarial loss: 1.017231, acc: 0.296875]\n",
      "1449: [discriminator loss: 0.555400, acc: 0.726562] [adversarial loss: 1.118914, acc: 0.187500]\n",
      "1450: [discriminator loss: 0.496459, acc: 0.742188] [adversarial loss: 1.042057, acc: 0.187500]\n",
      "1451: [discriminator loss: 0.509423, acc: 0.773438] [adversarial loss: 1.286227, acc: 0.125000]\n",
      "1452: [discriminator loss: 0.510004, acc: 0.789062] [adversarial loss: 1.050406, acc: 0.265625]\n",
      "1453: [discriminator loss: 0.436099, acc: 0.835938] [adversarial loss: 1.406713, acc: 0.125000]\n",
      "1454: [discriminator loss: 0.539145, acc: 0.687500] [adversarial loss: 1.051889, acc: 0.343750]\n",
      "1455: [discriminator loss: 0.527779, acc: 0.718750] [adversarial loss: 1.199718, acc: 0.187500]\n",
      "1456: [discriminator loss: 0.535683, acc: 0.703125] [adversarial loss: 1.096813, acc: 0.218750]\n",
      "1457: [discriminator loss: 0.528525, acc: 0.789062] [adversarial loss: 1.084344, acc: 0.218750]\n",
      "1458: [discriminator loss: 0.524167, acc: 0.781250] [adversarial loss: 1.178415, acc: 0.187500]\n",
      "1459: [discriminator loss: 0.498713, acc: 0.773438] [adversarial loss: 1.546414, acc: 0.062500]\n",
      "1460: [discriminator loss: 0.533783, acc: 0.734375] [adversarial loss: 0.720482, acc: 0.578125]\n",
      "1461: [discriminator loss: 0.649326, acc: 0.601562] [adversarial loss: 1.868152, acc: 0.046875]\n",
      "1462: [discriminator loss: 0.676472, acc: 0.617188] [adversarial loss: 0.680458, acc: 0.578125]\n",
      "1463: [discriminator loss: 0.599812, acc: 0.695312] [adversarial loss: 1.327420, acc: 0.203125]\n",
      "1464: [discriminator loss: 0.529362, acc: 0.734375] [adversarial loss: 1.046478, acc: 0.265625]\n",
      "1465: [discriminator loss: 0.494181, acc: 0.796875] [adversarial loss: 1.494703, acc: 0.140625]\n",
      "1466: [discriminator loss: 0.427390, acc: 0.859375] [adversarial loss: 1.137535, acc: 0.187500]\n",
      "1467: [discriminator loss: 0.571692, acc: 0.726562] [adversarial loss: 1.249238, acc: 0.109375]\n",
      "1468: [discriminator loss: 0.488662, acc: 0.773438] [adversarial loss: 1.158852, acc: 0.140625]\n",
      "1469: [discriminator loss: 0.521975, acc: 0.742188] [adversarial loss: 1.359700, acc: 0.093750]\n",
      "1470: [discriminator loss: 0.527397, acc: 0.765625] [adversarial loss: 0.817621, acc: 0.484375]\n",
      "1471: [discriminator loss: 0.523278, acc: 0.757812] [adversarial loss: 1.349653, acc: 0.125000]\n",
      "1472: [discriminator loss: 0.583715, acc: 0.703125] [adversarial loss: 0.726254, acc: 0.515625]\n",
      "1473: [discriminator loss: 0.552151, acc: 0.664062] [adversarial loss: 1.980570, acc: 0.046875]\n",
      "1474: [discriminator loss: 0.684532, acc: 0.617188] [adversarial loss: 0.665838, acc: 0.593750]\n",
      "1475: [discriminator loss: 0.515790, acc: 0.703125] [adversarial loss: 1.625582, acc: 0.031250]\n",
      "1476: [discriminator loss: 0.500924, acc: 0.773438] [adversarial loss: 1.144922, acc: 0.125000]\n",
      "1477: [discriminator loss: 0.460691, acc: 0.812500] [adversarial loss: 1.165369, acc: 0.218750]\n",
      "1478: [discriminator loss: 0.528514, acc: 0.742188] [adversarial loss: 1.015404, acc: 0.281250]\n",
      "1479: [discriminator loss: 0.480838, acc: 0.781250] [adversarial loss: 1.032703, acc: 0.281250]\n",
      "1480: [discriminator loss: 0.530922, acc: 0.742188] [adversarial loss: 1.399723, acc: 0.109375]\n",
      "1481: [discriminator loss: 0.506142, acc: 0.742188] [adversarial loss: 0.905471, acc: 0.296875]\n",
      "1482: [discriminator loss: 0.558111, acc: 0.710938] [adversarial loss: 1.418584, acc: 0.093750]\n",
      "1483: [discriminator loss: 0.491844, acc: 0.750000] [adversarial loss: 0.619386, acc: 0.687500]\n",
      "1484: [discriminator loss: 0.575079, acc: 0.703125] [adversarial loss: 2.133290, acc: 0.015625]\n",
      "1485: [discriminator loss: 0.612766, acc: 0.687500] [adversarial loss: 0.736650, acc: 0.515625]\n",
      "1486: [discriminator loss: 0.565011, acc: 0.703125] [adversarial loss: 1.715357, acc: 0.046875]\n",
      "1487: [discriminator loss: 0.575919, acc: 0.703125] [adversarial loss: 0.905868, acc: 0.406250]\n",
      "1488: [discriminator loss: 0.502041, acc: 0.742188] [adversarial loss: 1.317296, acc: 0.109375]\n",
      "1489: [discriminator loss: 0.556739, acc: 0.718750] [adversarial loss: 0.937343, acc: 0.312500]\n",
      "1490: [discriminator loss: 0.480339, acc: 0.781250] [adversarial loss: 1.393129, acc: 0.078125]\n",
      "1491: [discriminator loss: 0.488321, acc: 0.796875] [adversarial loss: 1.041018, acc: 0.250000]\n",
      "1492: [discriminator loss: 0.473038, acc: 0.773438] [adversarial loss: 1.568299, acc: 0.031250]\n",
      "1493: [discriminator loss: 0.567812, acc: 0.718750] [adversarial loss: 0.666902, acc: 0.640625]\n",
      "1494: [discriminator loss: 0.548279, acc: 0.671875] [adversarial loss: 2.073794, acc: 0.000000]\n",
      "1495: [discriminator loss: 0.583723, acc: 0.695312] [adversarial loss: 0.784527, acc: 0.500000]\n",
      "1496: [discriminator loss: 0.548500, acc: 0.710938] [adversarial loss: 1.826844, acc: 0.015625]\n",
      "1497: [discriminator loss: 0.639828, acc: 0.625000] [adversarial loss: 0.717738, acc: 0.593750]\n",
      "1498: [discriminator loss: 0.569989, acc: 0.648438] [adversarial loss: 1.578661, acc: 0.078125]\n",
      "1499: [discriminator loss: 0.549839, acc: 0.718750] [adversarial loss: 0.940133, acc: 0.312500]\n",
      "1500: [discriminator loss: 0.527877, acc: 0.765625] [adversarial loss: 1.315033, acc: 0.093750]\n",
      "1501: [discriminator loss: 0.520764, acc: 0.734375] [adversarial loss: 1.223431, acc: 0.125000]\n",
      "1502: [discriminator loss: 0.522491, acc: 0.734375] [adversarial loss: 1.145650, acc: 0.156250]\n",
      "1503: [discriminator loss: 0.486892, acc: 0.757812] [adversarial loss: 0.996669, acc: 0.281250]\n",
      "1504: [discriminator loss: 0.479068, acc: 0.789062] [adversarial loss: 1.195150, acc: 0.187500]\n",
      "1505: [discriminator loss: 0.563321, acc: 0.726562] [adversarial loss: 1.080097, acc: 0.203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506: [discriminator loss: 0.584161, acc: 0.726562] [adversarial loss: 1.048748, acc: 0.265625]\n",
      "1507: [discriminator loss: 0.508379, acc: 0.765625] [adversarial loss: 1.432913, acc: 0.078125]\n",
      "1508: [discriminator loss: 0.563663, acc: 0.656250] [adversarial loss: 0.785492, acc: 0.453125]\n",
      "1509: [discriminator loss: 0.575282, acc: 0.687500] [adversarial loss: 1.701401, acc: 0.031250]\n",
      "1510: [discriminator loss: 0.616917, acc: 0.695312] [adversarial loss: 0.595397, acc: 0.687500]\n",
      "1511: [discriminator loss: 0.608086, acc: 0.625000] [adversarial loss: 1.584641, acc: 0.078125]\n",
      "1512: [discriminator loss: 0.500782, acc: 0.695312] [adversarial loss: 0.969323, acc: 0.265625]\n",
      "1513: [discriminator loss: 0.519878, acc: 0.757812] [adversarial loss: 1.604083, acc: 0.046875]\n",
      "1514: [discriminator loss: 0.579526, acc: 0.710938] [adversarial loss: 0.895357, acc: 0.359375]\n",
      "1515: [discriminator loss: 0.441268, acc: 0.835938] [adversarial loss: 1.182824, acc: 0.203125]\n",
      "1516: [discriminator loss: 0.517421, acc: 0.750000] [adversarial loss: 0.969504, acc: 0.359375]\n",
      "1517: [discriminator loss: 0.507339, acc: 0.750000] [adversarial loss: 1.279395, acc: 0.140625]\n",
      "1518: [discriminator loss: 0.586851, acc: 0.640625] [adversarial loss: 0.883081, acc: 0.312500]\n",
      "1519: [discriminator loss: 0.538253, acc: 0.718750] [adversarial loss: 1.497357, acc: 0.078125]\n",
      "1520: [discriminator loss: 0.512933, acc: 0.765625] [adversarial loss: 0.934404, acc: 0.343750]\n",
      "1521: [discriminator loss: 0.531200, acc: 0.734375] [adversarial loss: 1.428932, acc: 0.078125]\n",
      "1522: [discriminator loss: 0.495360, acc: 0.742188] [adversarial loss: 0.809891, acc: 0.437500]\n",
      "1523: [discriminator loss: 0.429792, acc: 0.820312] [adversarial loss: 1.300462, acc: 0.156250]\n",
      "1524: [discriminator loss: 0.479814, acc: 0.796875] [adversarial loss: 0.879990, acc: 0.343750]\n",
      "1525: [discriminator loss: 0.548349, acc: 0.718750] [adversarial loss: 1.574899, acc: 0.062500]\n",
      "1526: [discriminator loss: 0.522058, acc: 0.734375] [adversarial loss: 0.832027, acc: 0.421875]\n",
      "1527: [discriminator loss: 0.511418, acc: 0.757812] [adversarial loss: 1.725963, acc: 0.031250]\n",
      "1528: [discriminator loss: 0.561369, acc: 0.703125] [adversarial loss: 0.719952, acc: 0.515625]\n",
      "1529: [discriminator loss: 0.574011, acc: 0.664062] [adversarial loss: 1.930514, acc: 0.015625]\n",
      "1530: [discriminator loss: 0.654274, acc: 0.617188] [adversarial loss: 0.730821, acc: 0.468750]\n",
      "1531: [discriminator loss: 0.542712, acc: 0.718750] [adversarial loss: 1.303320, acc: 0.140625]\n",
      "1532: [discriminator loss: 0.508100, acc: 0.781250] [adversarial loss: 1.124853, acc: 0.203125]\n",
      "1533: [discriminator loss: 0.455861, acc: 0.796875] [adversarial loss: 1.347096, acc: 0.140625]\n",
      "1534: [discriminator loss: 0.502714, acc: 0.765625] [adversarial loss: 0.883262, acc: 0.390625]\n",
      "1535: [discriminator loss: 0.536159, acc: 0.710938] [adversarial loss: 1.630869, acc: 0.093750]\n",
      "1536: [discriminator loss: 0.555510, acc: 0.726562] [adversarial loss: 0.723833, acc: 0.562500]\n",
      "1537: [discriminator loss: 0.542835, acc: 0.726562] [adversarial loss: 1.597793, acc: 0.046875]\n",
      "1538: [discriminator loss: 0.482618, acc: 0.765625] [adversarial loss: 0.787370, acc: 0.437500]\n",
      "1539: [discriminator loss: 0.537539, acc: 0.734375] [adversarial loss: 1.309033, acc: 0.093750]\n",
      "1540: [discriminator loss: 0.511328, acc: 0.726562] [adversarial loss: 0.966050, acc: 0.328125]\n",
      "1541: [discriminator loss: 0.506059, acc: 0.781250] [adversarial loss: 1.495803, acc: 0.062500]\n",
      "1542: [discriminator loss: 0.494559, acc: 0.742188] [adversarial loss: 0.956780, acc: 0.296875]\n",
      "1543: [discriminator loss: 0.622270, acc: 0.687500] [adversarial loss: 1.705059, acc: 0.046875]\n",
      "1544: [discriminator loss: 0.515722, acc: 0.757812] [adversarial loss: 0.891468, acc: 0.296875]\n",
      "1545: [discriminator loss: 0.521955, acc: 0.687500] [adversarial loss: 1.582469, acc: 0.078125]\n",
      "1546: [discriminator loss: 0.579800, acc: 0.695312] [adversarial loss: 0.677509, acc: 0.593750]\n",
      "1547: [discriminator loss: 0.597979, acc: 0.640625] [adversarial loss: 1.710757, acc: 0.015625]\n",
      "1548: [discriminator loss: 0.552030, acc: 0.726562] [adversarial loss: 0.998946, acc: 0.281250]\n",
      "1549: [discriminator loss: 0.478653, acc: 0.789062] [adversarial loss: 1.154379, acc: 0.156250]\n",
      "1550: [discriminator loss: 0.523476, acc: 0.765625] [adversarial loss: 1.245006, acc: 0.125000]\n",
      "1551: [discriminator loss: 0.530043, acc: 0.726562] [adversarial loss: 1.137999, acc: 0.265625]\n",
      "1552: [discriminator loss: 0.547846, acc: 0.742188] [adversarial loss: 1.229844, acc: 0.171875]\n",
      "1553: [discriminator loss: 0.523435, acc: 0.734375] [adversarial loss: 1.061520, acc: 0.281250]\n",
      "1554: [discriminator loss: 0.490217, acc: 0.742188] [adversarial loss: 1.384872, acc: 0.046875]\n",
      "1555: [discriminator loss: 0.466908, acc: 0.757812] [adversarial loss: 1.074262, acc: 0.218750]\n",
      "1556: [discriminator loss: 0.527675, acc: 0.734375] [adversarial loss: 1.415701, acc: 0.093750]\n",
      "1557: [discriminator loss: 0.592103, acc: 0.695312] [adversarial loss: 0.793248, acc: 0.437500]\n",
      "1558: [discriminator loss: 0.475173, acc: 0.773438] [adversarial loss: 1.793601, acc: 0.015625]\n",
      "1559: [discriminator loss: 0.536877, acc: 0.734375] [adversarial loss: 0.726848, acc: 0.531250]\n",
      "1560: [discriminator loss: 0.550383, acc: 0.710938] [adversarial loss: 2.015293, acc: 0.015625]\n",
      "1561: [discriminator loss: 0.511806, acc: 0.710938] [adversarial loss: 0.918689, acc: 0.359375]\n",
      "1562: [discriminator loss: 0.573318, acc: 0.765625] [adversarial loss: 1.487931, acc: 0.109375]\n",
      "1563: [discriminator loss: 0.535545, acc: 0.742188] [adversarial loss: 0.862042, acc: 0.421875]\n",
      "1564: [discriminator loss: 0.526793, acc: 0.734375] [adversarial loss: 1.609434, acc: 0.015625]\n",
      "1565: [discriminator loss: 0.585451, acc: 0.664062] [adversarial loss: 0.866453, acc: 0.375000]\n",
      "1566: [discriminator loss: 0.525784, acc: 0.773438] [adversarial loss: 1.469406, acc: 0.015625]\n",
      "1567: [discriminator loss: 0.513661, acc: 0.796875] [adversarial loss: 0.912623, acc: 0.343750]\n",
      "1568: [discriminator loss: 0.583153, acc: 0.664062] [adversarial loss: 1.476337, acc: 0.093750]\n",
      "1569: [discriminator loss: 0.547644, acc: 0.710938] [adversarial loss: 1.134712, acc: 0.187500]\n",
      "1570: [discriminator loss: 0.477951, acc: 0.812500] [adversarial loss: 0.911187, acc: 0.359375]\n",
      "1571: [discriminator loss: 0.463066, acc: 0.789062] [adversarial loss: 1.223762, acc: 0.125000]\n",
      "1572: [discriminator loss: 0.551504, acc: 0.726562] [adversarial loss: 1.002953, acc: 0.281250]\n",
      "1573: [discriminator loss: 0.478205, acc: 0.742188] [adversarial loss: 1.420776, acc: 0.156250]\n",
      "1574: [discriminator loss: 0.549870, acc: 0.718750] [adversarial loss: 0.985873, acc: 0.312500]\n",
      "1575: [discriminator loss: 0.506040, acc: 0.750000] [adversarial loss: 1.435645, acc: 0.109375]\n",
      "1576: [discriminator loss: 0.560071, acc: 0.718750] [adversarial loss: 0.854566, acc: 0.406250]\n",
      "1577: [discriminator loss: 0.497381, acc: 0.765625] [adversarial loss: 2.030141, acc: 0.031250]\n",
      "1578: [discriminator loss: 0.642453, acc: 0.648438] [adversarial loss: 0.670188, acc: 0.609375]\n",
      "1579: [discriminator loss: 0.620661, acc: 0.625000] [adversarial loss: 1.789923, acc: 0.046875]\n",
      "1580: [discriminator loss: 0.613513, acc: 0.679688] [adversarial loss: 0.856488, acc: 0.421875]\n",
      "1581: [discriminator loss: 0.560500, acc: 0.718750] [adversarial loss: 1.514835, acc: 0.031250]\n",
      "1582: [discriminator loss: 0.583287, acc: 0.734375] [adversarial loss: 1.337810, acc: 0.078125]\n",
      "1583: [discriminator loss: 0.535444, acc: 0.757812] [adversarial loss: 1.045314, acc: 0.187500]\n",
      "1584: [discriminator loss: 0.500856, acc: 0.804688] [adversarial loss: 1.395141, acc: 0.109375]\n",
      "1585: [discriminator loss: 0.474427, acc: 0.796875] [adversarial loss: 0.787735, acc: 0.406250]\n",
      "1586: [discriminator loss: 0.462855, acc: 0.789062] [adversarial loss: 1.476793, acc: 0.156250]\n",
      "1587: [discriminator loss: 0.592420, acc: 0.695312] [adversarial loss: 0.789107, acc: 0.390625]\n",
      "1588: [discriminator loss: 0.568845, acc: 0.734375] [adversarial loss: 1.642928, acc: 0.015625]\n",
      "1589: [discriminator loss: 0.513736, acc: 0.750000] [adversarial loss: 1.091077, acc: 0.218750]\n",
      "1590: [discriminator loss: 0.455223, acc: 0.820312] [adversarial loss: 1.341844, acc: 0.109375]\n",
      "1591: [discriminator loss: 0.485931, acc: 0.789062] [adversarial loss: 1.108754, acc: 0.250000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592: [discriminator loss: 0.481923, acc: 0.781250] [adversarial loss: 1.383908, acc: 0.171875]\n",
      "1593: [discriminator loss: 0.504672, acc: 0.757812] [adversarial loss: 1.284299, acc: 0.140625]\n",
      "1594: [discriminator loss: 0.485491, acc: 0.757812] [adversarial loss: 1.075230, acc: 0.171875]\n",
      "1595: [discriminator loss: 0.540952, acc: 0.718750] [adversarial loss: 1.654309, acc: 0.109375]\n",
      "1596: [discriminator loss: 0.576145, acc: 0.718750] [adversarial loss: 0.749487, acc: 0.515625]\n",
      "1597: [discriminator loss: 0.623798, acc: 0.656250] [adversarial loss: 2.035941, acc: 0.046875]\n",
      "1598: [discriminator loss: 0.599822, acc: 0.687500] [adversarial loss: 0.838151, acc: 0.375000]\n",
      "1599: [discriminator loss: 0.577480, acc: 0.679688] [adversarial loss: 1.601082, acc: 0.062500]\n",
      "1600: [discriminator loss: 0.618754, acc: 0.656250] [adversarial loss: 0.899672, acc: 0.375000]\n",
      "1601: [discriminator loss: 0.557707, acc: 0.687500] [adversarial loss: 1.312057, acc: 0.125000]\n",
      "1602: [discriminator loss: 0.559967, acc: 0.703125] [adversarial loss: 1.141740, acc: 0.187500]\n",
      "1603: [discriminator loss: 0.574872, acc: 0.679688] [adversarial loss: 1.171436, acc: 0.109375]\n",
      "1604: [discriminator loss: 0.540787, acc: 0.734375] [adversarial loss: 0.816001, acc: 0.468750]\n",
      "1605: [discriminator loss: 0.567515, acc: 0.718750] [adversarial loss: 1.613767, acc: 0.031250]\n",
      "1606: [discriminator loss: 0.530257, acc: 0.671875] [adversarial loss: 0.926022, acc: 0.312500]\n",
      "1607: [discriminator loss: 0.520714, acc: 0.757812] [adversarial loss: 1.667697, acc: 0.078125]\n",
      "1608: [discriminator loss: 0.556543, acc: 0.710938] [adversarial loss: 0.936993, acc: 0.343750]\n",
      "1609: [discriminator loss: 0.583211, acc: 0.687500] [adversarial loss: 1.309951, acc: 0.171875]\n",
      "1610: [discriminator loss: 0.537962, acc: 0.757812] [adversarial loss: 0.813561, acc: 0.390625]\n",
      "1611: [discriminator loss: 0.567369, acc: 0.726562] [adversarial loss: 1.290960, acc: 0.140625]\n",
      "1612: [discriminator loss: 0.512068, acc: 0.781250] [adversarial loss: 0.991365, acc: 0.281250]\n",
      "1613: [discriminator loss: 0.519607, acc: 0.750000] [adversarial loss: 1.209981, acc: 0.125000]\n",
      "1614: [discriminator loss: 0.465563, acc: 0.757812] [adversarial loss: 1.203671, acc: 0.156250]\n",
      "1615: [discriminator loss: 0.438083, acc: 0.859375] [adversarial loss: 1.130623, acc: 0.171875]\n",
      "1616: [discriminator loss: 0.469703, acc: 0.757812] [adversarial loss: 1.619416, acc: 0.078125]\n",
      "1617: [discriminator loss: 0.557294, acc: 0.750000] [adversarial loss: 0.702802, acc: 0.546875]\n",
      "1618: [discriminator loss: 0.634670, acc: 0.742188] [adversarial loss: 1.718186, acc: 0.093750]\n",
      "1619: [discriminator loss: 0.616421, acc: 0.656250] [adversarial loss: 0.779629, acc: 0.468750]\n",
      "1620: [discriminator loss: 0.570770, acc: 0.664062] [adversarial loss: 1.475336, acc: 0.062500]\n",
      "1621: [discriminator loss: 0.549205, acc: 0.718750] [adversarial loss: 1.049384, acc: 0.312500]\n",
      "1622: [discriminator loss: 0.511476, acc: 0.750000] [adversarial loss: 1.495875, acc: 0.078125]\n",
      "1623: [discriminator loss: 0.509625, acc: 0.726562] [adversarial loss: 0.916957, acc: 0.343750]\n",
      "1624: [discriminator loss: 0.520827, acc: 0.718750] [adversarial loss: 1.477428, acc: 0.093750]\n",
      "1625: [discriminator loss: 0.545312, acc: 0.695312] [adversarial loss: 0.842812, acc: 0.421875]\n",
      "1626: [discriminator loss: 0.484784, acc: 0.765625] [adversarial loss: 1.729890, acc: 0.078125]\n",
      "1627: [discriminator loss: 0.532748, acc: 0.718750] [adversarial loss: 0.774063, acc: 0.515625]\n",
      "1628: [discriminator loss: 0.487509, acc: 0.773438] [adversarial loss: 1.250621, acc: 0.140625]\n",
      "1629: [discriminator loss: 0.437101, acc: 0.812500] [adversarial loss: 0.992068, acc: 0.359375]\n",
      "1630: [discriminator loss: 0.488628, acc: 0.765625] [adversarial loss: 1.080420, acc: 0.171875]\n",
      "1631: [discriminator loss: 0.459285, acc: 0.781250] [adversarial loss: 1.269424, acc: 0.171875]\n",
      "1632: [discriminator loss: 0.497367, acc: 0.765625] [adversarial loss: 0.872653, acc: 0.390625]\n",
      "1633: [discriminator loss: 0.530500, acc: 0.734375] [adversarial loss: 1.566837, acc: 0.046875]\n",
      "1634: [discriminator loss: 0.512017, acc: 0.734375] [adversarial loss: 0.762018, acc: 0.546875]\n",
      "1635: [discriminator loss: 0.477232, acc: 0.773438] [adversarial loss: 1.906179, acc: 0.031250]\n",
      "1636: [discriminator loss: 0.627532, acc: 0.656250] [adversarial loss: 0.835858, acc: 0.484375]\n",
      "1637: [discriminator loss: 0.549024, acc: 0.742188] [adversarial loss: 1.549943, acc: 0.062500]\n",
      "1638: [discriminator loss: 0.588019, acc: 0.695312] [adversarial loss: 0.825678, acc: 0.484375]\n",
      "1639: [discriminator loss: 0.509285, acc: 0.750000] [adversarial loss: 1.521175, acc: 0.031250]\n",
      "1640: [discriminator loss: 0.584767, acc: 0.687500] [adversarial loss: 0.850134, acc: 0.484375]\n",
      "1641: [discriminator loss: 0.493398, acc: 0.695312] [adversarial loss: 1.519027, acc: 0.109375]\n",
      "1642: [discriminator loss: 0.590036, acc: 0.679688] [adversarial loss: 0.803103, acc: 0.515625]\n",
      "1643: [discriminator loss: 0.568986, acc: 0.671875] [adversarial loss: 1.672179, acc: 0.031250]\n",
      "1644: [discriminator loss: 0.487952, acc: 0.734375] [adversarial loss: 1.078956, acc: 0.281250]\n",
      "1645: [discriminator loss: 0.469350, acc: 0.765625] [adversarial loss: 1.280224, acc: 0.125000]\n",
      "1646: [discriminator loss: 0.475846, acc: 0.750000] [adversarial loss: 1.067335, acc: 0.187500]\n",
      "1647: [discriminator loss: 0.518094, acc: 0.703125] [adversarial loss: 1.392990, acc: 0.078125]\n",
      "1648: [discriminator loss: 0.506306, acc: 0.757812] [adversarial loss: 1.109175, acc: 0.296875]\n",
      "1649: [discriminator loss: 0.479443, acc: 0.750000] [adversarial loss: 1.345028, acc: 0.203125]\n",
      "1650: [discriminator loss: 0.489419, acc: 0.773438] [adversarial loss: 1.022054, acc: 0.296875]\n",
      "1651: [discriminator loss: 0.497774, acc: 0.765625] [adversarial loss: 1.563277, acc: 0.031250]\n",
      "1652: [discriminator loss: 0.516165, acc: 0.765625] [adversarial loss: 0.857082, acc: 0.375000]\n",
      "1653: [discriminator loss: 0.498789, acc: 0.742188] [adversarial loss: 1.728632, acc: 0.046875]\n",
      "1654: [discriminator loss: 0.559761, acc: 0.765625] [adversarial loss: 0.910906, acc: 0.375000]\n",
      "1655: [discriminator loss: 0.540194, acc: 0.773438] [adversarial loss: 1.770760, acc: 0.078125]\n",
      "1656: [discriminator loss: 0.560167, acc: 0.750000] [adversarial loss: 0.599098, acc: 0.687500]\n",
      "1657: [discriminator loss: 0.586274, acc: 0.687500] [adversarial loss: 1.806156, acc: 0.062500]\n",
      "1658: [discriminator loss: 0.569550, acc: 0.656250] [adversarial loss: 0.670083, acc: 0.578125]\n",
      "1659: [discriminator loss: 0.528040, acc: 0.687500] [adversarial loss: 1.348769, acc: 0.140625]\n",
      "1660: [discriminator loss: 0.588111, acc: 0.679688] [adversarial loss: 0.908463, acc: 0.359375]\n",
      "1661: [discriminator loss: 0.490080, acc: 0.750000] [adversarial loss: 1.559442, acc: 0.109375]\n",
      "1662: [discriminator loss: 0.481095, acc: 0.765625] [adversarial loss: 0.898724, acc: 0.390625]\n",
      "1663: [discriminator loss: 0.502919, acc: 0.750000] [adversarial loss: 1.414293, acc: 0.062500]\n",
      "1664: [discriminator loss: 0.470060, acc: 0.781250] [adversarial loss: 0.935016, acc: 0.421875]\n",
      "1665: [discriminator loss: 0.515425, acc: 0.757812] [adversarial loss: 1.133864, acc: 0.265625]\n",
      "1666: [discriminator loss: 0.497290, acc: 0.742188] [adversarial loss: 1.301353, acc: 0.187500]\n",
      "1667: [discriminator loss: 0.555640, acc: 0.734375] [adversarial loss: 1.155869, acc: 0.281250]\n",
      "1668: [discriminator loss: 0.596755, acc: 0.671875] [adversarial loss: 1.011224, acc: 0.296875]\n",
      "1669: [discriminator loss: 0.445441, acc: 0.789062] [adversarial loss: 1.639876, acc: 0.109375]\n",
      "1670: [discriminator loss: 0.527163, acc: 0.718750] [adversarial loss: 0.928579, acc: 0.281250]\n",
      "1671: [discriminator loss: 0.605254, acc: 0.703125] [adversarial loss: 1.678331, acc: 0.046875]\n",
      "1672: [discriminator loss: 0.565156, acc: 0.726562] [adversarial loss: 0.746100, acc: 0.500000]\n",
      "1673: [discriminator loss: 0.536551, acc: 0.765625] [adversarial loss: 1.587324, acc: 0.093750]\n",
      "1674: [discriminator loss: 0.607775, acc: 0.671875] [adversarial loss: 0.967037, acc: 0.406250]\n",
      "1675: [discriminator loss: 0.511246, acc: 0.734375] [adversarial loss: 1.693726, acc: 0.046875]\n",
      "1676: [discriminator loss: 0.576024, acc: 0.695312] [adversarial loss: 0.890093, acc: 0.421875]\n",
      "1677: [discriminator loss: 0.518375, acc: 0.726562] [adversarial loss: 1.403099, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1678: [discriminator loss: 0.544617, acc: 0.703125] [adversarial loss: 0.758061, acc: 0.500000]\n",
      "1679: [discriminator loss: 0.581901, acc: 0.648438] [adversarial loss: 1.570051, acc: 0.031250]\n",
      "1680: [discriminator loss: 0.522886, acc: 0.718750] [adversarial loss: 0.844333, acc: 0.500000]\n",
      "1681: [discriminator loss: 0.566431, acc: 0.718750] [adversarial loss: 1.454058, acc: 0.093750]\n",
      "1682: [discriminator loss: 0.530329, acc: 0.757812] [adversarial loss: 1.127785, acc: 0.328125]\n",
      "1683: [discriminator loss: 0.535754, acc: 0.742188] [adversarial loss: 1.259563, acc: 0.171875]\n",
      "1684: [discriminator loss: 0.454009, acc: 0.773438] [adversarial loss: 1.148638, acc: 0.312500]\n",
      "1685: [discriminator loss: 0.499097, acc: 0.750000] [adversarial loss: 1.101095, acc: 0.234375]\n",
      "1686: [discriminator loss: 0.424892, acc: 0.843750] [adversarial loss: 1.026662, acc: 0.281250]\n",
      "1687: [discriminator loss: 0.471735, acc: 0.773438] [adversarial loss: 1.490254, acc: 0.125000]\n",
      "1688: [discriminator loss: 0.505227, acc: 0.726562] [adversarial loss: 0.965415, acc: 0.359375]\n",
      "1689: [discriminator loss: 0.554648, acc: 0.710938] [adversarial loss: 1.503195, acc: 0.078125]\n",
      "1690: [discriminator loss: 0.527840, acc: 0.742188] [adversarial loss: 0.809097, acc: 0.500000]\n",
      "1691: [discriminator loss: 0.536400, acc: 0.710938] [adversarial loss: 2.088350, acc: 0.031250]\n",
      "1692: [discriminator loss: 0.599032, acc: 0.718750] [adversarial loss: 0.999364, acc: 0.296875]\n",
      "1693: [discriminator loss: 0.589823, acc: 0.664062] [adversarial loss: 1.833151, acc: 0.000000]\n",
      "1694: [discriminator loss: 0.555868, acc: 0.726562] [adversarial loss: 0.803480, acc: 0.453125]\n",
      "1695: [discriminator loss: 0.560987, acc: 0.664062] [adversarial loss: 1.623576, acc: 0.062500]\n",
      "1696: [discriminator loss: 0.555356, acc: 0.710938] [adversarial loss: 0.912397, acc: 0.375000]\n",
      "1697: [discriminator loss: 0.511488, acc: 0.734375] [adversarial loss: 1.546279, acc: 0.171875]\n",
      "1698: [discriminator loss: 0.547264, acc: 0.742188] [adversarial loss: 1.119210, acc: 0.234375]\n",
      "1699: [discriminator loss: 0.492266, acc: 0.757812] [adversarial loss: 1.153054, acc: 0.250000]\n",
      "1700: [discriminator loss: 0.574641, acc: 0.671875] [adversarial loss: 1.194767, acc: 0.187500]\n",
      "1701: [discriminator loss: 0.547481, acc: 0.710938] [adversarial loss: 1.063025, acc: 0.296875]\n",
      "1702: [discriminator loss: 0.498251, acc: 0.765625] [adversarial loss: 1.165565, acc: 0.218750]\n",
      "1703: [discriminator loss: 0.482096, acc: 0.812500] [adversarial loss: 1.268738, acc: 0.187500]\n",
      "1704: [discriminator loss: 0.454597, acc: 0.781250] [adversarial loss: 1.209227, acc: 0.250000]\n",
      "1705: [discriminator loss: 0.512288, acc: 0.781250] [adversarial loss: 1.678520, acc: 0.062500]\n",
      "1706: [discriminator loss: 0.558708, acc: 0.734375] [adversarial loss: 0.742450, acc: 0.578125]\n",
      "1707: [discriminator loss: 0.603658, acc: 0.671875] [adversarial loss: 2.068315, acc: 0.031250]\n",
      "1708: [discriminator loss: 0.540925, acc: 0.687500] [adversarial loss: 0.909108, acc: 0.421875]\n",
      "1709: [discriminator loss: 0.604948, acc: 0.664062] [adversarial loss: 1.268925, acc: 0.125000]\n",
      "1710: [discriminator loss: 0.490575, acc: 0.773438] [adversarial loss: 1.165956, acc: 0.265625]\n",
      "1711: [discriminator loss: 0.513162, acc: 0.734375] [adversarial loss: 1.526884, acc: 0.156250]\n",
      "1712: [discriminator loss: 0.572726, acc: 0.687500] [adversarial loss: 1.169413, acc: 0.156250]\n",
      "1713: [discriminator loss: 0.525203, acc: 0.742188] [adversarial loss: 1.465111, acc: 0.156250]\n",
      "1714: [discriminator loss: 0.489746, acc: 0.796875] [adversarial loss: 1.141622, acc: 0.218750]\n",
      "1715: [discriminator loss: 0.492538, acc: 0.789062] [adversarial loss: 1.226555, acc: 0.140625]\n",
      "1716: [discriminator loss: 0.512882, acc: 0.757812] [adversarial loss: 1.109917, acc: 0.218750]\n",
      "1717: [discriminator loss: 0.467369, acc: 0.804688] [adversarial loss: 1.194980, acc: 0.203125]\n",
      "1718: [discriminator loss: 0.515803, acc: 0.742188] [adversarial loss: 0.999838, acc: 0.265625]\n",
      "1719: [discriminator loss: 0.507417, acc: 0.773438] [adversarial loss: 1.306818, acc: 0.203125]\n",
      "1720: [discriminator loss: 0.597450, acc: 0.734375] [adversarial loss: 0.965545, acc: 0.406250]\n",
      "1721: [discriminator loss: 0.526778, acc: 0.718750] [adversarial loss: 1.859201, acc: 0.015625]\n",
      "1722: [discriminator loss: 0.549319, acc: 0.742188] [adversarial loss: 0.562833, acc: 0.687500]\n",
      "1723: [discriminator loss: 0.599004, acc: 0.656250] [adversarial loss: 1.552862, acc: 0.171875]\n",
      "1724: [discriminator loss: 0.506110, acc: 0.742188] [adversarial loss: 0.798352, acc: 0.500000]\n",
      "1725: [discriminator loss: 0.470491, acc: 0.812500] [adversarial loss: 1.503444, acc: 0.062500]\n",
      "1726: [discriminator loss: 0.549912, acc: 0.726562] [adversarial loss: 1.218875, acc: 0.187500]\n",
      "1727: [discriminator loss: 0.536224, acc: 0.703125] [adversarial loss: 1.145754, acc: 0.187500]\n",
      "1728: [discriminator loss: 0.520148, acc: 0.718750] [adversarial loss: 1.652514, acc: 0.031250]\n",
      "1729: [discriminator loss: 0.534711, acc: 0.742188] [adversarial loss: 0.674394, acc: 0.609375]\n",
      "1730: [discriminator loss: 0.539921, acc: 0.695312] [adversarial loss: 1.863614, acc: 0.093750]\n",
      "1731: [discriminator loss: 0.605903, acc: 0.671875] [adversarial loss: 0.694557, acc: 0.578125]\n",
      "1732: [discriminator loss: 0.587100, acc: 0.734375] [adversarial loss: 1.367310, acc: 0.109375]\n",
      "1733: [discriminator loss: 0.511106, acc: 0.742188] [adversarial loss: 1.013234, acc: 0.343750]\n",
      "1734: [discriminator loss: 0.460429, acc: 0.750000] [adversarial loss: 1.158679, acc: 0.218750]\n",
      "1735: [discriminator loss: 0.521461, acc: 0.757812] [adversarial loss: 0.835245, acc: 0.500000]\n",
      "1736: [discriminator loss: 0.564880, acc: 0.671875] [adversarial loss: 1.440475, acc: 0.062500]\n",
      "1737: [discriminator loss: 0.539656, acc: 0.757812] [adversarial loss: 0.645968, acc: 0.718750]\n",
      "1738: [discriminator loss: 0.591838, acc: 0.679688] [adversarial loss: 1.629107, acc: 0.062500]\n",
      "1739: [discriminator loss: 0.415707, acc: 0.828125] [adversarial loss: 0.976459, acc: 0.375000]\n",
      "1740: [discriminator loss: 0.472334, acc: 0.773438] [adversarial loss: 1.498109, acc: 0.109375]\n",
      "1741: [discriminator loss: 0.536725, acc: 0.734375] [adversarial loss: 0.826736, acc: 0.406250]\n",
      "1742: [discriminator loss: 0.625881, acc: 0.703125] [adversarial loss: 1.759437, acc: 0.062500]\n",
      "1743: [discriminator loss: 0.582886, acc: 0.656250] [adversarial loss: 0.827118, acc: 0.500000]\n",
      "1744: [discriminator loss: 0.544495, acc: 0.726562] [adversarial loss: 1.445798, acc: 0.125000]\n",
      "1745: [discriminator loss: 0.566963, acc: 0.742188] [adversarial loss: 0.896623, acc: 0.359375]\n",
      "1746: [discriminator loss: 0.462864, acc: 0.796875] [adversarial loss: 1.285694, acc: 0.203125]\n",
      "1747: [discriminator loss: 0.524526, acc: 0.773438] [adversarial loss: 1.059840, acc: 0.328125]\n",
      "1748: [discriminator loss: 0.471601, acc: 0.820312] [adversarial loss: 1.346126, acc: 0.140625]\n",
      "1749: [discriminator loss: 0.467804, acc: 0.789062] [adversarial loss: 1.430264, acc: 0.109375]\n",
      "1750: [discriminator loss: 0.533545, acc: 0.687500] [adversarial loss: 1.055962, acc: 0.296875]\n",
      "1751: [discriminator loss: 0.494551, acc: 0.734375] [adversarial loss: 1.214060, acc: 0.234375]\n",
      "1752: [discriminator loss: 0.469633, acc: 0.757812] [adversarial loss: 1.310598, acc: 0.156250]\n",
      "1753: [discriminator loss: 0.535526, acc: 0.757812] [adversarial loss: 1.437060, acc: 0.093750]\n",
      "1754: [discriminator loss: 0.476782, acc: 0.757812] [adversarial loss: 0.916969, acc: 0.453125]\n",
      "1755: [discriminator loss: 0.609318, acc: 0.664062] [adversarial loss: 1.801790, acc: 0.046875]\n",
      "1756: [discriminator loss: 0.574226, acc: 0.664062] [adversarial loss: 0.740065, acc: 0.468750]\n",
      "1757: [discriminator loss: 0.466393, acc: 0.781250] [adversarial loss: 1.892709, acc: 0.000000]\n",
      "1758: [discriminator loss: 0.516194, acc: 0.726562] [adversarial loss: 0.842978, acc: 0.406250]\n",
      "1759: [discriminator loss: 0.541125, acc: 0.695312] [adversarial loss: 1.817895, acc: 0.046875]\n",
      "1760: [discriminator loss: 0.515873, acc: 0.765625] [adversarial loss: 0.764994, acc: 0.484375]\n",
      "1761: [discriminator loss: 0.548968, acc: 0.695312] [adversarial loss: 1.423529, acc: 0.125000]\n",
      "1762: [discriminator loss: 0.468763, acc: 0.765625] [adversarial loss: 0.914498, acc: 0.312500]\n",
      "1763: [discriminator loss: 0.496280, acc: 0.781250] [adversarial loss: 1.654161, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764: [discriminator loss: 0.565879, acc: 0.648438] [adversarial loss: 0.879765, acc: 0.406250]\n",
      "1765: [discriminator loss: 0.528634, acc: 0.742188] [adversarial loss: 1.422581, acc: 0.078125]\n",
      "1766: [discriminator loss: 0.493706, acc: 0.765625] [adversarial loss: 0.959148, acc: 0.328125]\n",
      "1767: [discriminator loss: 0.547346, acc: 0.687500] [adversarial loss: 1.214679, acc: 0.125000]\n",
      "1768: [discriminator loss: 0.535336, acc: 0.750000] [adversarial loss: 0.956340, acc: 0.343750]\n",
      "1769: [discriminator loss: 0.553625, acc: 0.718750] [adversarial loss: 1.453205, acc: 0.093750]\n",
      "1770: [discriminator loss: 0.529043, acc: 0.765625] [adversarial loss: 1.380778, acc: 0.156250]\n",
      "1771: [discriminator loss: 0.535745, acc: 0.742188] [adversarial loss: 1.052145, acc: 0.343750]\n",
      "1772: [discriminator loss: 0.469246, acc: 0.757812] [adversarial loss: 1.350436, acc: 0.140625]\n",
      "1773: [discriminator loss: 0.506936, acc: 0.679688] [adversarial loss: 0.759977, acc: 0.515625]\n",
      "1774: [discriminator loss: 0.561218, acc: 0.718750] [adversarial loss: 1.883850, acc: 0.046875]\n",
      "1775: [discriminator loss: 0.514084, acc: 0.765625] [adversarial loss: 0.952426, acc: 0.359375]\n",
      "1776: [discriminator loss: 0.568921, acc: 0.750000] [adversarial loss: 1.563439, acc: 0.062500]\n",
      "1777: [discriminator loss: 0.470286, acc: 0.765625] [adversarial loss: 0.909119, acc: 0.453125]\n",
      "1778: [discriminator loss: 0.550911, acc: 0.710938] [adversarial loss: 1.613546, acc: 0.046875]\n",
      "1779: [discriminator loss: 0.493346, acc: 0.750000] [adversarial loss: 0.775101, acc: 0.531250]\n",
      "1780: [discriminator loss: 0.552499, acc: 0.695312] [adversarial loss: 1.729045, acc: 0.109375]\n",
      "1781: [discriminator loss: 0.478510, acc: 0.781250] [adversarial loss: 1.129467, acc: 0.265625]\n",
      "1782: [discriminator loss: 0.523007, acc: 0.773438] [adversarial loss: 1.274710, acc: 0.140625]\n",
      "1783: [discriminator loss: 0.519536, acc: 0.734375] [adversarial loss: 1.470552, acc: 0.093750]\n",
      "1784: [discriminator loss: 0.571961, acc: 0.710938] [adversarial loss: 1.015217, acc: 0.390625]\n",
      "1785: [discriminator loss: 0.514624, acc: 0.796875] [adversarial loss: 1.867683, acc: 0.078125]\n",
      "1786: [discriminator loss: 0.515892, acc: 0.734375] [adversarial loss: 0.747402, acc: 0.531250]\n",
      "1787: [discriminator loss: 0.649498, acc: 0.687500] [adversarial loss: 1.783374, acc: 0.031250]\n",
      "1788: [discriminator loss: 0.571494, acc: 0.695312] [adversarial loss: 0.681678, acc: 0.562500]\n",
      "1789: [discriminator loss: 0.550790, acc: 0.710938] [adversarial loss: 1.607132, acc: 0.031250]\n",
      "1790: [discriminator loss: 0.528538, acc: 0.742188] [adversarial loss: 1.063949, acc: 0.234375]\n",
      "1791: [discriminator loss: 0.524483, acc: 0.734375] [adversarial loss: 1.476043, acc: 0.093750]\n",
      "1792: [discriminator loss: 0.429970, acc: 0.820312] [adversarial loss: 1.388621, acc: 0.078125]\n",
      "1793: [discriminator loss: 0.471098, acc: 0.773438] [adversarial loss: 1.040649, acc: 0.234375]\n",
      "1794: [discriminator loss: 0.522314, acc: 0.726562] [adversarial loss: 1.158864, acc: 0.203125]\n",
      "1795: [discriminator loss: 0.505759, acc: 0.765625] [adversarial loss: 1.257142, acc: 0.203125]\n",
      "1796: [discriminator loss: 0.500890, acc: 0.734375] [adversarial loss: 1.329721, acc: 0.171875]\n",
      "1797: [discriminator loss: 0.506499, acc: 0.734375] [adversarial loss: 1.194149, acc: 0.234375]\n",
      "1798: [discriminator loss: 0.530138, acc: 0.757812] [adversarial loss: 1.544783, acc: 0.125000]\n",
      "1799: [discriminator loss: 0.538826, acc: 0.710938] [adversarial loss: 0.799717, acc: 0.453125]\n",
      "1800: [discriminator loss: 0.524773, acc: 0.742188] [adversarial loss: 1.664443, acc: 0.046875]\n",
      "1801: [discriminator loss: 0.492025, acc: 0.757812] [adversarial loss: 1.191718, acc: 0.234375]\n",
      "1802: [discriminator loss: 0.484021, acc: 0.804688] [adversarial loss: 1.076028, acc: 0.187500]\n",
      "1803: [discriminator loss: 0.430560, acc: 0.812500] [adversarial loss: 1.526854, acc: 0.125000]\n",
      "1804: [discriminator loss: 0.521457, acc: 0.718750] [adversarial loss: 0.666351, acc: 0.703125]\n",
      "1805: [discriminator loss: 0.624042, acc: 0.703125] [adversarial loss: 1.871338, acc: 0.031250]\n",
      "1806: [discriminator loss: 0.621832, acc: 0.625000] [adversarial loss: 0.757926, acc: 0.515625]\n",
      "1807: [discriminator loss: 0.616126, acc: 0.648438] [adversarial loss: 1.789708, acc: 0.093750]\n",
      "1808: [discriminator loss: 0.589769, acc: 0.664062] [adversarial loss: 0.931220, acc: 0.359375]\n",
      "1809: [discriminator loss: 0.474545, acc: 0.765625] [adversarial loss: 1.400610, acc: 0.109375]\n",
      "1810: [discriminator loss: 0.477585, acc: 0.757812] [adversarial loss: 1.046408, acc: 0.265625]\n",
      "1811: [discriminator loss: 0.490746, acc: 0.773438] [adversarial loss: 1.329629, acc: 0.250000]\n",
      "1812: [discriminator loss: 0.467510, acc: 0.781250] [adversarial loss: 0.893866, acc: 0.468750]\n",
      "1813: [discriminator loss: 0.488939, acc: 0.789062] [adversarial loss: 1.615126, acc: 0.093750]\n",
      "1814: [discriminator loss: 0.540270, acc: 0.718750] [adversarial loss: 0.805844, acc: 0.562500]\n",
      "1815: [discriminator loss: 0.539894, acc: 0.750000] [adversarial loss: 1.146336, acc: 0.281250]\n",
      "1816: [discriminator loss: 0.516236, acc: 0.765625] [adversarial loss: 1.121526, acc: 0.312500]\n",
      "1817: [discriminator loss: 0.543349, acc: 0.750000] [adversarial loss: 0.877206, acc: 0.468750]\n",
      "1818: [discriminator loss: 0.505916, acc: 0.757812] [adversarial loss: 1.822698, acc: 0.078125]\n",
      "1819: [discriminator loss: 0.545512, acc: 0.703125] [adversarial loss: 0.878939, acc: 0.359375]\n",
      "1820: [discriminator loss: 0.554258, acc: 0.734375] [adversarial loss: 1.997876, acc: 0.000000]\n",
      "1821: [discriminator loss: 0.588304, acc: 0.664062] [adversarial loss: 0.948135, acc: 0.421875]\n",
      "1822: [discriminator loss: 0.506020, acc: 0.765625] [adversarial loss: 1.102982, acc: 0.312500]\n",
      "1823: [discriminator loss: 0.510507, acc: 0.804688] [adversarial loss: 1.165605, acc: 0.203125]\n",
      "1824: [discriminator loss: 0.499746, acc: 0.757812] [adversarial loss: 1.288404, acc: 0.140625]\n",
      "1825: [discriminator loss: 0.510358, acc: 0.773438] [adversarial loss: 0.874705, acc: 0.390625]\n",
      "1826: [discriminator loss: 0.571256, acc: 0.734375] [adversarial loss: 1.741498, acc: 0.046875]\n",
      "1827: [discriminator loss: 0.578962, acc: 0.671875] [adversarial loss: 0.985499, acc: 0.421875]\n",
      "1828: [discriminator loss: 0.528546, acc: 0.726562] [adversarial loss: 1.453301, acc: 0.156250]\n",
      "1829: [discriminator loss: 0.527695, acc: 0.765625] [adversarial loss: 1.062010, acc: 0.265625]\n",
      "1830: [discriminator loss: 0.502932, acc: 0.757812] [adversarial loss: 1.495509, acc: 0.125000]\n",
      "1831: [discriminator loss: 0.453672, acc: 0.765625] [adversarial loss: 1.136615, acc: 0.328125]\n",
      "1832: [discriminator loss: 0.508984, acc: 0.765625] [adversarial loss: 1.670160, acc: 0.062500]\n",
      "1833: [discriminator loss: 0.509506, acc: 0.718750] [adversarial loss: 0.848142, acc: 0.421875]\n",
      "1834: [discriminator loss: 0.521264, acc: 0.734375] [adversarial loss: 1.656956, acc: 0.093750]\n",
      "1835: [discriminator loss: 0.496547, acc: 0.742188] [adversarial loss: 0.861007, acc: 0.375000]\n",
      "1836: [discriminator loss: 0.554530, acc: 0.718750] [adversarial loss: 1.881260, acc: 0.078125]\n",
      "1837: [discriminator loss: 0.575836, acc: 0.710938] [adversarial loss: 0.725192, acc: 0.515625]\n",
      "1838: [discriminator loss: 0.544565, acc: 0.726562] [adversarial loss: 1.548118, acc: 0.109375]\n",
      "1839: [discriminator loss: 0.538992, acc: 0.695312] [adversarial loss: 1.004026, acc: 0.343750]\n",
      "1840: [discriminator loss: 0.469313, acc: 0.820312] [adversarial loss: 1.164328, acc: 0.187500]\n",
      "1841: [discriminator loss: 0.500890, acc: 0.796875] [adversarial loss: 1.066130, acc: 0.312500]\n",
      "1842: [discriminator loss: 0.541028, acc: 0.703125] [adversarial loss: 1.561264, acc: 0.078125]\n",
      "1843: [discriminator loss: 0.497333, acc: 0.765625] [adversarial loss: 0.990200, acc: 0.296875]\n",
      "1844: [discriminator loss: 0.476226, acc: 0.781250] [adversarial loss: 1.785585, acc: 0.031250]\n",
      "1845: [discriminator loss: 0.512789, acc: 0.757812] [adversarial loss: 0.894481, acc: 0.453125]\n",
      "1846: [discriminator loss: 0.603499, acc: 0.617188] [adversarial loss: 1.805356, acc: 0.093750]\n",
      "1847: [discriminator loss: 0.661593, acc: 0.648438] [adversarial loss: 0.838922, acc: 0.343750]\n",
      "1848: [discriminator loss: 0.568411, acc: 0.695312] [adversarial loss: 1.678030, acc: 0.062500]\n",
      "1849: [discriminator loss: 0.514917, acc: 0.734375] [adversarial loss: 0.986982, acc: 0.453125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850: [discriminator loss: 0.517033, acc: 0.726562] [adversarial loss: 1.435491, acc: 0.140625]\n",
      "1851: [discriminator loss: 0.520922, acc: 0.742188] [adversarial loss: 1.049547, acc: 0.359375]\n",
      "1852: [discriminator loss: 0.512416, acc: 0.742188] [adversarial loss: 1.181852, acc: 0.234375]\n",
      "1853: [discriminator loss: 0.446973, acc: 0.851562] [adversarial loss: 1.707093, acc: 0.046875]\n",
      "1854: [discriminator loss: 0.478173, acc: 0.750000] [adversarial loss: 0.875323, acc: 0.468750]\n",
      "1855: [discriminator loss: 0.498112, acc: 0.781250] [adversarial loss: 1.289149, acc: 0.078125]\n",
      "1856: [discriminator loss: 0.490094, acc: 0.726562] [adversarial loss: 1.030737, acc: 0.312500]\n",
      "1857: [discriminator loss: 0.524192, acc: 0.757812] [adversarial loss: 1.541963, acc: 0.109375]\n",
      "1858: [discriminator loss: 0.532394, acc: 0.710938] [adversarial loss: 1.035763, acc: 0.390625]\n",
      "1859: [discriminator loss: 0.438358, acc: 0.835938] [adversarial loss: 1.437980, acc: 0.125000]\n",
      "1860: [discriminator loss: 0.508633, acc: 0.726562] [adversarial loss: 1.032635, acc: 0.343750]\n",
      "1861: [discriminator loss: 0.464447, acc: 0.789062] [adversarial loss: 1.840871, acc: 0.046875]\n",
      "1862: [discriminator loss: 0.538275, acc: 0.750000] [adversarial loss: 1.060416, acc: 0.328125]\n",
      "1863: [discriminator loss: 0.540880, acc: 0.734375] [adversarial loss: 2.216246, acc: 0.046875]\n",
      "1864: [discriminator loss: 0.668226, acc: 0.671875] [adversarial loss: 0.665611, acc: 0.640625]\n",
      "1865: [discriminator loss: 0.539850, acc: 0.679688] [adversarial loss: 1.592162, acc: 0.125000]\n",
      "1866: [discriminator loss: 0.544219, acc: 0.726562] [adversarial loss: 0.995738, acc: 0.343750]\n",
      "1867: [discriminator loss: 0.482009, acc: 0.804688] [adversarial loss: 1.281241, acc: 0.156250]\n",
      "1868: [discriminator loss: 0.452388, acc: 0.781250] [adversarial loss: 1.071160, acc: 0.250000]\n",
      "1869: [discriminator loss: 0.517563, acc: 0.734375] [adversarial loss: 1.563050, acc: 0.062500]\n",
      "1870: [discriminator loss: 0.542223, acc: 0.726562] [adversarial loss: 0.948684, acc: 0.359375]\n",
      "1871: [discriminator loss: 0.453416, acc: 0.781250] [adversarial loss: 1.618969, acc: 0.125000]\n",
      "1872: [discriminator loss: 0.563273, acc: 0.648438] [adversarial loss: 1.200674, acc: 0.265625]\n",
      "1873: [discriminator loss: 0.508129, acc: 0.757812] [adversarial loss: 1.143415, acc: 0.296875]\n",
      "1874: [discriminator loss: 0.477180, acc: 0.773438] [adversarial loss: 1.091129, acc: 0.359375]\n",
      "1875: [discriminator loss: 0.557699, acc: 0.695312] [adversarial loss: 1.264170, acc: 0.156250]\n",
      "1876: [discriminator loss: 0.491610, acc: 0.742188] [adversarial loss: 1.088385, acc: 0.265625]\n",
      "1877: [discriminator loss: 0.518036, acc: 0.734375] [adversarial loss: 1.304934, acc: 0.156250]\n",
      "1878: [discriminator loss: 0.460819, acc: 0.773438] [adversarial loss: 1.020346, acc: 0.328125]\n",
      "1879: [discriminator loss: 0.521840, acc: 0.726562] [adversarial loss: 1.618833, acc: 0.109375]\n",
      "1880: [discriminator loss: 0.546397, acc: 0.703125] [adversarial loss: 1.068772, acc: 0.296875]\n",
      "1881: [discriminator loss: 0.453164, acc: 0.820312] [adversarial loss: 1.675103, acc: 0.109375]\n",
      "1882: [discriminator loss: 0.578843, acc: 0.679688] [adversarial loss: 0.952938, acc: 0.328125]\n",
      "1883: [discriminator loss: 0.470281, acc: 0.765625] [adversarial loss: 1.564214, acc: 0.093750]\n",
      "1884: [discriminator loss: 0.492271, acc: 0.789062] [adversarial loss: 0.981912, acc: 0.328125]\n",
      "1885: [discriminator loss: 0.599118, acc: 0.703125] [adversarial loss: 1.874037, acc: 0.046875]\n",
      "1886: [discriminator loss: 0.575910, acc: 0.703125] [adversarial loss: 0.749203, acc: 0.578125]\n",
      "1887: [discriminator loss: 0.526160, acc: 0.679688] [adversarial loss: 1.694672, acc: 0.093750]\n",
      "1888: [discriminator loss: 0.619622, acc: 0.671875] [adversarial loss: 1.052391, acc: 0.265625]\n",
      "1889: [discriminator loss: 0.476931, acc: 0.781250] [adversarial loss: 1.400332, acc: 0.140625]\n",
      "1890: [discriminator loss: 0.498458, acc: 0.757812] [adversarial loss: 1.021993, acc: 0.328125]\n",
      "1891: [discriminator loss: 0.563294, acc: 0.757812] [adversarial loss: 1.812889, acc: 0.093750]\n",
      "1892: [discriminator loss: 0.504723, acc: 0.750000] [adversarial loss: 0.875143, acc: 0.421875]\n",
      "1893: [discriminator loss: 0.539868, acc: 0.734375] [adversarial loss: 1.843851, acc: 0.093750]\n",
      "1894: [discriminator loss: 0.552331, acc: 0.726562] [adversarial loss: 0.966515, acc: 0.375000]\n",
      "1895: [discriminator loss: 0.517603, acc: 0.765625] [adversarial loss: 1.742009, acc: 0.046875]\n",
      "1896: [discriminator loss: 0.541440, acc: 0.742188] [adversarial loss: 1.088674, acc: 0.296875]\n",
      "1897: [discriminator loss: 0.488589, acc: 0.789062] [adversarial loss: 1.354871, acc: 0.140625]\n",
      "1898: [discriminator loss: 0.520605, acc: 0.781250] [adversarial loss: 0.997107, acc: 0.359375]\n",
      "1899: [discriminator loss: 0.485602, acc: 0.773438] [adversarial loss: 1.182575, acc: 0.265625]\n",
      "1900: [discriminator loss: 0.517693, acc: 0.742188] [adversarial loss: 1.211801, acc: 0.250000]\n",
      "1901: [discriminator loss: 0.472750, acc: 0.789062] [adversarial loss: 1.401617, acc: 0.109375]\n",
      "1902: [discriminator loss: 0.491787, acc: 0.796875] [adversarial loss: 1.389795, acc: 0.156250]\n",
      "1903: [discriminator loss: 0.517313, acc: 0.765625] [adversarial loss: 1.215974, acc: 0.187500]\n",
      "1904: [discriminator loss: 0.569543, acc: 0.687500] [adversarial loss: 1.592142, acc: 0.156250]\n",
      "1905: [discriminator loss: 0.493795, acc: 0.757812] [adversarial loss: 0.944855, acc: 0.375000]\n",
      "1906: [discriminator loss: 0.494163, acc: 0.757812] [adversarial loss: 1.626649, acc: 0.140625]\n",
      "1907: [discriminator loss: 0.550571, acc: 0.703125] [adversarial loss: 0.792234, acc: 0.453125]\n",
      "1908: [discriminator loss: 0.573429, acc: 0.710938] [adversarial loss: 1.829667, acc: 0.046875]\n",
      "1909: [discriminator loss: 0.596366, acc: 0.687500] [adversarial loss: 0.835374, acc: 0.406250]\n",
      "1910: [discriminator loss: 0.514426, acc: 0.742188] [adversarial loss: 1.662869, acc: 0.078125]\n",
      "1911: [discriminator loss: 0.520949, acc: 0.750000] [adversarial loss: 0.931212, acc: 0.390625]\n",
      "1912: [discriminator loss: 0.493679, acc: 0.781250] [adversarial loss: 1.666612, acc: 0.078125]\n",
      "1913: [discriminator loss: 0.506890, acc: 0.710938] [adversarial loss: 0.942847, acc: 0.359375]\n",
      "1914: [discriminator loss: 0.454924, acc: 0.804688] [adversarial loss: 1.285909, acc: 0.265625]\n",
      "1915: [discriminator loss: 0.477079, acc: 0.789062] [adversarial loss: 0.977678, acc: 0.343750]\n",
      "1916: [discriminator loss: 0.411210, acc: 0.859375] [adversarial loss: 1.361845, acc: 0.093750]\n",
      "1917: [discriminator loss: 0.455778, acc: 0.773438] [adversarial loss: 1.219194, acc: 0.218750]\n",
      "1918: [discriminator loss: 0.548726, acc: 0.679688] [adversarial loss: 1.015265, acc: 0.296875]\n",
      "1919: [discriminator loss: 0.490199, acc: 0.750000] [adversarial loss: 1.252687, acc: 0.250000]\n",
      "1920: [discriminator loss: 0.448173, acc: 0.828125] [adversarial loss: 1.365043, acc: 0.171875]\n",
      "1921: [discriminator loss: 0.491637, acc: 0.710938] [adversarial loss: 1.308086, acc: 0.125000]\n",
      "1922: [discriminator loss: 0.457232, acc: 0.835938] [adversarial loss: 0.911834, acc: 0.390625]\n",
      "1923: [discriminator loss: 0.559631, acc: 0.734375] [adversarial loss: 1.510986, acc: 0.109375]\n",
      "1924: [discriminator loss: 0.537864, acc: 0.726562] [adversarial loss: 0.837113, acc: 0.406250]\n",
      "1925: [discriminator loss: 0.542646, acc: 0.734375] [adversarial loss: 1.974540, acc: 0.078125]\n",
      "1926: [discriminator loss: 0.581639, acc: 0.648438] [adversarial loss: 0.655333, acc: 0.593750]\n",
      "1927: [discriminator loss: 0.628466, acc: 0.656250] [adversarial loss: 1.962622, acc: 0.031250]\n",
      "1928: [discriminator loss: 0.530187, acc: 0.742188] [adversarial loss: 0.834763, acc: 0.406250]\n",
      "1929: [discriminator loss: 0.533992, acc: 0.750000] [adversarial loss: 1.459142, acc: 0.109375]\n",
      "1930: [discriminator loss: 0.558859, acc: 0.718750] [adversarial loss: 0.953348, acc: 0.421875]\n",
      "1931: [discriminator loss: 0.474720, acc: 0.781250] [adversarial loss: 1.113931, acc: 0.203125]\n",
      "1932: [discriminator loss: 0.483662, acc: 0.773438] [adversarial loss: 1.057578, acc: 0.250000]\n",
      "1933: [discriminator loss: 0.495145, acc: 0.765625] [adversarial loss: 1.360175, acc: 0.140625]\n",
      "1934: [discriminator loss: 0.471654, acc: 0.828125] [adversarial loss: 1.187984, acc: 0.203125]\n",
      "1935: [discriminator loss: 0.513395, acc: 0.773438] [adversarial loss: 1.240365, acc: 0.281250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936: [discriminator loss: 0.446604, acc: 0.750000] [adversarial loss: 1.566128, acc: 0.093750]\n",
      "1937: [discriminator loss: 0.552075, acc: 0.718750] [adversarial loss: 0.709273, acc: 0.609375]\n",
      "1938: [discriminator loss: 0.605061, acc: 0.679688] [adversarial loss: 2.052779, acc: 0.015625]\n",
      "1939: [discriminator loss: 0.494296, acc: 0.804688] [adversarial loss: 1.066671, acc: 0.281250]\n",
      "1940: [discriminator loss: 0.580584, acc: 0.671875] [adversarial loss: 1.610734, acc: 0.093750]\n",
      "1941: [discriminator loss: 0.551168, acc: 0.710938] [adversarial loss: 1.080949, acc: 0.328125]\n",
      "1942: [discriminator loss: 0.542739, acc: 0.750000] [adversarial loss: 1.499317, acc: 0.140625]\n",
      "1943: [discriminator loss: 0.520511, acc: 0.734375] [adversarial loss: 1.101290, acc: 0.328125]\n",
      "1944: [discriminator loss: 0.447610, acc: 0.812500] [adversarial loss: 1.269901, acc: 0.234375]\n",
      "1945: [discriminator loss: 0.497629, acc: 0.796875] [adversarial loss: 1.082145, acc: 0.234375]\n",
      "1946: [discriminator loss: 0.491273, acc: 0.726562] [adversarial loss: 1.335845, acc: 0.187500]\n",
      "1947: [discriminator loss: 0.451522, acc: 0.804688] [adversarial loss: 1.200271, acc: 0.265625]\n",
      "1948: [discriminator loss: 0.545429, acc: 0.726562] [adversarial loss: 1.346953, acc: 0.140625]\n",
      "1949: [discriminator loss: 0.449387, acc: 0.781250] [adversarial loss: 1.438568, acc: 0.140625]\n",
      "1950: [discriminator loss: 0.524873, acc: 0.765625] [adversarial loss: 1.265249, acc: 0.218750]\n",
      "1951: [discriminator loss: 0.538367, acc: 0.710938] [adversarial loss: 1.222036, acc: 0.296875]\n",
      "1952: [discriminator loss: 0.476232, acc: 0.804688] [adversarial loss: 1.129454, acc: 0.296875]\n",
      "1953: [discriminator loss: 0.503711, acc: 0.765625] [adversarial loss: 1.330418, acc: 0.156250]\n",
      "1954: [discriminator loss: 0.521044, acc: 0.734375] [adversarial loss: 0.998205, acc: 0.375000]\n",
      "1955: [discriminator loss: 0.484952, acc: 0.781250] [adversarial loss: 1.545324, acc: 0.109375]\n",
      "1956: [discriminator loss: 0.488732, acc: 0.757812] [adversarial loss: 0.816017, acc: 0.500000]\n",
      "1957: [discriminator loss: 0.572001, acc: 0.695312] [adversarial loss: 1.931752, acc: 0.046875]\n",
      "1958: [discriminator loss: 0.666533, acc: 0.679688] [adversarial loss: 0.834692, acc: 0.453125]\n",
      "1959: [discriminator loss: 0.518154, acc: 0.757812] [adversarial loss: 1.694292, acc: 0.062500]\n",
      "1960: [discriminator loss: 0.597248, acc: 0.679688] [adversarial loss: 0.809177, acc: 0.546875]\n",
      "1961: [discriminator loss: 0.621062, acc: 0.656250] [adversarial loss: 1.733238, acc: 0.109375]\n",
      "1962: [discriminator loss: 0.547464, acc: 0.703125] [adversarial loss: 0.910775, acc: 0.359375]\n",
      "1963: [discriminator loss: 0.564805, acc: 0.710938] [adversarial loss: 1.623961, acc: 0.046875]\n",
      "1964: [discriminator loss: 0.550723, acc: 0.734375] [adversarial loss: 1.185078, acc: 0.281250]\n",
      "1965: [discriminator loss: 0.468515, acc: 0.828125] [adversarial loss: 1.345034, acc: 0.234375]\n",
      "1966: [discriminator loss: 0.531074, acc: 0.734375] [adversarial loss: 1.108706, acc: 0.265625]\n",
      "1967: [discriminator loss: 0.468176, acc: 0.789062] [adversarial loss: 1.448127, acc: 0.093750]\n",
      "1968: [discriminator loss: 0.510086, acc: 0.734375] [adversarial loss: 0.847044, acc: 0.437500]\n",
      "1969: [discriminator loss: 0.570379, acc: 0.710938] [adversarial loss: 1.485608, acc: 0.062500]\n",
      "1970: [discriminator loss: 0.491114, acc: 0.820312] [adversarial loss: 0.983599, acc: 0.328125]\n",
      "1971: [discriminator loss: 0.563635, acc: 0.695312] [adversarial loss: 1.544328, acc: 0.093750]\n",
      "1972: [discriminator loss: 0.528063, acc: 0.726562] [adversarial loss: 1.150728, acc: 0.187500]\n",
      "1973: [discriminator loss: 0.561544, acc: 0.687500] [adversarial loss: 1.427138, acc: 0.171875]\n",
      "1974: [discriminator loss: 0.533285, acc: 0.695312] [adversarial loss: 1.470950, acc: 0.125000]\n",
      "1975: [discriminator loss: 0.490559, acc: 0.765625] [adversarial loss: 1.096629, acc: 0.281250]\n",
      "1976: [discriminator loss: 0.513690, acc: 0.765625] [adversarial loss: 1.426069, acc: 0.218750]\n",
      "1977: [discriminator loss: 0.577716, acc: 0.671875] [adversarial loss: 1.270689, acc: 0.203125]\n",
      "1978: [discriminator loss: 0.487636, acc: 0.796875] [adversarial loss: 1.543692, acc: 0.156250]\n",
      "1979: [discriminator loss: 0.490294, acc: 0.773438] [adversarial loss: 1.040947, acc: 0.390625]\n",
      "1980: [discriminator loss: 0.567762, acc: 0.742188] [adversarial loss: 1.826367, acc: 0.046875]\n",
      "1981: [discriminator loss: 0.521857, acc: 0.734375] [adversarial loss: 0.833175, acc: 0.593750]\n",
      "1982: [discriminator loss: 0.552019, acc: 0.640625] [adversarial loss: 2.048659, acc: 0.062500]\n",
      "1983: [discriminator loss: 0.597354, acc: 0.695312] [adversarial loss: 0.863269, acc: 0.406250]\n",
      "1984: [discriminator loss: 0.549139, acc: 0.726562] [adversarial loss: 1.526948, acc: 0.093750]\n",
      "1985: [discriminator loss: 0.474009, acc: 0.812500] [adversarial loss: 1.156676, acc: 0.218750]\n",
      "1986: [discriminator loss: 0.505878, acc: 0.781250] [adversarial loss: 1.023419, acc: 0.296875]\n",
      "1987: [discriminator loss: 0.465637, acc: 0.812500] [adversarial loss: 0.965906, acc: 0.390625]\n",
      "1988: [discriminator loss: 0.552571, acc: 0.718750] [adversarial loss: 1.394163, acc: 0.156250]\n",
      "1989: [discriminator loss: 0.531316, acc: 0.750000] [adversarial loss: 1.021342, acc: 0.312500]\n",
      "1990: [discriminator loss: 0.518792, acc: 0.765625] [adversarial loss: 1.726565, acc: 0.156250]\n",
      "1991: [discriminator loss: 0.550396, acc: 0.726562] [adversarial loss: 1.035946, acc: 0.343750]\n",
      "1992: [discriminator loss: 0.538211, acc: 0.710938] [adversarial loss: 1.903994, acc: 0.078125]\n",
      "1993: [discriminator loss: 0.506819, acc: 0.710938] [adversarial loss: 0.822263, acc: 0.406250]\n",
      "1994: [discriminator loss: 0.570716, acc: 0.718750] [adversarial loss: 1.868764, acc: 0.015625]\n",
      "1995: [discriminator loss: 0.498036, acc: 0.757812] [adversarial loss: 1.164829, acc: 0.218750]\n",
      "1996: [discriminator loss: 0.495702, acc: 0.742188] [adversarial loss: 1.091797, acc: 0.312500]\n",
      "1997: [discriminator loss: 0.501442, acc: 0.765625] [adversarial loss: 1.037059, acc: 0.218750]\n",
      "1998: [discriminator loss: 0.602109, acc: 0.695312] [adversarial loss: 1.388961, acc: 0.109375]\n",
      "1999: [discriminator loss: 0.513322, acc: 0.742188] [adversarial loss: 1.102061, acc: 0.234375]\n",
      "2000: [discriminator loss: 0.444690, acc: 0.835938] [adversarial loss: 1.509080, acc: 0.109375]\n",
      "2001: [discriminator loss: 0.479244, acc: 0.789062] [adversarial loss: 1.001941, acc: 0.328125]\n",
      "2002: [discriminator loss: 0.463277, acc: 0.796875] [adversarial loss: 1.473261, acc: 0.125000]\n",
      "2003: [discriminator loss: 0.508988, acc: 0.742188] [adversarial loss: 1.004369, acc: 0.265625]\n",
      "2004: [discriminator loss: 0.531143, acc: 0.710938] [adversarial loss: 1.844194, acc: 0.046875]\n",
      "2005: [discriminator loss: 0.495898, acc: 0.734375] [adversarial loss: 0.924876, acc: 0.390625]\n",
      "2006: [discriminator loss: 0.599434, acc: 0.656250] [adversarial loss: 1.957919, acc: 0.031250]\n",
      "2007: [discriminator loss: 0.502449, acc: 0.796875] [adversarial loss: 0.929247, acc: 0.421875]\n",
      "2008: [discriminator loss: 0.483262, acc: 0.742188] [adversarial loss: 1.586641, acc: 0.046875]\n",
      "2009: [discriminator loss: 0.549175, acc: 0.726562] [adversarial loss: 0.887228, acc: 0.406250]\n",
      "2010: [discriminator loss: 0.520652, acc: 0.726562] [adversarial loss: 1.886769, acc: 0.109375]\n",
      "2011: [discriminator loss: 0.620106, acc: 0.656250] [adversarial loss: 0.831045, acc: 0.468750]\n",
      "2012: [discriminator loss: 0.482403, acc: 0.757812] [adversarial loss: 1.666230, acc: 0.140625]\n",
      "2013: [discriminator loss: 0.504284, acc: 0.765625] [adversarial loss: 1.164022, acc: 0.187500]\n",
      "2014: [discriminator loss: 0.480645, acc: 0.742188] [adversarial loss: 1.351404, acc: 0.156250]\n",
      "2015: [discriminator loss: 0.551549, acc: 0.687500] [adversarial loss: 0.952046, acc: 0.359375]\n",
      "2016: [discriminator loss: 0.498645, acc: 0.750000] [adversarial loss: 1.176791, acc: 0.187500]\n",
      "2017: [discriminator loss: 0.501066, acc: 0.757812] [adversarial loss: 0.995342, acc: 0.375000]\n",
      "2018: [discriminator loss: 0.514651, acc: 0.765625] [adversarial loss: 1.231005, acc: 0.187500]\n",
      "2019: [discriminator loss: 0.458031, acc: 0.804688] [adversarial loss: 1.158334, acc: 0.218750]\n",
      "2020: [discriminator loss: 0.544114, acc: 0.765625] [adversarial loss: 1.611064, acc: 0.093750]\n",
      "2021: [discriminator loss: 0.580488, acc: 0.687500] [adversarial loss: 0.995765, acc: 0.343750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: [discriminator loss: 0.548595, acc: 0.734375] [adversarial loss: 1.794048, acc: 0.109375]\n",
      "2023: [discriminator loss: 0.549517, acc: 0.781250] [adversarial loss: 0.996495, acc: 0.328125]\n",
      "2024: [discriminator loss: 0.550297, acc: 0.695312] [adversarial loss: 1.570770, acc: 0.109375]\n",
      "2025: [discriminator loss: 0.580941, acc: 0.734375] [adversarial loss: 0.874460, acc: 0.453125]\n",
      "2026: [discriminator loss: 0.517247, acc: 0.710938] [adversarial loss: 1.277161, acc: 0.187500]\n",
      "2027: [discriminator loss: 0.489160, acc: 0.750000] [adversarial loss: 1.190972, acc: 0.250000]\n",
      "2028: [discriminator loss: 0.540047, acc: 0.734375] [adversarial loss: 1.263022, acc: 0.203125]\n",
      "2029: [discriminator loss: 0.558602, acc: 0.718750] [adversarial loss: 0.896798, acc: 0.421875]\n",
      "2030: [discriminator loss: 0.529967, acc: 0.765625] [adversarial loss: 1.192839, acc: 0.265625]\n",
      "2031: [discriminator loss: 0.455675, acc: 0.765625] [adversarial loss: 1.482974, acc: 0.125000]\n",
      "2032: [discriminator loss: 0.488676, acc: 0.796875] [adversarial loss: 1.308933, acc: 0.125000]\n",
      "2033: [discriminator loss: 0.469575, acc: 0.812500] [adversarial loss: 1.301875, acc: 0.187500]\n",
      "2034: [discriminator loss: 0.513492, acc: 0.750000] [adversarial loss: 1.105209, acc: 0.296875]\n",
      "2035: [discriminator loss: 0.450400, acc: 0.828125] [adversarial loss: 1.119454, acc: 0.218750]\n",
      "2036: [discriminator loss: 0.551385, acc: 0.742188] [adversarial loss: 1.688856, acc: 0.062500]\n",
      "2037: [discriminator loss: 0.542762, acc: 0.695312] [adversarial loss: 1.094418, acc: 0.218750]\n",
      "2038: [discriminator loss: 0.458672, acc: 0.796875] [adversarial loss: 1.469805, acc: 0.109375]\n",
      "2039: [discriminator loss: 0.586970, acc: 0.648438] [adversarial loss: 1.124284, acc: 0.281250]\n",
      "2040: [discriminator loss: 0.547086, acc: 0.703125] [adversarial loss: 2.163508, acc: 0.000000]\n",
      "2041: [discriminator loss: 0.571695, acc: 0.710938] [adversarial loss: 0.717194, acc: 0.531250]\n",
      "2042: [discriminator loss: 0.593518, acc: 0.687500] [adversarial loss: 1.562859, acc: 0.062500]\n",
      "2043: [discriminator loss: 0.518080, acc: 0.726562] [adversarial loss: 1.077847, acc: 0.250000]\n",
      "2044: [discriminator loss: 0.544102, acc: 0.718750] [adversarial loss: 1.697045, acc: 0.109375]\n",
      "2045: [discriminator loss: 0.533347, acc: 0.742188] [adversarial loss: 1.245928, acc: 0.265625]\n",
      "2046: [discriminator loss: 0.559975, acc: 0.726562] [adversarial loss: 1.337992, acc: 0.218750]\n",
      "2047: [discriminator loss: 0.475436, acc: 0.773438] [adversarial loss: 0.953366, acc: 0.406250]\n",
      "2048: [discriminator loss: 0.545635, acc: 0.703125] [adversarial loss: 1.523991, acc: 0.187500]\n",
      "2049: [discriminator loss: 0.517364, acc: 0.757812] [adversarial loss: 1.018958, acc: 0.281250]\n",
      "2050: [discriminator loss: 0.461208, acc: 0.789062] [adversarial loss: 1.491200, acc: 0.125000]\n",
      "2051: [discriminator loss: 0.545108, acc: 0.718750] [adversarial loss: 0.943987, acc: 0.390625]\n",
      "2052: [discriminator loss: 0.515931, acc: 0.742188] [adversarial loss: 1.543640, acc: 0.093750]\n",
      "2053: [discriminator loss: 0.628333, acc: 0.687500] [adversarial loss: 0.973970, acc: 0.359375]\n",
      "2054: [discriminator loss: 0.458874, acc: 0.773438] [adversarial loss: 1.864862, acc: 0.062500]\n",
      "2055: [discriminator loss: 0.592264, acc: 0.679688] [adversarial loss: 0.915021, acc: 0.515625]\n",
      "2056: [discriminator loss: 0.634984, acc: 0.656250] [adversarial loss: 1.779266, acc: 0.062500]\n",
      "2057: [discriminator loss: 0.533468, acc: 0.742188] [adversarial loss: 0.992810, acc: 0.312500]\n",
      "2058: [discriminator loss: 0.514099, acc: 0.710938] [adversarial loss: 1.420363, acc: 0.156250]\n",
      "2059: [discriminator loss: 0.523055, acc: 0.726562] [adversarial loss: 1.262638, acc: 0.218750]\n",
      "2060: [discriminator loss: 0.510519, acc: 0.734375] [adversarial loss: 1.323766, acc: 0.078125]\n",
      "2061: [discriminator loss: 0.453896, acc: 0.812500] [adversarial loss: 1.166868, acc: 0.156250]\n",
      "2062: [discriminator loss: 0.520089, acc: 0.718750] [adversarial loss: 1.050121, acc: 0.296875]\n",
      "2063: [discriminator loss: 0.469264, acc: 0.796875] [adversarial loss: 1.657695, acc: 0.078125]\n",
      "2064: [discriminator loss: 0.541910, acc: 0.703125] [adversarial loss: 0.990474, acc: 0.343750]\n",
      "2065: [discriminator loss: 0.534839, acc: 0.726562] [adversarial loss: 1.909413, acc: 0.046875]\n",
      "2066: [discriminator loss: 0.546613, acc: 0.710938] [adversarial loss: 0.924498, acc: 0.390625]\n",
      "2067: [discriminator loss: 0.633552, acc: 0.656250] [adversarial loss: 1.493267, acc: 0.093750]\n",
      "2068: [discriminator loss: 0.550792, acc: 0.734375] [adversarial loss: 0.922625, acc: 0.421875]\n",
      "2069: [discriminator loss: 0.524903, acc: 0.757812] [adversarial loss: 1.537863, acc: 0.140625]\n",
      "2070: [discriminator loss: 0.439997, acc: 0.812500] [adversarial loss: 1.221494, acc: 0.218750]\n",
      "2071: [discriminator loss: 0.480545, acc: 0.804688] [adversarial loss: 1.304024, acc: 0.187500]\n",
      "2072: [discriminator loss: 0.439034, acc: 0.812500] [adversarial loss: 1.327461, acc: 0.171875]\n",
      "2073: [discriminator loss: 0.593836, acc: 0.710938] [adversarial loss: 0.985039, acc: 0.296875]\n",
      "2074: [discriminator loss: 0.512163, acc: 0.765625] [adversarial loss: 1.532998, acc: 0.140625]\n",
      "2075: [discriminator loss: 0.622691, acc: 0.617188] [adversarial loss: 0.810208, acc: 0.484375]\n",
      "2076: [discriminator loss: 0.473966, acc: 0.773438] [adversarial loss: 1.351852, acc: 0.171875]\n",
      "2077: [discriminator loss: 0.603542, acc: 0.703125] [adversarial loss: 0.870658, acc: 0.359375]\n",
      "2078: [discriminator loss: 0.545772, acc: 0.726562] [adversarial loss: 1.593908, acc: 0.125000]\n",
      "2079: [discriminator loss: 0.552277, acc: 0.734375] [adversarial loss: 1.224231, acc: 0.234375]\n",
      "2080: [discriminator loss: 0.476059, acc: 0.757812] [adversarial loss: 1.354607, acc: 0.156250]\n",
      "2081: [discriminator loss: 0.477181, acc: 0.781250] [adversarial loss: 1.102533, acc: 0.250000]\n",
      "2082: [discriminator loss: 0.485907, acc: 0.726562] [adversarial loss: 1.452663, acc: 0.171875]\n",
      "2083: [discriminator loss: 0.576651, acc: 0.710938] [adversarial loss: 0.774013, acc: 0.453125]\n",
      "2084: [discriminator loss: 0.598487, acc: 0.648438] [adversarial loss: 1.788777, acc: 0.046875]\n",
      "2085: [discriminator loss: 0.573275, acc: 0.687500] [adversarial loss: 0.780759, acc: 0.500000]\n",
      "2086: [discriminator loss: 0.591285, acc: 0.671875] [adversarial loss: 1.933799, acc: 0.062500]\n",
      "2087: [discriminator loss: 0.588766, acc: 0.664062] [adversarial loss: 0.739955, acc: 0.531250]\n",
      "2088: [discriminator loss: 0.526030, acc: 0.726562] [adversarial loss: 1.565007, acc: 0.062500]\n",
      "2089: [discriminator loss: 0.494712, acc: 0.750000] [adversarial loss: 0.905359, acc: 0.437500]\n",
      "2090: [discriminator loss: 0.542832, acc: 0.734375] [adversarial loss: 1.962376, acc: 0.031250]\n",
      "2091: [discriminator loss: 0.531422, acc: 0.750000] [adversarial loss: 0.973327, acc: 0.328125]\n",
      "2092: [discriminator loss: 0.515981, acc: 0.742188] [adversarial loss: 1.404403, acc: 0.062500]\n",
      "2093: [discriminator loss: 0.489661, acc: 0.781250] [adversarial loss: 1.081314, acc: 0.250000]\n",
      "2094: [discriminator loss: 0.507497, acc: 0.789062] [adversarial loss: 1.568595, acc: 0.093750]\n",
      "2095: [discriminator loss: 0.512311, acc: 0.734375] [adversarial loss: 0.900211, acc: 0.343750]\n",
      "2096: [discriminator loss: 0.536408, acc: 0.750000] [adversarial loss: 1.831970, acc: 0.062500]\n",
      "2097: [discriminator loss: 0.515561, acc: 0.742188] [adversarial loss: 1.057357, acc: 0.328125]\n",
      "2098: [discriminator loss: 0.485626, acc: 0.742188] [adversarial loss: 1.415519, acc: 0.140625]\n",
      "2099: [discriminator loss: 0.451468, acc: 0.828125] [adversarial loss: 1.171197, acc: 0.203125]\n",
      "2100: [discriminator loss: 0.535687, acc: 0.718750] [adversarial loss: 0.902430, acc: 0.390625]\n",
      "2101: [discriminator loss: 0.530258, acc: 0.726562] [adversarial loss: 1.478718, acc: 0.093750]\n",
      "2102: [discriminator loss: 0.464230, acc: 0.820312] [adversarial loss: 1.085184, acc: 0.296875]\n",
      "2103: [discriminator loss: 0.450838, acc: 0.789062] [adversarial loss: 1.572995, acc: 0.093750]\n",
      "2104: [discriminator loss: 0.522088, acc: 0.703125] [adversarial loss: 0.810296, acc: 0.468750]\n",
      "2105: [discriminator loss: 0.513366, acc: 0.734375] [adversarial loss: 1.742790, acc: 0.046875]\n",
      "2106: [discriminator loss: 0.585703, acc: 0.710938] [adversarial loss: 0.980663, acc: 0.328125]\n",
      "2107: [discriminator loss: 0.536120, acc: 0.757812] [adversarial loss: 1.668960, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108: [discriminator loss: 0.538882, acc: 0.726562] [adversarial loss: 0.967455, acc: 0.453125]\n",
      "2109: [discriminator loss: 0.566784, acc: 0.687500] [adversarial loss: 1.391524, acc: 0.171875]\n",
      "2110: [discriminator loss: 0.522242, acc: 0.734375] [adversarial loss: 1.209935, acc: 0.234375]\n",
      "2111: [discriminator loss: 0.532122, acc: 0.781250] [adversarial loss: 0.953931, acc: 0.375000]\n",
      "2112: [discriminator loss: 0.492087, acc: 0.789062] [adversarial loss: 1.402765, acc: 0.234375]\n",
      "2113: [discriminator loss: 0.521731, acc: 0.750000] [adversarial loss: 1.004514, acc: 0.343750]\n",
      "2114: [discriminator loss: 0.499922, acc: 0.789062] [adversarial loss: 1.689214, acc: 0.062500]\n",
      "2115: [discriminator loss: 0.558686, acc: 0.687500] [adversarial loss: 0.833970, acc: 0.421875]\n",
      "2116: [discriminator loss: 0.548396, acc: 0.664062] [adversarial loss: 2.102563, acc: 0.062500]\n",
      "2117: [discriminator loss: 0.635738, acc: 0.687500] [adversarial loss: 0.701980, acc: 0.515625]\n",
      "2118: [discriminator loss: 0.614637, acc: 0.679688] [adversarial loss: 1.799667, acc: 0.031250]\n",
      "2119: [discriminator loss: 0.623742, acc: 0.656250] [adversarial loss: 0.854884, acc: 0.421875]\n",
      "2120: [discriminator loss: 0.479579, acc: 0.765625] [adversarial loss: 1.505179, acc: 0.125000]\n",
      "2121: [discriminator loss: 0.515368, acc: 0.765625] [adversarial loss: 1.041699, acc: 0.343750]\n",
      "2122: [discriminator loss: 0.449555, acc: 0.828125] [adversarial loss: 1.205992, acc: 0.281250]\n",
      "2123: [discriminator loss: 0.466035, acc: 0.765625] [adversarial loss: 1.336665, acc: 0.140625]\n",
      "2124: [discriminator loss: 0.457745, acc: 0.835938] [adversarial loss: 1.505843, acc: 0.171875]\n",
      "2125: [discriminator loss: 0.495272, acc: 0.757812] [adversarial loss: 1.193954, acc: 0.250000]\n",
      "2126: [discriminator loss: 0.481304, acc: 0.804688] [adversarial loss: 1.392593, acc: 0.187500]\n",
      "2127: [discriminator loss: 0.508723, acc: 0.710938] [adversarial loss: 0.896436, acc: 0.406250]\n",
      "2128: [discriminator loss: 0.441368, acc: 0.789062] [adversarial loss: 1.461039, acc: 0.171875]\n",
      "2129: [discriminator loss: 0.479819, acc: 0.765625] [adversarial loss: 1.030434, acc: 0.328125]\n",
      "2130: [discriminator loss: 0.507661, acc: 0.703125] [adversarial loss: 1.377314, acc: 0.171875]\n",
      "2131: [discriminator loss: 0.493882, acc: 0.789062] [adversarial loss: 1.290816, acc: 0.125000]\n",
      "2132: [discriminator loss: 0.537173, acc: 0.734375] [adversarial loss: 1.125994, acc: 0.296875]\n",
      "2133: [discriminator loss: 0.597545, acc: 0.710938] [adversarial loss: 0.988861, acc: 0.375000]\n",
      "2134: [discriminator loss: 0.536894, acc: 0.734375] [adversarial loss: 1.788898, acc: 0.078125]\n",
      "2135: [discriminator loss: 0.506170, acc: 0.750000] [adversarial loss: 0.839821, acc: 0.484375]\n",
      "2136: [discriminator loss: 0.668010, acc: 0.632812] [adversarial loss: 1.872928, acc: 0.109375]\n",
      "2137: [discriminator loss: 0.550259, acc: 0.710938] [adversarial loss: 0.835832, acc: 0.546875]\n",
      "2138: [discriminator loss: 0.549091, acc: 0.695312] [adversarial loss: 1.870862, acc: 0.046875]\n",
      "2139: [discriminator loss: 0.542921, acc: 0.687500] [adversarial loss: 0.827931, acc: 0.437500]\n",
      "2140: [discriminator loss: 0.552604, acc: 0.734375] [adversarial loss: 1.533198, acc: 0.140625]\n",
      "2141: [discriminator loss: 0.478769, acc: 0.773438] [adversarial loss: 1.080559, acc: 0.328125]\n",
      "2142: [discriminator loss: 0.549169, acc: 0.757812] [adversarial loss: 1.436465, acc: 0.062500]\n",
      "2143: [discriminator loss: 0.466852, acc: 0.765625] [adversarial loss: 1.088448, acc: 0.218750]\n",
      "2144: [discriminator loss: 0.490296, acc: 0.789062] [adversarial loss: 1.560411, acc: 0.109375]\n",
      "2145: [discriminator loss: 0.591250, acc: 0.679688] [adversarial loss: 0.968897, acc: 0.343750]\n",
      "2146: [discriminator loss: 0.570155, acc: 0.726562] [adversarial loss: 1.481928, acc: 0.093750]\n",
      "2147: [discriminator loss: 0.529940, acc: 0.734375] [adversarial loss: 1.139515, acc: 0.218750]\n",
      "2148: [discriminator loss: 0.505491, acc: 0.789062] [adversarial loss: 1.264099, acc: 0.218750]\n",
      "2149: [discriminator loss: 0.534300, acc: 0.695312] [adversarial loss: 1.345120, acc: 0.140625]\n",
      "2150: [discriminator loss: 0.527567, acc: 0.757812] [adversarial loss: 1.199826, acc: 0.171875]\n",
      "2151: [discriminator loss: 0.448106, acc: 0.828125] [adversarial loss: 1.336110, acc: 0.125000]\n",
      "2152: [discriminator loss: 0.481029, acc: 0.757812] [adversarial loss: 1.263751, acc: 0.234375]\n",
      "2153: [discriminator loss: 0.518125, acc: 0.750000] [adversarial loss: 1.017915, acc: 0.328125]\n",
      "2154: [discriminator loss: 0.496098, acc: 0.765625] [adversarial loss: 1.646331, acc: 0.062500]\n",
      "2155: [discriminator loss: 0.580714, acc: 0.718750] [adversarial loss: 1.094887, acc: 0.312500]\n",
      "2156: [discriminator loss: 0.595066, acc: 0.640625] [adversarial loss: 1.196528, acc: 0.140625]\n",
      "2157: [discriminator loss: 0.508990, acc: 0.757812] [adversarial loss: 1.047864, acc: 0.359375]\n",
      "2158: [discriminator loss: 0.522471, acc: 0.773438] [adversarial loss: 1.169263, acc: 0.218750]\n",
      "2159: [discriminator loss: 0.481157, acc: 0.773438] [adversarial loss: 1.314453, acc: 0.281250]\n",
      "2160: [discriminator loss: 0.460408, acc: 0.835938] [adversarial loss: 1.208728, acc: 0.203125]\n",
      "2161: [discriminator loss: 0.551545, acc: 0.679688] [adversarial loss: 1.592294, acc: 0.156250]\n",
      "2162: [discriminator loss: 0.512802, acc: 0.757812] [adversarial loss: 0.928331, acc: 0.296875]\n",
      "2163: [discriminator loss: 0.579625, acc: 0.726562] [adversarial loss: 1.831041, acc: 0.046875]\n",
      "2164: [discriminator loss: 0.542054, acc: 0.750000] [adversarial loss: 0.930997, acc: 0.437500]\n",
      "2165: [discriminator loss: 0.442460, acc: 0.765625] [adversarial loss: 1.903777, acc: 0.046875]\n",
      "2166: [discriminator loss: 0.558903, acc: 0.703125] [adversarial loss: 0.806950, acc: 0.437500]\n",
      "2167: [discriminator loss: 0.565681, acc: 0.718750] [adversarial loss: 1.507467, acc: 0.093750]\n",
      "2168: [discriminator loss: 0.538298, acc: 0.734375] [adversarial loss: 0.942514, acc: 0.328125]\n",
      "2169: [discriminator loss: 0.504846, acc: 0.757812] [adversarial loss: 1.673449, acc: 0.031250]\n",
      "2170: [discriminator loss: 0.576270, acc: 0.703125] [adversarial loss: 1.114957, acc: 0.328125]\n",
      "2171: [discriminator loss: 0.540795, acc: 0.742188] [adversarial loss: 1.642097, acc: 0.109375]\n",
      "2172: [discriminator loss: 0.481387, acc: 0.773438] [adversarial loss: 1.060482, acc: 0.281250]\n",
      "2173: [discriminator loss: 0.528963, acc: 0.742188] [adversarial loss: 1.423372, acc: 0.203125]\n",
      "2174: [discriminator loss: 0.413992, acc: 0.820312] [adversarial loss: 1.051284, acc: 0.296875]\n",
      "2175: [discriminator loss: 0.554235, acc: 0.718750] [adversarial loss: 1.646739, acc: 0.187500]\n",
      "2176: [discriminator loss: 0.536463, acc: 0.710938] [adversarial loss: 0.762336, acc: 0.484375]\n",
      "2177: [discriminator loss: 0.573952, acc: 0.718750] [adversarial loss: 1.500291, acc: 0.125000]\n",
      "2178: [discriminator loss: 0.502089, acc: 0.757812] [adversarial loss: 1.007579, acc: 0.328125]\n",
      "2179: [discriminator loss: 0.486028, acc: 0.734375] [adversarial loss: 1.632543, acc: 0.140625]\n",
      "2180: [discriminator loss: 0.477030, acc: 0.757812] [adversarial loss: 0.987155, acc: 0.312500]\n",
      "2181: [discriminator loss: 0.540025, acc: 0.726562] [adversarial loss: 1.571971, acc: 0.093750]\n",
      "2182: [discriminator loss: 0.530981, acc: 0.734375] [adversarial loss: 1.048982, acc: 0.281250]\n",
      "2183: [discriminator loss: 0.568586, acc: 0.687500] [adversarial loss: 1.455409, acc: 0.156250]\n",
      "2184: [discriminator loss: 0.524685, acc: 0.757812] [adversarial loss: 1.005375, acc: 0.328125]\n",
      "2185: [discriminator loss: 0.534336, acc: 0.750000] [adversarial loss: 1.652864, acc: 0.109375]\n",
      "2186: [discriminator loss: 0.600530, acc: 0.703125] [adversarial loss: 1.049059, acc: 0.312500]\n",
      "2187: [discriminator loss: 0.570411, acc: 0.742188] [adversarial loss: 1.639896, acc: 0.046875]\n",
      "2188: [discriminator loss: 0.470903, acc: 0.804688] [adversarial loss: 0.960837, acc: 0.343750]\n",
      "2189: [discriminator loss: 0.454433, acc: 0.804688] [adversarial loss: 1.480782, acc: 0.125000]\n",
      "2190: [discriminator loss: 0.500226, acc: 0.812500] [adversarial loss: 1.079037, acc: 0.265625]\n",
      "2191: [discriminator loss: 0.523297, acc: 0.734375] [adversarial loss: 1.839067, acc: 0.140625]\n",
      "2192: [discriminator loss: 0.528199, acc: 0.703125] [adversarial loss: 1.030090, acc: 0.218750]\n",
      "2193: [discriminator loss: 0.482786, acc: 0.773438] [adversarial loss: 1.573200, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2194: [discriminator loss: 0.516473, acc: 0.773438] [adversarial loss: 1.079020, acc: 0.203125]\n",
      "2195: [discriminator loss: 0.462907, acc: 0.804688] [adversarial loss: 1.564396, acc: 0.109375]\n",
      "2196: [discriminator loss: 0.447603, acc: 0.789062] [adversarial loss: 1.230822, acc: 0.250000]\n",
      "2197: [discriminator loss: 0.478910, acc: 0.789062] [adversarial loss: 1.618516, acc: 0.062500]\n",
      "2198: [discriminator loss: 0.480873, acc: 0.726562] [adversarial loss: 1.073879, acc: 0.328125]\n",
      "2199: [discriminator loss: 0.434060, acc: 0.796875] [adversarial loss: 1.002287, acc: 0.296875]\n",
      "2200: [discriminator loss: 0.519847, acc: 0.773438] [adversarial loss: 1.710354, acc: 0.078125]\n",
      "2201: [discriminator loss: 0.535008, acc: 0.742188] [adversarial loss: 0.828026, acc: 0.421875]\n",
      "2202: [discriminator loss: 0.549679, acc: 0.695312] [adversarial loss: 1.808247, acc: 0.125000]\n",
      "2203: [discriminator loss: 0.489121, acc: 0.726562] [adversarial loss: 1.067351, acc: 0.312500]\n",
      "2204: [discriminator loss: 0.601363, acc: 0.695312] [adversarial loss: 1.589723, acc: 0.109375]\n",
      "2205: [discriminator loss: 0.581305, acc: 0.703125] [adversarial loss: 1.101599, acc: 0.375000]\n",
      "2206: [discriminator loss: 0.538298, acc: 0.718750] [adversarial loss: 1.589695, acc: 0.140625]\n",
      "2207: [discriminator loss: 0.491693, acc: 0.742188] [adversarial loss: 1.081664, acc: 0.312500]\n",
      "2208: [discriminator loss: 0.508825, acc: 0.742188] [adversarial loss: 1.694650, acc: 0.109375]\n",
      "2209: [discriminator loss: 0.471669, acc: 0.757812] [adversarial loss: 0.854318, acc: 0.437500]\n",
      "2210: [discriminator loss: 0.501543, acc: 0.750000] [adversarial loss: 1.321455, acc: 0.203125]\n",
      "2211: [discriminator loss: 0.553645, acc: 0.718750] [adversarial loss: 1.107514, acc: 0.312500]\n",
      "2212: [discriminator loss: 0.536194, acc: 0.734375] [adversarial loss: 1.476734, acc: 0.140625]\n",
      "2213: [discriminator loss: 0.441997, acc: 0.812500] [adversarial loss: 1.009108, acc: 0.375000]\n",
      "2214: [discriminator loss: 0.474846, acc: 0.734375] [adversarial loss: 1.536963, acc: 0.125000]\n",
      "2215: [discriminator loss: 0.522405, acc: 0.718750] [adversarial loss: 1.313830, acc: 0.125000]\n",
      "2216: [discriminator loss: 0.490791, acc: 0.781250] [adversarial loss: 1.098554, acc: 0.312500]\n",
      "2217: [discriminator loss: 0.400706, acc: 0.843750] [adversarial loss: 1.207321, acc: 0.218750]\n",
      "2218: [discriminator loss: 0.502210, acc: 0.773438] [adversarial loss: 1.148930, acc: 0.312500]\n",
      "2219: [discriminator loss: 0.525281, acc: 0.757812] [adversarial loss: 1.335592, acc: 0.187500]\n",
      "2220: [discriminator loss: 0.554821, acc: 0.742188] [adversarial loss: 0.895151, acc: 0.484375]\n",
      "2221: [discriminator loss: 0.595622, acc: 0.710938] [adversarial loss: 1.967795, acc: 0.046875]\n",
      "2222: [discriminator loss: 0.513150, acc: 0.773438] [adversarial loss: 0.870186, acc: 0.500000]\n",
      "2223: [discriminator loss: 0.598788, acc: 0.687500] [adversarial loss: 1.717088, acc: 0.093750]\n",
      "2224: [discriminator loss: 0.484650, acc: 0.757812] [adversarial loss: 0.927699, acc: 0.437500]\n",
      "2225: [discriminator loss: 0.539447, acc: 0.726562] [adversarial loss: 1.198526, acc: 0.265625]\n",
      "2226: [discriminator loss: 0.529791, acc: 0.742188] [adversarial loss: 1.250623, acc: 0.234375]\n",
      "2227: [discriminator loss: 0.493736, acc: 0.742188] [adversarial loss: 1.235690, acc: 0.203125]\n",
      "2228: [discriminator loss: 0.484727, acc: 0.828125] [adversarial loss: 1.217261, acc: 0.187500]\n",
      "2229: [discriminator loss: 0.555673, acc: 0.679688] [adversarial loss: 1.544260, acc: 0.140625]\n",
      "2230: [discriminator loss: 0.511225, acc: 0.718750] [adversarial loss: 1.243009, acc: 0.171875]\n",
      "2231: [discriminator loss: 0.499505, acc: 0.765625] [adversarial loss: 1.277842, acc: 0.265625]\n",
      "2232: [discriminator loss: 0.515809, acc: 0.726562] [adversarial loss: 1.561618, acc: 0.140625]\n",
      "2233: [discriminator loss: 0.500441, acc: 0.757812] [adversarial loss: 0.973684, acc: 0.281250]\n",
      "2234: [discriminator loss: 0.603185, acc: 0.710938] [adversarial loss: 1.724568, acc: 0.109375]\n",
      "2235: [discriminator loss: 0.607324, acc: 0.703125] [adversarial loss: 0.957157, acc: 0.328125]\n",
      "2236: [discriminator loss: 0.533213, acc: 0.742188] [adversarial loss: 1.740544, acc: 0.125000]\n",
      "2237: [discriminator loss: 0.542068, acc: 0.734375] [adversarial loss: 0.864788, acc: 0.406250]\n",
      "2238: [discriminator loss: 0.565087, acc: 0.710938] [adversarial loss: 1.661508, acc: 0.109375]\n",
      "2239: [discriminator loss: 0.511115, acc: 0.757812] [adversarial loss: 1.210022, acc: 0.218750]\n",
      "2240: [discriminator loss: 0.515737, acc: 0.718750] [adversarial loss: 1.011401, acc: 0.343750]\n",
      "2241: [discriminator loss: 0.604261, acc: 0.679688] [adversarial loss: 1.766574, acc: 0.093750]\n",
      "2242: [discriminator loss: 0.474896, acc: 0.765625] [adversarial loss: 0.798440, acc: 0.500000]\n",
      "2243: [discriminator loss: 0.510107, acc: 0.742188] [adversarial loss: 1.462628, acc: 0.140625]\n",
      "2244: [discriminator loss: 0.550350, acc: 0.718750] [adversarial loss: 1.192130, acc: 0.265625]\n",
      "2245: [discriminator loss: 0.488205, acc: 0.742188] [adversarial loss: 1.629679, acc: 0.093750]\n",
      "2246: [discriminator loss: 0.515774, acc: 0.726562] [adversarial loss: 1.097213, acc: 0.250000]\n",
      "2247: [discriminator loss: 0.493975, acc: 0.710938] [adversarial loss: 1.453002, acc: 0.156250]\n",
      "2248: [discriminator loss: 0.512218, acc: 0.742188] [adversarial loss: 1.222194, acc: 0.218750]\n",
      "2249: [discriminator loss: 0.537771, acc: 0.742188] [adversarial loss: 0.798985, acc: 0.500000]\n",
      "2250: [discriminator loss: 0.494850, acc: 0.734375] [adversarial loss: 1.752518, acc: 0.046875]\n",
      "2251: [discriminator loss: 0.530443, acc: 0.687500] [adversarial loss: 1.100369, acc: 0.312500]\n",
      "2252: [discriminator loss: 0.519595, acc: 0.718750] [adversarial loss: 1.534329, acc: 0.125000]\n",
      "2253: [discriminator loss: 0.489159, acc: 0.804688] [adversarial loss: 0.994673, acc: 0.296875]\n",
      "2254: [discriminator loss: 0.456472, acc: 0.812500] [adversarial loss: 1.173121, acc: 0.265625]\n",
      "2255: [discriminator loss: 0.441277, acc: 0.804688] [adversarial loss: 1.131896, acc: 0.250000]\n",
      "2256: [discriminator loss: 0.493288, acc: 0.726562] [adversarial loss: 1.464322, acc: 0.125000]\n",
      "2257: [discriminator loss: 0.536040, acc: 0.726562] [adversarial loss: 0.914170, acc: 0.484375]\n",
      "2258: [discriminator loss: 0.552488, acc: 0.703125] [adversarial loss: 1.724552, acc: 0.093750]\n",
      "2259: [discriminator loss: 0.579700, acc: 0.703125] [adversarial loss: 0.807540, acc: 0.437500]\n",
      "2260: [discriminator loss: 0.582183, acc: 0.671875] [adversarial loss: 1.452019, acc: 0.156250]\n",
      "2261: [discriminator loss: 0.480933, acc: 0.750000] [adversarial loss: 1.272387, acc: 0.218750]\n",
      "2262: [discriminator loss: 0.461837, acc: 0.773438] [adversarial loss: 1.581787, acc: 0.078125]\n",
      "2263: [discriminator loss: 0.459522, acc: 0.804688] [adversarial loss: 1.082031, acc: 0.281250]\n",
      "2264: [discriminator loss: 0.528990, acc: 0.750000] [adversarial loss: 1.590408, acc: 0.125000]\n",
      "2265: [discriminator loss: 0.560516, acc: 0.718750] [adversarial loss: 0.940280, acc: 0.375000]\n",
      "2266: [discriminator loss: 0.521535, acc: 0.734375] [adversarial loss: 1.591695, acc: 0.093750]\n",
      "2267: [discriminator loss: 0.444562, acc: 0.804688] [adversarial loss: 0.999507, acc: 0.343750]\n",
      "2268: [discriminator loss: 0.549104, acc: 0.718750] [adversarial loss: 1.737644, acc: 0.046875]\n",
      "2269: [discriminator loss: 0.521061, acc: 0.765625] [adversarial loss: 1.022425, acc: 0.281250]\n",
      "2270: [discriminator loss: 0.567125, acc: 0.695312] [adversarial loss: 1.532473, acc: 0.078125]\n",
      "2271: [discriminator loss: 0.516370, acc: 0.703125] [adversarial loss: 0.969215, acc: 0.328125]\n",
      "2272: [discriminator loss: 0.497548, acc: 0.726562] [adversarial loss: 1.518817, acc: 0.093750]\n",
      "2273: [discriminator loss: 0.493795, acc: 0.750000] [adversarial loss: 1.099635, acc: 0.187500]\n",
      "2274: [discriminator loss: 0.509610, acc: 0.750000] [adversarial loss: 1.735679, acc: 0.078125]\n",
      "2275: [discriminator loss: 0.453141, acc: 0.781250] [adversarial loss: 0.865899, acc: 0.390625]\n",
      "2276: [discriminator loss: 0.503751, acc: 0.789062] [adversarial loss: 1.753743, acc: 0.140625]\n",
      "2277: [discriminator loss: 0.573983, acc: 0.679688] [adversarial loss: 1.000111, acc: 0.375000]\n",
      "2278: [discriminator loss: 0.499470, acc: 0.773438] [adversarial loss: 1.386322, acc: 0.125000]\n",
      "2279: [discriminator loss: 0.500921, acc: 0.781250] [adversarial loss: 1.285512, acc: 0.171875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280: [discriminator loss: 0.511326, acc: 0.765625] [adversarial loss: 1.360319, acc: 0.156250]\n",
      "2281: [discriminator loss: 0.535169, acc: 0.687500] [adversarial loss: 1.634016, acc: 0.093750]\n",
      "2282: [discriminator loss: 0.561921, acc: 0.710938] [adversarial loss: 1.048523, acc: 0.312500]\n",
      "2283: [discriminator loss: 0.519479, acc: 0.773438] [adversarial loss: 1.600933, acc: 0.093750]\n",
      "2284: [discriminator loss: 0.497112, acc: 0.796875] [adversarial loss: 1.002477, acc: 0.281250]\n",
      "2285: [discriminator loss: 0.490131, acc: 0.804688] [adversarial loss: 1.490990, acc: 0.140625]\n",
      "2286: [discriminator loss: 0.537984, acc: 0.710938] [adversarial loss: 1.210606, acc: 0.343750]\n",
      "2287: [discriminator loss: 0.591606, acc: 0.710938] [adversarial loss: 1.102794, acc: 0.218750]\n",
      "2288: [discriminator loss: 0.460585, acc: 0.781250] [adversarial loss: 1.083611, acc: 0.312500]\n",
      "2289: [discriminator loss: 0.557691, acc: 0.718750] [adversarial loss: 1.605010, acc: 0.093750]\n",
      "2290: [discriminator loss: 0.534795, acc: 0.773438] [adversarial loss: 0.992114, acc: 0.375000]\n",
      "2291: [discriminator loss: 0.466280, acc: 0.773438] [adversarial loss: 1.635850, acc: 0.093750]\n",
      "2292: [discriminator loss: 0.487121, acc: 0.750000] [adversarial loss: 0.969319, acc: 0.437500]\n",
      "2293: [discriminator loss: 0.538138, acc: 0.710938] [adversarial loss: 1.695900, acc: 0.031250]\n",
      "2294: [discriminator loss: 0.548288, acc: 0.726562] [adversarial loss: 0.952434, acc: 0.437500]\n",
      "2295: [discriminator loss: 0.493759, acc: 0.781250] [adversarial loss: 1.553908, acc: 0.078125]\n",
      "2296: [discriminator loss: 0.558168, acc: 0.734375] [adversarial loss: 1.320117, acc: 0.140625]\n",
      "2297: [discriminator loss: 0.493019, acc: 0.781250] [adversarial loss: 1.536269, acc: 0.062500]\n",
      "2298: [discriminator loss: 0.474717, acc: 0.750000] [adversarial loss: 1.165648, acc: 0.281250]\n",
      "2299: [discriminator loss: 0.535977, acc: 0.671875] [adversarial loss: 1.450373, acc: 0.125000]\n",
      "2300: [discriminator loss: 0.463748, acc: 0.742188] [adversarial loss: 0.993459, acc: 0.343750]\n",
      "2301: [discriminator loss: 0.493487, acc: 0.765625] [adversarial loss: 1.433550, acc: 0.078125]\n",
      "2302: [discriminator loss: 0.476850, acc: 0.757812] [adversarial loss: 1.312931, acc: 0.156250]\n",
      "2303: [discriminator loss: 0.509219, acc: 0.742188] [adversarial loss: 1.219272, acc: 0.203125]\n",
      "2304: [discriminator loss: 0.459593, acc: 0.820312] [adversarial loss: 1.091541, acc: 0.265625]\n",
      "2305: [discriminator loss: 0.451565, acc: 0.789062] [adversarial loss: 1.644095, acc: 0.125000]\n",
      "2306: [discriminator loss: 0.500951, acc: 0.773438] [adversarial loss: 1.064947, acc: 0.343750]\n",
      "2307: [discriminator loss: 0.503531, acc: 0.765625] [adversarial loss: 1.765839, acc: 0.156250]\n",
      "2308: [discriminator loss: 0.559289, acc: 0.710938] [adversarial loss: 0.732511, acc: 0.515625]\n",
      "2309: [discriminator loss: 0.464546, acc: 0.773438] [adversarial loss: 1.858393, acc: 0.093750]\n",
      "2310: [discriminator loss: 0.601276, acc: 0.695312] [adversarial loss: 0.794727, acc: 0.484375]\n",
      "2311: [discriminator loss: 0.635574, acc: 0.648438] [adversarial loss: 1.764920, acc: 0.046875]\n",
      "2312: [discriminator loss: 0.537683, acc: 0.718750] [adversarial loss: 0.895528, acc: 0.406250]\n",
      "2313: [discriminator loss: 0.565045, acc: 0.710938] [adversarial loss: 1.520596, acc: 0.109375]\n",
      "2314: [discriminator loss: 0.545922, acc: 0.750000] [adversarial loss: 1.158859, acc: 0.187500]\n",
      "2315: [discriminator loss: 0.468921, acc: 0.781250] [adversarial loss: 1.222060, acc: 0.171875]\n",
      "2316: [discriminator loss: 0.522690, acc: 0.742188] [adversarial loss: 1.670101, acc: 0.062500]\n",
      "2317: [discriminator loss: 0.514596, acc: 0.765625] [adversarial loss: 1.013446, acc: 0.406250]\n",
      "2318: [discriminator loss: 0.436437, acc: 0.812500] [adversarial loss: 1.573220, acc: 0.078125]\n",
      "2319: [discriminator loss: 0.475417, acc: 0.789062] [adversarial loss: 1.184445, acc: 0.281250]\n",
      "2320: [discriminator loss: 0.415326, acc: 0.781250] [adversarial loss: 1.695171, acc: 0.046875]\n",
      "2321: [discriminator loss: 0.516292, acc: 0.718750] [adversarial loss: 0.796585, acc: 0.500000]\n",
      "2322: [discriminator loss: 0.579921, acc: 0.734375] [adversarial loss: 2.229448, acc: 0.062500]\n",
      "2323: [discriminator loss: 0.681924, acc: 0.609375] [adversarial loss: 0.782360, acc: 0.453125]\n",
      "2324: [discriminator loss: 0.570099, acc: 0.718750] [adversarial loss: 1.451552, acc: 0.140625]\n",
      "2325: [discriminator loss: 0.478294, acc: 0.750000] [adversarial loss: 1.205719, acc: 0.281250]\n",
      "2326: [discriminator loss: 0.485102, acc: 0.734375] [adversarial loss: 1.012002, acc: 0.312500]\n",
      "2327: [discriminator loss: 0.483640, acc: 0.796875] [adversarial loss: 1.506159, acc: 0.078125]\n",
      "2328: [discriminator loss: 0.485870, acc: 0.781250] [adversarial loss: 1.047571, acc: 0.343750]\n",
      "2329: [discriminator loss: 0.521286, acc: 0.757812] [adversarial loss: 1.757063, acc: 0.078125]\n",
      "2330: [discriminator loss: 0.485931, acc: 0.765625] [adversarial loss: 1.017921, acc: 0.390625]\n",
      "2331: [discriminator loss: 0.570682, acc: 0.718750] [adversarial loss: 1.393202, acc: 0.156250]\n",
      "2332: [discriminator loss: 0.471515, acc: 0.773438] [adversarial loss: 1.389916, acc: 0.171875]\n",
      "2333: [discriminator loss: 0.522910, acc: 0.718750] [adversarial loss: 0.922442, acc: 0.359375]\n",
      "2334: [discriminator loss: 0.505517, acc: 0.812500] [adversarial loss: 1.443226, acc: 0.125000]\n",
      "2335: [discriminator loss: 0.498573, acc: 0.757812] [adversarial loss: 1.097473, acc: 0.375000]\n",
      "2336: [discriminator loss: 0.503148, acc: 0.789062] [adversarial loss: 1.715665, acc: 0.093750]\n",
      "2337: [discriminator loss: 0.464348, acc: 0.757812] [adversarial loss: 0.944211, acc: 0.437500]\n",
      "2338: [discriminator loss: 0.499699, acc: 0.742188] [adversarial loss: 1.634021, acc: 0.062500]\n",
      "2339: [discriminator loss: 0.542924, acc: 0.695312] [adversarial loss: 0.842095, acc: 0.437500]\n",
      "2340: [discriminator loss: 0.487347, acc: 0.789062] [adversarial loss: 1.455957, acc: 0.125000]\n",
      "2341: [discriminator loss: 0.523046, acc: 0.750000] [adversarial loss: 0.938423, acc: 0.359375]\n",
      "2342: [discriminator loss: 0.542442, acc: 0.726562] [adversarial loss: 1.658498, acc: 0.078125]\n",
      "2343: [discriminator loss: 0.586759, acc: 0.679688] [adversarial loss: 1.371839, acc: 0.218750]\n",
      "2344: [discriminator loss: 0.508362, acc: 0.718750] [adversarial loss: 1.463046, acc: 0.171875]\n",
      "2345: [discriminator loss: 0.519335, acc: 0.750000] [adversarial loss: 1.152498, acc: 0.281250]\n",
      "2346: [discriminator loss: 0.491150, acc: 0.750000] [adversarial loss: 1.024358, acc: 0.312500]\n",
      "2347: [discriminator loss: 0.454256, acc: 0.789062] [adversarial loss: 1.669548, acc: 0.078125]\n",
      "2348: [discriminator loss: 0.542994, acc: 0.718750] [adversarial loss: 1.193083, acc: 0.281250]\n",
      "2349: [discriminator loss: 0.518535, acc: 0.757812] [adversarial loss: 1.177197, acc: 0.312500]\n",
      "2350: [discriminator loss: 0.530203, acc: 0.757812] [adversarial loss: 1.363660, acc: 0.171875]\n",
      "2351: [discriminator loss: 0.486337, acc: 0.773438] [adversarial loss: 1.108289, acc: 0.312500]\n",
      "2352: [discriminator loss: 0.459464, acc: 0.789062] [adversarial loss: 1.831216, acc: 0.046875]\n",
      "2353: [discriminator loss: 0.468646, acc: 0.804688] [adversarial loss: 0.838048, acc: 0.500000]\n",
      "2354: [discriminator loss: 0.529815, acc: 0.734375] [adversarial loss: 1.641394, acc: 0.140625]\n",
      "2355: [discriminator loss: 0.537670, acc: 0.718750] [adversarial loss: 0.913933, acc: 0.468750]\n",
      "2356: [discriminator loss: 0.507423, acc: 0.710938] [adversarial loss: 1.519029, acc: 0.187500]\n",
      "2357: [discriminator loss: 0.532295, acc: 0.750000] [adversarial loss: 1.028654, acc: 0.390625]\n",
      "2358: [discriminator loss: 0.497862, acc: 0.742188] [adversarial loss: 1.562160, acc: 0.187500]\n",
      "2359: [discriminator loss: 0.469680, acc: 0.773438] [adversarial loss: 0.753071, acc: 0.562500]\n",
      "2360: [discriminator loss: 0.506914, acc: 0.703125] [adversarial loss: 1.856059, acc: 0.078125]\n",
      "2361: [discriminator loss: 0.585748, acc: 0.703125] [adversarial loss: 1.286408, acc: 0.250000]\n",
      "2362: [discriminator loss: 0.539412, acc: 0.734375] [adversarial loss: 1.204347, acc: 0.218750]\n",
      "2363: [discriminator loss: 0.431514, acc: 0.843750] [adversarial loss: 1.580158, acc: 0.109375]\n",
      "2364: [discriminator loss: 0.488508, acc: 0.804688] [adversarial loss: 1.383004, acc: 0.234375]\n",
      "2365: [discriminator loss: 0.580046, acc: 0.710938] [adversarial loss: 1.332287, acc: 0.187500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366: [discriminator loss: 0.443014, acc: 0.796875] [adversarial loss: 1.442482, acc: 0.140625]\n",
      "2367: [discriminator loss: 0.512215, acc: 0.757812] [adversarial loss: 1.263278, acc: 0.218750]\n",
      "2368: [discriminator loss: 0.549170, acc: 0.687500] [adversarial loss: 1.326830, acc: 0.187500]\n",
      "2369: [discriminator loss: 0.522230, acc: 0.742188] [adversarial loss: 1.173141, acc: 0.203125]\n",
      "2370: [discriminator loss: 0.500312, acc: 0.781250] [adversarial loss: 1.405684, acc: 0.234375]\n",
      "2371: [discriminator loss: 0.520772, acc: 0.718750] [adversarial loss: 1.523323, acc: 0.218750]\n",
      "2372: [discriminator loss: 0.538505, acc: 0.671875] [adversarial loss: 1.277765, acc: 0.171875]\n",
      "2373: [discriminator loss: 0.404649, acc: 0.835938] [adversarial loss: 1.127699, acc: 0.250000]\n",
      "2374: [discriminator loss: 0.490993, acc: 0.765625] [adversarial loss: 1.248392, acc: 0.203125]\n",
      "2375: [discriminator loss: 0.515539, acc: 0.781250] [adversarial loss: 1.189560, acc: 0.234375]\n",
      "2376: [discriminator loss: 0.441599, acc: 0.781250] [adversarial loss: 1.550905, acc: 0.109375]\n",
      "2377: [discriminator loss: 0.459509, acc: 0.789062] [adversarial loss: 1.236765, acc: 0.218750]\n",
      "2378: [discriminator loss: 0.496212, acc: 0.734375] [adversarial loss: 1.669178, acc: 0.140625]\n",
      "2379: [discriminator loss: 0.520557, acc: 0.781250] [adversarial loss: 0.915408, acc: 0.437500]\n",
      "2380: [discriminator loss: 0.579290, acc: 0.703125] [adversarial loss: 2.077927, acc: 0.031250]\n",
      "2381: [discriminator loss: 0.600443, acc: 0.679688] [adversarial loss: 0.777550, acc: 0.515625]\n",
      "2382: [discriminator loss: 0.613959, acc: 0.679688] [adversarial loss: 1.744630, acc: 0.109375]\n",
      "2383: [discriminator loss: 0.543659, acc: 0.734375] [adversarial loss: 1.220383, acc: 0.171875]\n",
      "2384: [discriminator loss: 0.559416, acc: 0.718750] [adversarial loss: 1.477140, acc: 0.140625]\n",
      "2385: [discriminator loss: 0.542046, acc: 0.765625] [adversarial loss: 1.123308, acc: 0.296875]\n",
      "2386: [discriminator loss: 0.424587, acc: 0.851562] [adversarial loss: 1.399012, acc: 0.187500]\n",
      "2387: [discriminator loss: 0.511912, acc: 0.718750] [adversarial loss: 1.068106, acc: 0.312500]\n",
      "2388: [discriminator loss: 0.519570, acc: 0.718750] [adversarial loss: 1.699318, acc: 0.093750]\n",
      "2389: [discriminator loss: 0.582141, acc: 0.734375] [adversarial loss: 1.114873, acc: 0.265625]\n",
      "2390: [discriminator loss: 0.494917, acc: 0.781250] [adversarial loss: 1.214628, acc: 0.250000]\n",
      "2391: [discriminator loss: 0.482078, acc: 0.773438] [adversarial loss: 1.538561, acc: 0.109375]\n",
      "2392: [discriminator loss: 0.575696, acc: 0.671875] [adversarial loss: 1.130540, acc: 0.312500]\n",
      "2393: [discriminator loss: 0.561099, acc: 0.679688] [adversarial loss: 1.577629, acc: 0.156250]\n",
      "2394: [discriminator loss: 0.551333, acc: 0.710938] [adversarial loss: 0.938959, acc: 0.421875]\n",
      "2395: [discriminator loss: 0.476309, acc: 0.757812] [adversarial loss: 1.432874, acc: 0.125000]\n",
      "2396: [discriminator loss: 0.525786, acc: 0.742188] [adversarial loss: 0.948934, acc: 0.359375]\n",
      "2397: [discriminator loss: 0.551397, acc: 0.710938] [adversarial loss: 1.507149, acc: 0.125000]\n",
      "2398: [discriminator loss: 0.484683, acc: 0.718750] [adversarial loss: 1.628520, acc: 0.109375]\n",
      "2399: [discriminator loss: 0.504803, acc: 0.757812] [adversarial loss: 0.868015, acc: 0.437500]\n",
      "2400: [discriminator loss: 0.534001, acc: 0.703125] [adversarial loss: 1.915224, acc: 0.062500]\n",
      "2401: [discriminator loss: 0.545125, acc: 0.703125] [adversarial loss: 0.988014, acc: 0.312500]\n",
      "2402: [discriminator loss: 0.579969, acc: 0.695312] [adversarial loss: 1.820180, acc: 0.109375]\n",
      "2403: [discriminator loss: 0.553382, acc: 0.757812] [adversarial loss: 1.168806, acc: 0.171875]\n",
      "2404: [discriminator loss: 0.460818, acc: 0.773438] [adversarial loss: 1.521036, acc: 0.109375]\n",
      "2405: [discriminator loss: 0.452934, acc: 0.812500] [adversarial loss: 0.856582, acc: 0.437500]\n",
      "2406: [discriminator loss: 0.561410, acc: 0.687500] [adversarial loss: 1.527671, acc: 0.109375]\n",
      "2407: [discriminator loss: 0.521811, acc: 0.742188] [adversarial loss: 1.193471, acc: 0.203125]\n",
      "2408: [discriminator loss: 0.507445, acc: 0.742188] [adversarial loss: 1.489137, acc: 0.171875]\n",
      "2409: [discriminator loss: 0.475102, acc: 0.750000] [adversarial loss: 1.057084, acc: 0.328125]\n",
      "2410: [discriminator loss: 0.438493, acc: 0.796875] [adversarial loss: 1.304783, acc: 0.234375]\n",
      "2411: [discriminator loss: 0.506206, acc: 0.773438] [adversarial loss: 1.384741, acc: 0.156250]\n",
      "2412: [discriminator loss: 0.487325, acc: 0.750000] [adversarial loss: 1.329446, acc: 0.203125]\n",
      "2413: [discriminator loss: 0.483614, acc: 0.789062] [adversarial loss: 1.539763, acc: 0.171875]\n",
      "2414: [discriminator loss: 0.510002, acc: 0.765625] [adversarial loss: 1.042083, acc: 0.375000]\n",
      "2415: [discriminator loss: 0.543712, acc: 0.781250] [adversarial loss: 1.518747, acc: 0.125000]\n",
      "2416: [discriminator loss: 0.549450, acc: 0.718750] [adversarial loss: 1.028440, acc: 0.375000]\n",
      "2417: [discriminator loss: 0.516881, acc: 0.757812] [adversarial loss: 1.895220, acc: 0.046875]\n",
      "2418: [discriminator loss: 0.525329, acc: 0.781250] [adversarial loss: 0.745029, acc: 0.500000]\n",
      "2419: [discriminator loss: 0.513182, acc: 0.742188] [adversarial loss: 1.884202, acc: 0.078125]\n",
      "2420: [discriminator loss: 0.570833, acc: 0.710938] [adversarial loss: 0.971531, acc: 0.406250]\n",
      "2421: [discriminator loss: 0.529235, acc: 0.726562] [adversarial loss: 2.017236, acc: 0.078125]\n",
      "2422: [discriminator loss: 0.622159, acc: 0.671875] [adversarial loss: 0.934803, acc: 0.421875]\n",
      "2423: [discriminator loss: 0.568887, acc: 0.695312] [adversarial loss: 1.738577, acc: 0.109375]\n",
      "2424: [discriminator loss: 0.529071, acc: 0.726562] [adversarial loss: 0.893482, acc: 0.421875]\n",
      "2425: [discriminator loss: 0.517689, acc: 0.750000] [adversarial loss: 1.697079, acc: 0.046875]\n",
      "2426: [discriminator loss: 0.583054, acc: 0.687500] [adversarial loss: 1.035865, acc: 0.296875]\n",
      "2427: [discriminator loss: 0.510678, acc: 0.781250] [adversarial loss: 1.532841, acc: 0.046875]\n",
      "2428: [discriminator loss: 0.539804, acc: 0.710938] [adversarial loss: 1.073479, acc: 0.359375]\n",
      "2429: [discriminator loss: 0.443529, acc: 0.820312] [adversarial loss: 1.226437, acc: 0.234375]\n",
      "2430: [discriminator loss: 0.429762, acc: 0.835938] [adversarial loss: 1.344373, acc: 0.171875]\n",
      "2431: [discriminator loss: 0.544552, acc: 0.734375] [adversarial loss: 1.277322, acc: 0.203125]\n",
      "2432: [discriminator loss: 0.455983, acc: 0.781250] [adversarial loss: 1.035405, acc: 0.421875]\n",
      "2433: [discriminator loss: 0.453746, acc: 0.820312] [adversarial loss: 1.690037, acc: 0.140625]\n",
      "2434: [discriminator loss: 0.545323, acc: 0.671875] [adversarial loss: 1.044031, acc: 0.328125]\n",
      "2435: [discriminator loss: 0.511647, acc: 0.757812] [adversarial loss: 1.590652, acc: 0.125000]\n",
      "2436: [discriminator loss: 0.477229, acc: 0.765625] [adversarial loss: 1.043828, acc: 0.375000]\n",
      "2437: [discriminator loss: 0.584680, acc: 0.726562] [adversarial loss: 1.650502, acc: 0.171875]\n",
      "2438: [discriminator loss: 0.604460, acc: 0.695312] [adversarial loss: 0.815309, acc: 0.421875]\n",
      "2439: [discriminator loss: 0.523358, acc: 0.718750] [adversarial loss: 1.623927, acc: 0.109375]\n",
      "2440: [discriminator loss: 0.504023, acc: 0.726562] [adversarial loss: 0.698814, acc: 0.515625]\n",
      "2441: [discriminator loss: 0.496634, acc: 0.773438] [adversarial loss: 1.304464, acc: 0.234375]\n",
      "2442: [discriminator loss: 0.503544, acc: 0.773438] [adversarial loss: 1.125306, acc: 0.328125]\n",
      "2443: [discriminator loss: 0.518752, acc: 0.742188] [adversarial loss: 1.256633, acc: 0.234375]\n",
      "2444: [discriminator loss: 0.465794, acc: 0.804688] [adversarial loss: 1.009140, acc: 0.281250]\n",
      "2445: [discriminator loss: 0.527056, acc: 0.750000] [adversarial loss: 1.437500, acc: 0.140625]\n",
      "2446: [discriminator loss: 0.459393, acc: 0.773438] [adversarial loss: 1.067558, acc: 0.375000]\n",
      "2447: [discriminator loss: 0.482671, acc: 0.781250] [adversarial loss: 1.619097, acc: 0.156250]\n",
      "2448: [discriminator loss: 0.558611, acc: 0.734375] [adversarial loss: 1.094282, acc: 0.328125]\n",
      "2449: [discriminator loss: 0.503187, acc: 0.773438] [adversarial loss: 1.573576, acc: 0.140625]\n",
      "2450: [discriminator loss: 0.531865, acc: 0.679688] [adversarial loss: 1.019590, acc: 0.390625]\n",
      "2451: [discriminator loss: 0.519699, acc: 0.718750] [adversarial loss: 1.713269, acc: 0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2452: [discriminator loss: 0.583879, acc: 0.718750] [adversarial loss: 1.039095, acc: 0.328125]\n",
      "2453: [discriminator loss: 0.530964, acc: 0.765625] [adversarial loss: 1.500047, acc: 0.109375]\n",
      "2454: [discriminator loss: 0.426106, acc: 0.859375] [adversarial loss: 1.431459, acc: 0.140625]\n",
      "2455: [discriminator loss: 0.496175, acc: 0.750000] [adversarial loss: 1.401778, acc: 0.140625]\n",
      "2456: [discriminator loss: 0.465924, acc: 0.781250] [adversarial loss: 1.019549, acc: 0.328125]\n",
      "2457: [discriminator loss: 0.617474, acc: 0.679688] [adversarial loss: 1.796991, acc: 0.093750]\n",
      "2458: [discriminator loss: 0.483112, acc: 0.726562] [adversarial loss: 1.109648, acc: 0.218750]\n",
      "2459: [discriminator loss: 0.529927, acc: 0.742188] [adversarial loss: 1.722018, acc: 0.109375]\n",
      "2460: [discriminator loss: 0.497859, acc: 0.765625] [adversarial loss: 0.934425, acc: 0.406250]\n",
      "2461: [discriminator loss: 0.532557, acc: 0.703125] [adversarial loss: 1.973149, acc: 0.109375]\n",
      "2462: [discriminator loss: 0.488352, acc: 0.765625] [adversarial loss: 0.982639, acc: 0.390625]\n",
      "2463: [discriminator loss: 0.539912, acc: 0.742188] [adversarial loss: 1.971045, acc: 0.015625]\n",
      "2464: [discriminator loss: 0.564987, acc: 0.710938] [adversarial loss: 1.021041, acc: 0.328125]\n",
      "2465: [discriminator loss: 0.521069, acc: 0.710938] [adversarial loss: 1.316302, acc: 0.156250]\n",
      "2466: [discriminator loss: 0.458225, acc: 0.789062] [adversarial loss: 1.207427, acc: 0.203125]\n",
      "2467: [discriminator loss: 0.565294, acc: 0.695312] [adversarial loss: 1.126900, acc: 0.250000]\n",
      "2468: [discriminator loss: 0.516125, acc: 0.695312] [adversarial loss: 1.684215, acc: 0.078125]\n",
      "2469: [discriminator loss: 0.477417, acc: 0.773438] [adversarial loss: 0.801755, acc: 0.453125]\n",
      "2470: [discriminator loss: 0.522939, acc: 0.757812] [adversarial loss: 1.554003, acc: 0.203125]\n",
      "2471: [discriminator loss: 0.540411, acc: 0.734375] [adversarial loss: 1.132062, acc: 0.312500]\n",
      "2472: [discriminator loss: 0.510328, acc: 0.726562] [adversarial loss: 1.422686, acc: 0.109375]\n",
      "2473: [discriminator loss: 0.486794, acc: 0.773438] [adversarial loss: 1.037298, acc: 0.281250]\n",
      "2474: [discriminator loss: 0.499491, acc: 0.742188] [adversarial loss: 1.304044, acc: 0.203125]\n",
      "2475: [discriminator loss: 0.465628, acc: 0.796875] [adversarial loss: 1.169788, acc: 0.250000]\n",
      "2476: [discriminator loss: 0.545957, acc: 0.734375] [adversarial loss: 1.740680, acc: 0.078125]\n",
      "2477: [discriminator loss: 0.531527, acc: 0.742188] [adversarial loss: 0.855829, acc: 0.328125]\n",
      "2478: [discriminator loss: 0.501959, acc: 0.781250] [adversarial loss: 1.881935, acc: 0.062500]\n",
      "2479: [discriminator loss: 0.575658, acc: 0.718750] [adversarial loss: 0.772192, acc: 0.500000]\n",
      "2480: [discriminator loss: 0.564868, acc: 0.703125] [adversarial loss: 1.944144, acc: 0.031250]\n",
      "2481: [discriminator loss: 0.552191, acc: 0.710938] [adversarial loss: 0.938199, acc: 0.406250]\n",
      "2482: [discriminator loss: 0.595837, acc: 0.695312] [adversarial loss: 1.796172, acc: 0.078125]\n",
      "2483: [discriminator loss: 0.546545, acc: 0.718750] [adversarial loss: 1.073837, acc: 0.281250]\n",
      "2484: [discriminator loss: 0.479902, acc: 0.812500] [adversarial loss: 1.449427, acc: 0.125000]\n",
      "2485: [discriminator loss: 0.482678, acc: 0.765625] [adversarial loss: 1.147607, acc: 0.281250]\n",
      "2486: [discriminator loss: 0.455814, acc: 0.812500] [adversarial loss: 1.410991, acc: 0.156250]\n",
      "2487: [discriminator loss: 0.582174, acc: 0.703125] [adversarial loss: 1.235767, acc: 0.187500]\n",
      "2488: [discriminator loss: 0.473264, acc: 0.781250] [adversarial loss: 1.585380, acc: 0.140625]\n",
      "2489: [discriminator loss: 0.504372, acc: 0.781250] [adversarial loss: 0.890751, acc: 0.421875]\n",
      "2490: [discriminator loss: 0.538162, acc: 0.710938] [adversarial loss: 1.684451, acc: 0.078125]\n",
      "2491: [discriminator loss: 0.463782, acc: 0.757812] [adversarial loss: 1.073305, acc: 0.265625]\n",
      "2492: [discriminator loss: 0.547821, acc: 0.750000] [adversarial loss: 1.907979, acc: 0.093750]\n",
      "2493: [discriminator loss: 0.598194, acc: 0.703125] [adversarial loss: 0.946821, acc: 0.390625]\n",
      "2494: [discriminator loss: 0.493854, acc: 0.781250] [adversarial loss: 2.122887, acc: 0.093750]\n",
      "2495: [discriminator loss: 0.563966, acc: 0.710938] [adversarial loss: 1.146541, acc: 0.250000]\n",
      "2496: [discriminator loss: 0.531965, acc: 0.789062] [adversarial loss: 1.645432, acc: 0.062500]\n",
      "2497: [discriminator loss: 0.531393, acc: 0.703125] [adversarial loss: 1.052952, acc: 0.312500]\n",
      "2498: [discriminator loss: 0.529366, acc: 0.726562] [adversarial loss: 1.523221, acc: 0.078125]\n",
      "2499: [discriminator loss: 0.547033, acc: 0.710938] [adversarial loss: 1.149094, acc: 0.187500]\n",
      "2500: [discriminator loss: 0.515419, acc: 0.765625] [adversarial loss: 1.630714, acc: 0.062500]\n",
      "2501: [discriminator loss: 0.570122, acc: 0.710938] [adversarial loss: 1.473513, acc: 0.125000]\n",
      "2502: [discriminator loss: 0.435083, acc: 0.796875] [adversarial loss: 1.117561, acc: 0.203125]\n",
      "2503: [discriminator loss: 0.403517, acc: 0.828125] [adversarial loss: 1.258095, acc: 0.203125]\n",
      "2504: [discriminator loss: 0.521299, acc: 0.703125] [adversarial loss: 1.422828, acc: 0.109375]\n",
      "2505: [discriminator loss: 0.556931, acc: 0.703125] [adversarial loss: 1.183243, acc: 0.265625]\n",
      "2506: [discriminator loss: 0.485975, acc: 0.773438] [adversarial loss: 1.335618, acc: 0.140625]\n",
      "2507: [discriminator loss: 0.505397, acc: 0.750000] [adversarial loss: 1.112139, acc: 0.265625]\n",
      "2508: [discriminator loss: 0.487940, acc: 0.773438] [adversarial loss: 1.803007, acc: 0.062500]\n",
      "2509: [discriminator loss: 0.538824, acc: 0.726562] [adversarial loss: 1.045838, acc: 0.312500]\n",
      "2510: [discriminator loss: 0.438152, acc: 0.812500] [adversarial loss: 1.634923, acc: 0.109375]\n",
      "2511: [discriminator loss: 0.584312, acc: 0.750000] [adversarial loss: 0.987225, acc: 0.312500]\n",
      "2512: [discriminator loss: 0.483837, acc: 0.765625] [adversarial loss: 1.735919, acc: 0.046875]\n",
      "2513: [discriminator loss: 0.494948, acc: 0.750000] [adversarial loss: 0.900527, acc: 0.437500]\n",
      "2514: [discriminator loss: 0.553912, acc: 0.710938] [adversarial loss: 1.575890, acc: 0.109375]\n",
      "2515: [discriminator loss: 0.527274, acc: 0.781250] [adversarial loss: 1.007279, acc: 0.375000]\n",
      "2516: [discriminator loss: 0.563815, acc: 0.710938] [adversarial loss: 1.966210, acc: 0.062500]\n",
      "2517: [discriminator loss: 0.619990, acc: 0.687500] [adversarial loss: 0.871535, acc: 0.390625]\n",
      "2518: [discriminator loss: 0.567037, acc: 0.750000] [adversarial loss: 1.726641, acc: 0.093750]\n",
      "2519: [discriminator loss: 0.601059, acc: 0.671875] [adversarial loss: 0.944843, acc: 0.453125]\n",
      "2520: [discriminator loss: 0.516757, acc: 0.695312] [adversarial loss: 1.301500, acc: 0.218750]\n",
      "2521: [discriminator loss: 0.510371, acc: 0.773438] [adversarial loss: 1.074202, acc: 0.296875]\n",
      "2522: [discriminator loss: 0.478832, acc: 0.773438] [adversarial loss: 1.370778, acc: 0.125000]\n",
      "2523: [discriminator loss: 0.455445, acc: 0.789062] [adversarial loss: 1.215883, acc: 0.203125]\n",
      "2524: [discriminator loss: 0.502383, acc: 0.710938] [adversarial loss: 1.579178, acc: 0.078125]\n",
      "2525: [discriminator loss: 0.635021, acc: 0.695312] [adversarial loss: 0.950222, acc: 0.343750]\n",
      "2526: [discriminator loss: 0.478779, acc: 0.773438] [adversarial loss: 1.382073, acc: 0.140625]\n",
      "2527: [discriminator loss: 0.447408, acc: 0.812500] [adversarial loss: 1.264938, acc: 0.156250]\n",
      "2528: [discriminator loss: 0.448415, acc: 0.789062] [adversarial loss: 1.407762, acc: 0.156250]\n",
      "2529: [discriminator loss: 0.470318, acc: 0.781250] [adversarial loss: 1.169258, acc: 0.312500]\n",
      "2530: [discriminator loss: 0.522776, acc: 0.718750] [adversarial loss: 1.129409, acc: 0.296875]\n",
      "2531: [discriminator loss: 0.527701, acc: 0.757812] [adversarial loss: 1.243548, acc: 0.203125]\n",
      "2532: [discriminator loss: 0.477373, acc: 0.804688] [adversarial loss: 1.101898, acc: 0.328125]\n",
      "2533: [discriminator loss: 0.444083, acc: 0.757812] [adversarial loss: 1.722992, acc: 0.046875]\n",
      "2534: [discriminator loss: 0.552871, acc: 0.750000] [adversarial loss: 0.974945, acc: 0.359375]\n",
      "2535: [discriminator loss: 0.462520, acc: 0.789062] [adversarial loss: 1.461147, acc: 0.109375]\n",
      "2536: [discriminator loss: 0.538973, acc: 0.773438] [adversarial loss: 1.226973, acc: 0.281250]\n",
      "2537: [discriminator loss: 0.499367, acc: 0.773438] [adversarial loss: 1.326587, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2538: [discriminator loss: 0.520486, acc: 0.742188] [adversarial loss: 1.376393, acc: 0.203125]\n",
      "2539: [discriminator loss: 0.527447, acc: 0.710938] [adversarial loss: 0.807811, acc: 0.515625]\n",
      "2540: [discriminator loss: 0.573027, acc: 0.734375] [adversarial loss: 1.729451, acc: 0.078125]\n",
      "2541: [discriminator loss: 0.578531, acc: 0.742188] [adversarial loss: 0.844359, acc: 0.437500]\n",
      "2542: [discriminator loss: 0.570592, acc: 0.671875] [adversarial loss: 1.599962, acc: 0.109375]\n",
      "2543: [discriminator loss: 0.474586, acc: 0.750000] [adversarial loss: 1.274958, acc: 0.250000]\n",
      "2544: [discriminator loss: 0.524534, acc: 0.789062] [adversarial loss: 1.228745, acc: 0.250000]\n",
      "2545: [discriminator loss: 0.440575, acc: 0.796875] [adversarial loss: 1.588505, acc: 0.078125]\n",
      "2546: [discriminator loss: 0.488166, acc: 0.757812] [adversarial loss: 1.086694, acc: 0.265625]\n",
      "2547: [discriminator loss: 0.524658, acc: 0.703125] [adversarial loss: 1.396981, acc: 0.234375]\n",
      "2548: [discriminator loss: 0.451274, acc: 0.820312] [adversarial loss: 1.065369, acc: 0.218750]\n",
      "2549: [discriminator loss: 0.400825, acc: 0.835938] [adversarial loss: 1.646084, acc: 0.125000]\n",
      "2550: [discriminator loss: 0.556312, acc: 0.703125] [adversarial loss: 0.892432, acc: 0.453125]\n",
      "2551: [discriminator loss: 0.615728, acc: 0.718750] [adversarial loss: 1.932741, acc: 0.093750]\n",
      "2552: [discriminator loss: 0.560662, acc: 0.695312] [adversarial loss: 0.973616, acc: 0.312500]\n",
      "2553: [discriminator loss: 0.471719, acc: 0.789062] [adversarial loss: 1.380205, acc: 0.093750]\n",
      "2554: [discriminator loss: 0.475945, acc: 0.773438] [adversarial loss: 1.439825, acc: 0.156250]\n",
      "2555: [discriminator loss: 0.531952, acc: 0.773438] [adversarial loss: 1.252551, acc: 0.281250]\n",
      "2556: [discriminator loss: 0.487867, acc: 0.765625] [adversarial loss: 1.499544, acc: 0.125000]\n",
      "2557: [discriminator loss: 0.457862, acc: 0.789062] [adversarial loss: 1.197642, acc: 0.234375]\n",
      "2558: [discriminator loss: 0.507265, acc: 0.781250] [adversarial loss: 1.568129, acc: 0.125000]\n",
      "2559: [discriminator loss: 0.428411, acc: 0.835938] [adversarial loss: 1.030614, acc: 0.328125]\n",
      "2560: [discriminator loss: 0.499995, acc: 0.750000] [adversarial loss: 2.032218, acc: 0.078125]\n",
      "2561: [discriminator loss: 0.505842, acc: 0.757812] [adversarial loss: 0.901328, acc: 0.421875]\n",
      "2562: [discriminator loss: 0.472628, acc: 0.734375] [adversarial loss: 1.888882, acc: 0.093750]\n",
      "2563: [discriminator loss: 0.591484, acc: 0.671875] [adversarial loss: 1.062730, acc: 0.375000]\n",
      "2564: [discriminator loss: 0.493848, acc: 0.796875] [adversarial loss: 1.463667, acc: 0.171875]\n",
      "2565: [discriminator loss: 0.514026, acc: 0.742188] [adversarial loss: 1.210227, acc: 0.218750]\n",
      "2566: [discriminator loss: 0.458298, acc: 0.781250] [adversarial loss: 1.442734, acc: 0.156250]\n",
      "2567: [discriminator loss: 0.422781, acc: 0.796875] [adversarial loss: 1.235404, acc: 0.203125]\n",
      "2568: [discriminator loss: 0.468201, acc: 0.804688] [adversarial loss: 1.067716, acc: 0.375000]\n",
      "2569: [discriminator loss: 0.447172, acc: 0.789062] [adversarial loss: 1.606865, acc: 0.171875]\n",
      "2570: [discriminator loss: 0.396616, acc: 0.835938] [adversarial loss: 1.028389, acc: 0.390625]\n",
      "2571: [discriminator loss: 0.500957, acc: 0.781250] [adversarial loss: 1.860378, acc: 0.062500]\n",
      "2572: [discriminator loss: 0.471475, acc: 0.789062] [adversarial loss: 0.842390, acc: 0.515625]\n",
      "2573: [discriminator loss: 0.548193, acc: 0.710938] [adversarial loss: 1.777491, acc: 0.046875]\n",
      "2574: [discriminator loss: 0.511627, acc: 0.734375] [adversarial loss: 1.287092, acc: 0.203125]\n",
      "2575: [discriminator loss: 0.478028, acc: 0.773438] [adversarial loss: 1.574884, acc: 0.140625]\n",
      "2576: [discriminator loss: 0.525509, acc: 0.703125] [adversarial loss: 1.290273, acc: 0.250000]\n",
      "2577: [discriminator loss: 0.572373, acc: 0.695312] [adversarial loss: 2.214909, acc: 0.031250]\n",
      "2578: [discriminator loss: 0.579122, acc: 0.687500] [adversarial loss: 0.982296, acc: 0.328125]\n",
      "2579: [discriminator loss: 0.604922, acc: 0.695312] [adversarial loss: 1.586738, acc: 0.125000]\n",
      "2580: [discriminator loss: 0.515303, acc: 0.750000] [adversarial loss: 1.056938, acc: 0.343750]\n",
      "2581: [discriminator loss: 0.540770, acc: 0.734375] [adversarial loss: 2.025399, acc: 0.046875]\n",
      "2582: [discriminator loss: 0.574686, acc: 0.703125] [adversarial loss: 0.865133, acc: 0.437500]\n",
      "2583: [discriminator loss: 0.536161, acc: 0.710938] [adversarial loss: 1.791834, acc: 0.046875]\n",
      "2584: [discriminator loss: 0.563541, acc: 0.710938] [adversarial loss: 0.964451, acc: 0.312500]\n",
      "2585: [discriminator loss: 0.531361, acc: 0.773438] [adversarial loss: 1.601096, acc: 0.140625]\n",
      "2586: [discriminator loss: 0.484771, acc: 0.781250] [adversarial loss: 1.251905, acc: 0.250000]\n",
      "2587: [discriminator loss: 0.550742, acc: 0.765625] [adversarial loss: 1.322793, acc: 0.062500]\n",
      "2588: [discriminator loss: 0.532652, acc: 0.750000] [adversarial loss: 1.294858, acc: 0.156250]\n",
      "2589: [discriminator loss: 0.432018, acc: 0.859375] [adversarial loss: 1.404957, acc: 0.171875]\n",
      "2590: [discriminator loss: 0.491908, acc: 0.773438] [adversarial loss: 1.330177, acc: 0.203125]\n",
      "2591: [discriminator loss: 0.485065, acc: 0.750000] [adversarial loss: 0.870765, acc: 0.421875]\n",
      "2592: [discriminator loss: 0.508927, acc: 0.710938] [adversarial loss: 1.799804, acc: 0.078125]\n",
      "2593: [discriminator loss: 0.482840, acc: 0.757812] [adversarial loss: 1.091927, acc: 0.296875]\n",
      "2594: [discriminator loss: 0.487144, acc: 0.757812] [adversarial loss: 1.891398, acc: 0.046875]\n",
      "2595: [discriminator loss: 0.596342, acc: 0.671875] [adversarial loss: 0.919299, acc: 0.468750]\n",
      "2596: [discriminator loss: 0.497511, acc: 0.750000] [adversarial loss: 1.359435, acc: 0.187500]\n",
      "2597: [discriminator loss: 0.475497, acc: 0.750000] [adversarial loss: 1.142641, acc: 0.234375]\n",
      "2598: [discriminator loss: 0.508838, acc: 0.757812] [adversarial loss: 1.217566, acc: 0.140625]\n",
      "2599: [discriminator loss: 0.525661, acc: 0.710938] [adversarial loss: 1.297107, acc: 0.187500]\n",
      "2600: [discriminator loss: 0.516697, acc: 0.742188] [adversarial loss: 1.153689, acc: 0.218750]\n",
      "2601: [discriminator loss: 0.548894, acc: 0.726562] [adversarial loss: 1.531907, acc: 0.171875]\n",
      "2602: [discriminator loss: 0.457165, acc: 0.773438] [adversarial loss: 1.159271, acc: 0.296875]\n",
      "2603: [discriminator loss: 0.498907, acc: 0.757812] [adversarial loss: 1.115108, acc: 0.296875]\n",
      "2604: [discriminator loss: 0.423261, acc: 0.796875] [adversarial loss: 1.074937, acc: 0.312500]\n",
      "2605: [discriminator loss: 0.545405, acc: 0.734375] [adversarial loss: 1.290091, acc: 0.265625]\n",
      "2606: [discriminator loss: 0.525817, acc: 0.726562] [adversarial loss: 1.067565, acc: 0.281250]\n",
      "2607: [discriminator loss: 0.443229, acc: 0.796875] [adversarial loss: 1.378459, acc: 0.156250]\n",
      "2608: [discriminator loss: 0.459648, acc: 0.789062] [adversarial loss: 1.360634, acc: 0.218750]\n",
      "2609: [discriminator loss: 0.480696, acc: 0.804688] [adversarial loss: 1.558102, acc: 0.187500]\n",
      "2610: [discriminator loss: 0.571483, acc: 0.687500] [adversarial loss: 1.163241, acc: 0.234375]\n",
      "2611: [discriminator loss: 0.468294, acc: 0.750000] [adversarial loss: 1.356569, acc: 0.203125]\n",
      "2612: [discriminator loss: 0.579905, acc: 0.710938] [adversarial loss: 1.427367, acc: 0.250000]\n",
      "2613: [discriminator loss: 0.524007, acc: 0.742188] [adversarial loss: 1.227807, acc: 0.203125]\n",
      "2614: [discriminator loss: 0.497938, acc: 0.757812] [adversarial loss: 1.240116, acc: 0.296875]\n",
      "2615: [discriminator loss: 0.555304, acc: 0.710938] [adversarial loss: 2.026946, acc: 0.093750]\n",
      "2616: [discriminator loss: 0.616396, acc: 0.679688] [adversarial loss: 0.842800, acc: 0.468750]\n",
      "2617: [discriminator loss: 0.580743, acc: 0.703125] [adversarial loss: 1.773484, acc: 0.078125]\n",
      "2618: [discriminator loss: 0.551461, acc: 0.742188] [adversarial loss: 0.775810, acc: 0.515625]\n",
      "2619: [discriminator loss: 0.504924, acc: 0.750000] [adversarial loss: 1.715911, acc: 0.109375]\n",
      "2620: [discriminator loss: 0.540545, acc: 0.757812] [adversarial loss: 1.201760, acc: 0.234375]\n",
      "2621: [discriminator loss: 0.539236, acc: 0.718750] [adversarial loss: 1.360251, acc: 0.218750]\n",
      "2622: [discriminator loss: 0.435434, acc: 0.812500] [adversarial loss: 1.325154, acc: 0.203125]\n",
      "2623: [discriminator loss: 0.469934, acc: 0.828125] [adversarial loss: 1.290596, acc: 0.250000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624: [discriminator loss: 0.530993, acc: 0.703125] [adversarial loss: 1.367797, acc: 0.187500]\n",
      "2625: [discriminator loss: 0.454961, acc: 0.765625] [adversarial loss: 1.184752, acc: 0.250000]\n",
      "2626: [discriminator loss: 0.480888, acc: 0.726562] [adversarial loss: 1.370460, acc: 0.171875]\n",
      "2627: [discriminator loss: 0.548289, acc: 0.656250] [adversarial loss: 0.998993, acc: 0.437500]\n",
      "2628: [discriminator loss: 0.543078, acc: 0.742188] [adversarial loss: 1.646841, acc: 0.093750]\n",
      "2629: [discriminator loss: 0.540461, acc: 0.687500] [adversarial loss: 0.772609, acc: 0.578125]\n",
      "2630: [discriminator loss: 0.553472, acc: 0.671875] [adversarial loss: 1.916350, acc: 0.078125]\n",
      "2631: [discriminator loss: 0.490356, acc: 0.750000] [adversarial loss: 0.973373, acc: 0.312500]\n",
      "2632: [discriminator loss: 0.485600, acc: 0.789062] [adversarial loss: 1.683022, acc: 0.093750]\n",
      "2633: [discriminator loss: 0.531804, acc: 0.742188] [adversarial loss: 1.090345, acc: 0.343750]\n",
      "2634: [discriminator loss: 0.524297, acc: 0.742188] [adversarial loss: 1.627954, acc: 0.078125]\n",
      "2635: [discriminator loss: 0.514007, acc: 0.757812] [adversarial loss: 0.936147, acc: 0.421875]\n",
      "2636: [discriminator loss: 0.480363, acc: 0.765625] [adversarial loss: 1.565139, acc: 0.093750]\n",
      "2637: [discriminator loss: 0.510133, acc: 0.765625] [adversarial loss: 1.093475, acc: 0.312500]\n",
      "2638: [discriminator loss: 0.607877, acc: 0.695312] [adversarial loss: 1.550416, acc: 0.078125]\n",
      "2639: [discriminator loss: 0.497858, acc: 0.750000] [adversarial loss: 1.257273, acc: 0.296875]\n",
      "2640: [discriminator loss: 0.545923, acc: 0.718750] [adversarial loss: 1.848331, acc: 0.093750]\n",
      "2641: [discriminator loss: 0.507589, acc: 0.750000] [adversarial loss: 1.215394, acc: 0.281250]\n",
      "2642: [discriminator loss: 0.546321, acc: 0.734375] [adversarial loss: 1.589343, acc: 0.093750]\n",
      "2643: [discriminator loss: 0.572872, acc: 0.687500] [adversarial loss: 1.134742, acc: 0.203125]\n",
      "2644: [discriminator loss: 0.530756, acc: 0.757812] [adversarial loss: 1.629637, acc: 0.156250]\n",
      "2645: [discriminator loss: 0.511254, acc: 0.773438] [adversarial loss: 1.047038, acc: 0.343750]\n",
      "2646: [discriminator loss: 0.563557, acc: 0.710938] [adversarial loss: 1.725914, acc: 0.062500]\n",
      "2647: [discriminator loss: 0.506394, acc: 0.773438] [adversarial loss: 0.913476, acc: 0.343750]\n",
      "2648: [discriminator loss: 0.508340, acc: 0.742188] [adversarial loss: 1.688967, acc: 0.062500]\n",
      "2649: [discriminator loss: 0.535453, acc: 0.742188] [adversarial loss: 0.870102, acc: 0.406250]\n",
      "2650: [discriminator loss: 0.575368, acc: 0.656250] [adversarial loss: 1.925137, acc: 0.031250]\n",
      "2651: [discriminator loss: 0.525040, acc: 0.742188] [adversarial loss: 1.047318, acc: 0.312500]\n",
      "2652: [discriminator loss: 0.535773, acc: 0.703125] [adversarial loss: 1.617058, acc: 0.109375]\n",
      "2653: [discriminator loss: 0.516080, acc: 0.781250] [adversarial loss: 1.303486, acc: 0.171875]\n",
      "2654: [discriminator loss: 0.419333, acc: 0.796875] [adversarial loss: 1.265806, acc: 0.234375]\n",
      "2655: [discriminator loss: 0.469508, acc: 0.789062] [adversarial loss: 1.235445, acc: 0.203125]\n",
      "2656: [discriminator loss: 0.535306, acc: 0.710938] [adversarial loss: 1.078515, acc: 0.359375]\n",
      "2657: [discriminator loss: 0.486893, acc: 0.757812] [adversarial loss: 1.886073, acc: 0.125000]\n",
      "2658: [discriminator loss: 0.547299, acc: 0.710938] [adversarial loss: 0.983151, acc: 0.359375]\n",
      "2659: [discriminator loss: 0.506537, acc: 0.781250] [adversarial loss: 1.629031, acc: 0.109375]\n",
      "2660: [discriminator loss: 0.496106, acc: 0.765625] [adversarial loss: 1.146208, acc: 0.312500]\n",
      "2661: [discriminator loss: 0.467893, acc: 0.812500] [adversarial loss: 1.653396, acc: 0.140625]\n",
      "2662: [discriminator loss: 0.503398, acc: 0.726562] [adversarial loss: 0.980998, acc: 0.421875]\n",
      "2663: [discriminator loss: 0.546199, acc: 0.726562] [adversarial loss: 1.573121, acc: 0.218750]\n",
      "2664: [discriminator loss: 0.495932, acc: 0.718750] [adversarial loss: 1.055481, acc: 0.328125]\n",
      "2665: [discriminator loss: 0.532004, acc: 0.734375] [adversarial loss: 1.581755, acc: 0.078125]\n",
      "2666: [discriminator loss: 0.531951, acc: 0.789062] [adversarial loss: 1.032314, acc: 0.312500]\n",
      "2667: [discriminator loss: 0.483722, acc: 0.796875] [adversarial loss: 1.483409, acc: 0.171875]\n",
      "2668: [discriminator loss: 0.475945, acc: 0.765625] [adversarial loss: 1.119101, acc: 0.312500]\n",
      "2669: [discriminator loss: 0.522848, acc: 0.750000] [adversarial loss: 1.600823, acc: 0.109375]\n",
      "2670: [discriminator loss: 0.563438, acc: 0.734375] [adversarial loss: 1.131172, acc: 0.234375]\n",
      "2671: [discriminator loss: 0.538766, acc: 0.718750] [adversarial loss: 1.778851, acc: 0.078125]\n",
      "2672: [discriminator loss: 0.538071, acc: 0.703125] [adversarial loss: 0.746965, acc: 0.515625]\n",
      "2673: [discriminator loss: 0.480009, acc: 0.757812] [adversarial loss: 1.740576, acc: 0.093750]\n",
      "2674: [discriminator loss: 0.533276, acc: 0.718750] [adversarial loss: 0.856376, acc: 0.437500]\n",
      "2675: [discriminator loss: 0.498049, acc: 0.726562] [adversarial loss: 1.558600, acc: 0.093750]\n",
      "2676: [discriminator loss: 0.494635, acc: 0.750000] [adversarial loss: 1.067606, acc: 0.296875]\n",
      "2677: [discriminator loss: 0.538603, acc: 0.757812] [adversarial loss: 1.688877, acc: 0.093750]\n",
      "2678: [discriminator loss: 0.569735, acc: 0.710938] [adversarial loss: 0.913750, acc: 0.406250]\n",
      "2679: [discriminator loss: 0.477927, acc: 0.750000] [adversarial loss: 1.202219, acc: 0.250000]\n",
      "2680: [discriminator loss: 0.505352, acc: 0.757812] [adversarial loss: 1.235624, acc: 0.234375]\n",
      "2681: [discriminator loss: 0.590039, acc: 0.718750] [adversarial loss: 1.113080, acc: 0.281250]\n",
      "2682: [discriminator loss: 0.498084, acc: 0.765625] [adversarial loss: 1.369370, acc: 0.140625]\n",
      "2683: [discriminator loss: 0.461691, acc: 0.773438] [adversarial loss: 1.135284, acc: 0.171875]\n",
      "2684: [discriminator loss: 0.439219, acc: 0.812500] [adversarial loss: 1.115332, acc: 0.265625]\n",
      "2685: [discriminator loss: 0.533354, acc: 0.710938] [adversarial loss: 1.627874, acc: 0.125000]\n",
      "2686: [discriminator loss: 0.447924, acc: 0.796875] [adversarial loss: 1.084410, acc: 0.375000]\n",
      "2687: [discriminator loss: 0.555959, acc: 0.679688] [adversarial loss: 1.856217, acc: 0.093750]\n",
      "2688: [discriminator loss: 0.498834, acc: 0.750000] [adversarial loss: 1.171126, acc: 0.312500]\n",
      "2689: [discriminator loss: 0.471124, acc: 0.773438] [adversarial loss: 1.347035, acc: 0.187500]\n",
      "2690: [discriminator loss: 0.542475, acc: 0.687500] [adversarial loss: 1.006154, acc: 0.359375]\n",
      "2691: [discriminator loss: 0.523484, acc: 0.757812] [adversarial loss: 1.587588, acc: 0.093750]\n",
      "2692: [discriminator loss: 0.541934, acc: 0.750000] [adversarial loss: 0.740891, acc: 0.578125]\n",
      "2693: [discriminator loss: 0.601137, acc: 0.679688] [adversarial loss: 2.035331, acc: 0.000000]\n",
      "2694: [discriminator loss: 0.522021, acc: 0.742188] [adversarial loss: 1.016225, acc: 0.406250]\n",
      "2695: [discriminator loss: 0.500763, acc: 0.820312] [adversarial loss: 1.118358, acc: 0.265625]\n",
      "2696: [discriminator loss: 0.519863, acc: 0.765625] [adversarial loss: 1.374199, acc: 0.125000]\n",
      "2697: [discriminator loss: 0.460360, acc: 0.828125] [adversarial loss: 1.292861, acc: 0.250000]\n",
      "2698: [discriminator loss: 0.501108, acc: 0.765625] [adversarial loss: 1.361352, acc: 0.109375]\n",
      "2699: [discriminator loss: 0.480922, acc: 0.757812] [adversarial loss: 1.390343, acc: 0.156250]\n",
      "2700: [discriminator loss: 0.517528, acc: 0.726562] [adversarial loss: 1.113072, acc: 0.312500]\n",
      "2701: [discriminator loss: 0.461463, acc: 0.757812] [adversarial loss: 1.339902, acc: 0.125000]\n",
      "2702: [discriminator loss: 0.482213, acc: 0.781250] [adversarial loss: 1.304215, acc: 0.203125]\n",
      "2703: [discriminator loss: 0.566442, acc: 0.726562] [adversarial loss: 1.436129, acc: 0.187500]\n",
      "2704: [discriminator loss: 0.496648, acc: 0.750000] [adversarial loss: 1.454891, acc: 0.171875]\n",
      "2705: [discriminator loss: 0.599411, acc: 0.679688] [adversarial loss: 1.086936, acc: 0.296875]\n",
      "2706: [discriminator loss: 0.574574, acc: 0.718750] [adversarial loss: 1.700232, acc: 0.125000]\n",
      "2707: [discriminator loss: 0.519866, acc: 0.742188] [adversarial loss: 0.917036, acc: 0.406250]\n",
      "2708: [discriminator loss: 0.578445, acc: 0.640625] [adversarial loss: 2.159089, acc: 0.046875]\n",
      "2709: [discriminator loss: 0.574501, acc: 0.703125] [adversarial loss: 1.127084, acc: 0.265625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2710: [discriminator loss: 0.579571, acc: 0.710938] [adversarial loss: 2.154164, acc: 0.078125]\n",
      "2711: [discriminator loss: 0.639301, acc: 0.679688] [adversarial loss: 0.898460, acc: 0.406250]\n",
      "2712: [discriminator loss: 0.573363, acc: 0.687500] [adversarial loss: 2.015397, acc: 0.062500]\n",
      "2713: [discriminator loss: 0.560326, acc: 0.695312] [adversarial loss: 0.801571, acc: 0.484375]\n",
      "2714: [discriminator loss: 0.518030, acc: 0.726562] [adversarial loss: 1.529987, acc: 0.156250]\n",
      "2715: [discriminator loss: 0.430095, acc: 0.804688] [adversarial loss: 1.031508, acc: 0.312500]\n",
      "2716: [discriminator loss: 0.450884, acc: 0.804688] [adversarial loss: 1.524089, acc: 0.156250]\n",
      "2717: [discriminator loss: 0.485280, acc: 0.804688] [adversarial loss: 1.594968, acc: 0.140625]\n",
      "2718: [discriminator loss: 0.458479, acc: 0.765625] [adversarial loss: 1.297714, acc: 0.234375]\n",
      "2719: [discriminator loss: 0.528157, acc: 0.726562] [adversarial loss: 1.341650, acc: 0.171875]\n",
      "2720: [discriminator loss: 0.522107, acc: 0.765625] [adversarial loss: 0.888405, acc: 0.390625]\n",
      "2721: [discriminator loss: 0.496624, acc: 0.781250] [adversarial loss: 1.700083, acc: 0.078125]\n",
      "2722: [discriminator loss: 0.518387, acc: 0.765625] [adversarial loss: 1.015731, acc: 0.265625]\n",
      "2723: [discriminator loss: 0.455790, acc: 0.765625] [adversarial loss: 1.721387, acc: 0.093750]\n",
      "2724: [discriminator loss: 0.514315, acc: 0.750000] [adversarial loss: 1.166712, acc: 0.265625]\n",
      "2725: [discriminator loss: 0.512362, acc: 0.742188] [adversarial loss: 1.894488, acc: 0.062500]\n",
      "2726: [discriminator loss: 0.527530, acc: 0.750000] [adversarial loss: 0.766084, acc: 0.562500]\n",
      "2727: [discriminator loss: 0.489028, acc: 0.742188] [adversarial loss: 1.653695, acc: 0.109375]\n",
      "2728: [discriminator loss: 0.565900, acc: 0.687500] [adversarial loss: 1.125119, acc: 0.296875]\n",
      "2729: [discriminator loss: 0.565963, acc: 0.757812] [adversarial loss: 1.816000, acc: 0.062500]\n",
      "2730: [discriminator loss: 0.479904, acc: 0.742188] [adversarial loss: 1.155495, acc: 0.265625]\n",
      "2731: [discriminator loss: 0.475053, acc: 0.781250] [adversarial loss: 1.543232, acc: 0.125000]\n",
      "2732: [discriminator loss: 0.513611, acc: 0.734375] [adversarial loss: 1.033722, acc: 0.312500]\n",
      "2733: [discriminator loss: 0.530604, acc: 0.726562] [adversarial loss: 1.732743, acc: 0.078125]\n",
      "2734: [discriminator loss: 0.468071, acc: 0.789062] [adversarial loss: 1.245048, acc: 0.234375]\n",
      "2735: [discriminator loss: 0.554781, acc: 0.757812] [adversarial loss: 1.840381, acc: 0.015625]\n",
      "2736: [discriminator loss: 0.512940, acc: 0.718750] [adversarial loss: 0.707252, acc: 0.453125]\n",
      "2737: [discriminator loss: 0.612536, acc: 0.664062] [adversarial loss: 1.744717, acc: 0.031250]\n",
      "2738: [discriminator loss: 0.506714, acc: 0.726562] [adversarial loss: 0.900577, acc: 0.421875]\n",
      "2739: [discriminator loss: 0.486430, acc: 0.757812] [adversarial loss: 1.669659, acc: 0.093750]\n",
      "2740: [discriminator loss: 0.531241, acc: 0.703125] [adversarial loss: 0.957437, acc: 0.328125]\n",
      "2741: [discriminator loss: 0.552305, acc: 0.710938] [adversarial loss: 1.786450, acc: 0.062500]\n",
      "2742: [discriminator loss: 0.485086, acc: 0.765625] [adversarial loss: 1.190833, acc: 0.265625]\n",
      "2743: [discriminator loss: 0.596683, acc: 0.718750] [adversarial loss: 1.553670, acc: 0.062500]\n",
      "2744: [discriminator loss: 0.490385, acc: 0.765625] [adversarial loss: 1.079128, acc: 0.281250]\n",
      "2745: [discriminator loss: 0.443137, acc: 0.796875] [adversarial loss: 1.559455, acc: 0.140625]\n",
      "2746: [discriminator loss: 0.510644, acc: 0.750000] [adversarial loss: 1.277026, acc: 0.234375]\n",
      "2747: [discriminator loss: 0.475605, acc: 0.710938] [adversarial loss: 1.619278, acc: 0.109375]\n",
      "2748: [discriminator loss: 0.402289, acc: 0.812500] [adversarial loss: 1.089368, acc: 0.312500]\n",
      "2749: [discriminator loss: 0.499606, acc: 0.742188] [adversarial loss: 1.667224, acc: 0.125000]\n",
      "2750: [discriminator loss: 0.506646, acc: 0.718750] [adversarial loss: 1.052059, acc: 0.312500]\n",
      "2751: [discriminator loss: 0.538861, acc: 0.679688] [adversarial loss: 1.554985, acc: 0.156250]\n",
      "2752: [discriminator loss: 0.539861, acc: 0.750000] [adversarial loss: 0.952379, acc: 0.390625]\n",
      "2753: [discriminator loss: 0.558275, acc: 0.726562] [adversarial loss: 1.611036, acc: 0.015625]\n",
      "2754: [discriminator loss: 0.477320, acc: 0.796875] [adversarial loss: 1.072920, acc: 0.250000]\n",
      "2755: [discriminator loss: 0.593187, acc: 0.687500] [adversarial loss: 1.690162, acc: 0.109375]\n",
      "2756: [discriminator loss: 0.474223, acc: 0.757812] [adversarial loss: 1.130968, acc: 0.281250]\n",
      "2757: [discriminator loss: 0.533422, acc: 0.710938] [adversarial loss: 1.268961, acc: 0.234375]\n",
      "2758: [discriminator loss: 0.449790, acc: 0.796875] [adversarial loss: 1.449296, acc: 0.140625]\n",
      "2759: [discriminator loss: 0.551294, acc: 0.671875] [adversarial loss: 0.755831, acc: 0.562500]\n",
      "2760: [discriminator loss: 0.619289, acc: 0.671875] [adversarial loss: 2.334155, acc: 0.000000]\n",
      "2761: [discriminator loss: 0.618302, acc: 0.695312] [adversarial loss: 0.924928, acc: 0.421875]\n",
      "2762: [discriminator loss: 0.539928, acc: 0.726562] [adversarial loss: 1.456597, acc: 0.187500]\n",
      "2763: [discriminator loss: 0.496589, acc: 0.710938] [adversarial loss: 1.222940, acc: 0.265625]\n",
      "2764: [discriminator loss: 0.517716, acc: 0.773438] [adversarial loss: 1.441743, acc: 0.171875]\n",
      "2765: [discriminator loss: 0.463873, acc: 0.765625] [adversarial loss: 1.327017, acc: 0.156250]\n",
      "2766: [discriminator loss: 0.543937, acc: 0.742188] [adversarial loss: 1.479306, acc: 0.125000]\n",
      "2767: [discriminator loss: 0.466179, acc: 0.789062] [adversarial loss: 1.039598, acc: 0.375000]\n",
      "2768: [discriminator loss: 0.470700, acc: 0.789062] [adversarial loss: 1.372656, acc: 0.187500]\n",
      "2769: [discriminator loss: 0.480731, acc: 0.820312] [adversarial loss: 0.902786, acc: 0.453125]\n",
      "2770: [discriminator loss: 0.484446, acc: 0.750000] [adversarial loss: 1.564962, acc: 0.140625]\n",
      "2771: [discriminator loss: 0.516136, acc: 0.742188] [adversarial loss: 0.897396, acc: 0.390625]\n",
      "2772: [discriminator loss: 0.510847, acc: 0.781250] [adversarial loss: 1.666390, acc: 0.031250]\n",
      "2773: [discriminator loss: 0.523524, acc: 0.750000] [adversarial loss: 1.091127, acc: 0.281250]\n",
      "2774: [discriminator loss: 0.511457, acc: 0.750000] [adversarial loss: 1.357054, acc: 0.187500]\n",
      "2775: [discriminator loss: 0.486272, acc: 0.781250] [adversarial loss: 1.196758, acc: 0.250000]\n",
      "2776: [discriminator loss: 0.522286, acc: 0.726562] [adversarial loss: 1.678753, acc: 0.078125]\n",
      "2777: [discriminator loss: 0.545792, acc: 0.726562] [adversarial loss: 0.996124, acc: 0.343750]\n",
      "2778: [discriminator loss: 0.512593, acc: 0.781250] [adversarial loss: 1.544947, acc: 0.187500]\n",
      "2779: [discriminator loss: 0.448308, acc: 0.781250] [adversarial loss: 0.993112, acc: 0.343750]\n",
      "2780: [discriminator loss: 0.520638, acc: 0.695312] [adversarial loss: 1.659825, acc: 0.046875]\n",
      "2781: [discriminator loss: 0.618709, acc: 0.687500] [adversarial loss: 0.799717, acc: 0.546875]\n",
      "2782: [discriminator loss: 0.594892, acc: 0.703125] [adversarial loss: 2.180988, acc: 0.031250]\n",
      "2783: [discriminator loss: 0.644530, acc: 0.656250] [adversarial loss: 0.819704, acc: 0.406250]\n",
      "2784: [discriminator loss: 0.595835, acc: 0.695312] [adversarial loss: 1.795451, acc: 0.062500]\n",
      "2785: [discriminator loss: 0.484114, acc: 0.765625] [adversarial loss: 1.025058, acc: 0.281250]\n",
      "2786: [discriminator loss: 0.485378, acc: 0.765625] [adversarial loss: 1.539723, acc: 0.093750]\n",
      "2787: [discriminator loss: 0.497316, acc: 0.757812] [adversarial loss: 1.114304, acc: 0.250000]\n",
      "2788: [discriminator loss: 0.469464, acc: 0.781250] [adversarial loss: 1.396688, acc: 0.218750]\n",
      "2789: [discriminator loss: 0.458606, acc: 0.757812] [adversarial loss: 1.076041, acc: 0.281250]\n",
      "2790: [discriminator loss: 0.504473, acc: 0.765625] [adversarial loss: 1.184092, acc: 0.218750]\n",
      "2791: [discriminator loss: 0.473449, acc: 0.812500] [adversarial loss: 1.462202, acc: 0.109375]\n",
      "2792: [discriminator loss: 0.503466, acc: 0.765625] [adversarial loss: 1.274906, acc: 0.171875]\n",
      "2793: [discriminator loss: 0.564829, acc: 0.710938] [adversarial loss: 1.205943, acc: 0.218750]\n",
      "2794: [discriminator loss: 0.515126, acc: 0.789062] [adversarial loss: 1.582930, acc: 0.093750]\n",
      "2795: [discriminator loss: 0.461026, acc: 0.757812] [adversarial loss: 1.072927, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2796: [discriminator loss: 0.472271, acc: 0.773438] [adversarial loss: 1.980070, acc: 0.062500]\n",
      "2797: [discriminator loss: 0.582002, acc: 0.695312] [adversarial loss: 0.742791, acc: 0.500000]\n",
      "2798: [discriminator loss: 0.526696, acc: 0.773438] [adversarial loss: 1.654326, acc: 0.093750]\n",
      "2799: [discriminator loss: 0.607757, acc: 0.656250] [adversarial loss: 0.645345, acc: 0.640625]\n",
      "2800: [discriminator loss: 0.521531, acc: 0.726562] [adversarial loss: 1.818764, acc: 0.109375]\n",
      "2801: [discriminator loss: 0.502896, acc: 0.718750] [adversarial loss: 1.303283, acc: 0.156250]\n",
      "2802: [discriminator loss: 0.480268, acc: 0.789062] [adversarial loss: 1.597070, acc: 0.093750]\n",
      "2803: [discriminator loss: 0.495043, acc: 0.742188] [adversarial loss: 0.865851, acc: 0.453125]\n",
      "2804: [discriminator loss: 0.612460, acc: 0.718750] [adversarial loss: 1.939828, acc: 0.062500]\n",
      "2805: [discriminator loss: 0.634050, acc: 0.640625] [adversarial loss: 0.918396, acc: 0.437500]\n",
      "2806: [discriminator loss: 0.606615, acc: 0.679688] [adversarial loss: 1.685849, acc: 0.109375]\n",
      "2807: [discriminator loss: 0.514626, acc: 0.703125] [adversarial loss: 0.906343, acc: 0.421875]\n",
      "2808: [discriminator loss: 0.552461, acc: 0.726562] [adversarial loss: 1.538136, acc: 0.125000]\n",
      "2809: [discriminator loss: 0.560936, acc: 0.765625] [adversarial loss: 1.067326, acc: 0.250000]\n",
      "2810: [discriminator loss: 0.505068, acc: 0.750000] [adversarial loss: 1.208651, acc: 0.250000]\n",
      "2811: [discriminator loss: 0.509745, acc: 0.773438] [adversarial loss: 1.357248, acc: 0.140625]\n",
      "2812: [discriminator loss: 0.497681, acc: 0.734375] [adversarial loss: 1.070096, acc: 0.296875]\n",
      "2813: [discriminator loss: 0.515151, acc: 0.765625] [adversarial loss: 1.335825, acc: 0.265625]\n",
      "2814: [discriminator loss: 0.550556, acc: 0.679688] [adversarial loss: 1.469578, acc: 0.156250]\n",
      "2815: [discriminator loss: 0.527835, acc: 0.734375] [adversarial loss: 1.020586, acc: 0.265625]\n",
      "2816: [discriminator loss: 0.530687, acc: 0.742188] [adversarial loss: 1.457844, acc: 0.140625]\n",
      "2817: [discriminator loss: 0.551806, acc: 0.734375] [adversarial loss: 0.906015, acc: 0.359375]\n",
      "2818: [discriminator loss: 0.518501, acc: 0.734375] [adversarial loss: 1.342443, acc: 0.171875]\n",
      "2819: [discriminator loss: 0.488406, acc: 0.765625] [adversarial loss: 1.261414, acc: 0.203125]\n",
      "2820: [discriminator loss: 0.473636, acc: 0.750000] [adversarial loss: 0.912863, acc: 0.406250]\n",
      "2821: [discriminator loss: 0.480296, acc: 0.773438] [adversarial loss: 1.333925, acc: 0.203125]\n",
      "2822: [discriminator loss: 0.502710, acc: 0.734375] [adversarial loss: 0.844810, acc: 0.437500]\n",
      "2823: [discriminator loss: 0.556151, acc: 0.734375] [adversarial loss: 1.609215, acc: 0.125000]\n",
      "2824: [discriminator loss: 0.585840, acc: 0.726562] [adversarial loss: 1.049901, acc: 0.250000]\n",
      "2825: [discriminator loss: 0.554302, acc: 0.710938] [adversarial loss: 1.865858, acc: 0.078125]\n",
      "2826: [discriminator loss: 0.518299, acc: 0.726562] [adversarial loss: 1.132654, acc: 0.265625]\n",
      "2827: [discriminator loss: 0.416358, acc: 0.773438] [adversarial loss: 1.431775, acc: 0.140625]\n",
      "2828: [discriminator loss: 0.474657, acc: 0.726562] [adversarial loss: 0.932623, acc: 0.359375]\n",
      "2829: [discriminator loss: 0.520846, acc: 0.718750] [adversarial loss: 1.459870, acc: 0.140625]\n",
      "2830: [discriminator loss: 0.495312, acc: 0.781250] [adversarial loss: 0.965974, acc: 0.296875]\n",
      "2831: [discriminator loss: 0.471096, acc: 0.796875] [adversarial loss: 1.490119, acc: 0.171875]\n",
      "2832: [discriminator loss: 0.495836, acc: 0.804688] [adversarial loss: 1.233679, acc: 0.187500]\n",
      "2833: [discriminator loss: 0.500231, acc: 0.726562] [adversarial loss: 1.794113, acc: 0.078125]\n",
      "2834: [discriminator loss: 0.547207, acc: 0.710938] [adversarial loss: 0.801199, acc: 0.515625]\n",
      "2835: [discriminator loss: 0.581578, acc: 0.703125] [adversarial loss: 1.978380, acc: 0.078125]\n",
      "2836: [discriminator loss: 0.602547, acc: 0.679688] [adversarial loss: 0.815827, acc: 0.453125]\n",
      "2837: [discriminator loss: 0.539923, acc: 0.671875] [adversarial loss: 1.567410, acc: 0.093750]\n",
      "2838: [discriminator loss: 0.514728, acc: 0.773438] [adversarial loss: 1.223679, acc: 0.234375]\n",
      "2839: [discriminator loss: 0.564840, acc: 0.718750] [adversarial loss: 1.230770, acc: 0.250000]\n",
      "2840: [discriminator loss: 0.455159, acc: 0.804688] [adversarial loss: 1.044146, acc: 0.343750]\n",
      "2841: [discriminator loss: 0.567945, acc: 0.679688] [adversarial loss: 1.646262, acc: 0.171875]\n",
      "2842: [discriminator loss: 0.488343, acc: 0.765625] [adversarial loss: 1.216253, acc: 0.265625]\n",
      "2843: [discriminator loss: 0.450829, acc: 0.796875] [adversarial loss: 1.419353, acc: 0.156250]\n",
      "2844: [discriminator loss: 0.482468, acc: 0.757812] [adversarial loss: 1.295668, acc: 0.265625]\n",
      "2845: [discriminator loss: 0.532307, acc: 0.718750] [adversarial loss: 1.749368, acc: 0.078125]\n",
      "2846: [discriminator loss: 0.507601, acc: 0.734375] [adversarial loss: 0.875391, acc: 0.390625]\n",
      "2847: [discriminator loss: 0.458401, acc: 0.820312] [adversarial loss: 1.370012, acc: 0.234375]\n",
      "2848: [discriminator loss: 0.460762, acc: 0.812500] [adversarial loss: 1.309726, acc: 0.187500]\n",
      "2849: [discriminator loss: 0.515692, acc: 0.796875] [adversarial loss: 1.255244, acc: 0.265625]\n",
      "2850: [discriminator loss: 0.480937, acc: 0.789062] [adversarial loss: 1.431258, acc: 0.140625]\n",
      "2851: [discriminator loss: 0.520863, acc: 0.734375] [adversarial loss: 1.132558, acc: 0.328125]\n",
      "2852: [discriminator loss: 0.470282, acc: 0.757812] [adversarial loss: 1.493495, acc: 0.109375]\n",
      "2853: [discriminator loss: 0.552607, acc: 0.718750] [adversarial loss: 0.840270, acc: 0.484375]\n",
      "2854: [discriminator loss: 0.506003, acc: 0.750000] [adversarial loss: 1.690898, acc: 0.093750]\n",
      "2855: [discriminator loss: 0.530084, acc: 0.765625] [adversarial loss: 1.097919, acc: 0.296875]\n",
      "2856: [discriminator loss: 0.591434, acc: 0.695312] [adversarial loss: 1.617447, acc: 0.140625]\n",
      "2857: [discriminator loss: 0.638544, acc: 0.656250] [adversarial loss: 1.186070, acc: 0.312500]\n",
      "2858: [discriminator loss: 0.502578, acc: 0.796875] [adversarial loss: 1.412618, acc: 0.156250]\n",
      "2859: [discriminator loss: 0.542447, acc: 0.765625] [adversarial loss: 1.163577, acc: 0.265625]\n",
      "2860: [discriminator loss: 0.491192, acc: 0.781250] [adversarial loss: 1.790174, acc: 0.078125]\n",
      "2861: [discriminator loss: 0.604207, acc: 0.757812] [adversarial loss: 0.839059, acc: 0.484375]\n",
      "2862: [discriminator loss: 0.661407, acc: 0.664062] [adversarial loss: 1.722136, acc: 0.078125]\n",
      "2863: [discriminator loss: 0.536101, acc: 0.726562] [adversarial loss: 0.958912, acc: 0.406250]\n",
      "2864: [discriminator loss: 0.484867, acc: 0.750000] [adversarial loss: 1.451318, acc: 0.171875]\n",
      "2865: [discriminator loss: 0.611142, acc: 0.703125] [adversarial loss: 1.076490, acc: 0.250000]\n",
      "2866: [discriminator loss: 0.514602, acc: 0.765625] [adversarial loss: 1.643022, acc: 0.125000]\n",
      "2867: [discriminator loss: 0.517275, acc: 0.773438] [adversarial loss: 1.247218, acc: 0.218750]\n",
      "2868: [discriminator loss: 0.513529, acc: 0.734375] [adversarial loss: 1.313005, acc: 0.218750]\n",
      "2869: [discriminator loss: 0.448118, acc: 0.757812] [adversarial loss: 1.294396, acc: 0.234375]\n",
      "2870: [discriminator loss: 0.576198, acc: 0.718750] [adversarial loss: 1.447087, acc: 0.203125]\n",
      "2871: [discriminator loss: 0.538914, acc: 0.710938] [adversarial loss: 1.016919, acc: 0.359375]\n",
      "2872: [discriminator loss: 0.561492, acc: 0.695312] [adversarial loss: 1.595698, acc: 0.093750]\n",
      "2873: [discriminator loss: 0.498213, acc: 0.781250] [adversarial loss: 0.936973, acc: 0.359375]\n",
      "2874: [discriminator loss: 0.617896, acc: 0.695312] [adversarial loss: 1.432182, acc: 0.171875]\n",
      "2875: [discriminator loss: 0.488101, acc: 0.734375] [adversarial loss: 1.117508, acc: 0.171875]\n",
      "2876: [discriminator loss: 0.482991, acc: 0.773438] [adversarial loss: 1.583668, acc: 0.093750]\n",
      "2877: [discriminator loss: 0.546813, acc: 0.703125] [adversarial loss: 0.919258, acc: 0.421875]\n",
      "2878: [discriminator loss: 0.614973, acc: 0.671875] [adversarial loss: 1.516954, acc: 0.187500]\n",
      "2879: [discriminator loss: 0.517868, acc: 0.765625] [adversarial loss: 1.095142, acc: 0.265625]\n",
      "2880: [discriminator loss: 0.450367, acc: 0.773438] [adversarial loss: 1.460226, acc: 0.187500]\n",
      "2881: [discriminator loss: 0.505163, acc: 0.773438] [adversarial loss: 1.355201, acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882: [discriminator loss: 0.464991, acc: 0.781250] [adversarial loss: 1.215221, acc: 0.265625]\n",
      "2883: [discriminator loss: 0.554342, acc: 0.718750] [adversarial loss: 1.574167, acc: 0.093750]\n",
      "2884: [discriminator loss: 0.496056, acc: 0.757812] [adversarial loss: 1.315584, acc: 0.218750]\n",
      "2885: [discriminator loss: 0.523925, acc: 0.781250] [adversarial loss: 1.462627, acc: 0.109375]\n",
      "2886: [discriminator loss: 0.476715, acc: 0.757812] [adversarial loss: 1.064261, acc: 0.281250]\n",
      "2887: [discriminator loss: 0.449586, acc: 0.812500] [adversarial loss: 1.431614, acc: 0.187500]\n",
      "2888: [discriminator loss: 0.521778, acc: 0.718750] [adversarial loss: 1.158172, acc: 0.312500]\n",
      "2889: [discriminator loss: 0.467934, acc: 0.773438] [adversarial loss: 1.318321, acc: 0.218750]\n",
      "2890: [discriminator loss: 0.575391, acc: 0.679688] [adversarial loss: 1.245955, acc: 0.187500]\n",
      "2891: [discriminator loss: 0.501902, acc: 0.773438] [adversarial loss: 1.412435, acc: 0.078125]\n",
      "2892: [discriminator loss: 0.574757, acc: 0.703125] [adversarial loss: 1.486026, acc: 0.156250]\n",
      "2893: [discriminator loss: 0.521895, acc: 0.742188] [adversarial loss: 1.517108, acc: 0.078125]\n",
      "2894: [discriminator loss: 0.532875, acc: 0.750000] [adversarial loss: 1.181588, acc: 0.296875]\n",
      "2895: [discriminator loss: 0.481701, acc: 0.781250] [adversarial loss: 1.513533, acc: 0.156250]\n",
      "2896: [discriminator loss: 0.474962, acc: 0.734375] [adversarial loss: 0.773457, acc: 0.562500]\n",
      "2897: [discriminator loss: 0.650527, acc: 0.632812] [adversarial loss: 1.779509, acc: 0.093750]\n",
      "2898: [discriminator loss: 0.523417, acc: 0.757812] [adversarial loss: 1.083534, acc: 0.437500]\n",
      "2899: [discriminator loss: 0.489973, acc: 0.765625] [adversarial loss: 1.466156, acc: 0.156250]\n",
      "2900: [discriminator loss: 0.487583, acc: 0.742188] [adversarial loss: 1.036789, acc: 0.281250]\n",
      "2901: [discriminator loss: 0.446503, acc: 0.804688] [adversarial loss: 1.490031, acc: 0.156250]\n",
      "2902: [discriminator loss: 0.469306, acc: 0.750000] [adversarial loss: 1.263448, acc: 0.250000]\n",
      "2903: [discriminator loss: 0.509000, acc: 0.757812] [adversarial loss: 1.718398, acc: 0.078125]\n",
      "2904: [discriminator loss: 0.534559, acc: 0.710938] [adversarial loss: 0.820383, acc: 0.406250]\n",
      "2905: [discriminator loss: 0.581555, acc: 0.679688] [adversarial loss: 1.698283, acc: 0.093750]\n",
      "2906: [discriminator loss: 0.501216, acc: 0.718750] [adversarial loss: 1.118498, acc: 0.328125]\n",
      "2907: [discriminator loss: 0.463743, acc: 0.835938] [adversarial loss: 1.820131, acc: 0.078125]\n",
      "2908: [discriminator loss: 0.567885, acc: 0.703125] [adversarial loss: 0.909384, acc: 0.437500]\n",
      "2909: [discriminator loss: 0.539362, acc: 0.695312] [adversarial loss: 1.715161, acc: 0.062500]\n",
      "2910: [discriminator loss: 0.496292, acc: 0.757812] [adversarial loss: 0.942648, acc: 0.406250]\n",
      "2911: [discriminator loss: 0.538905, acc: 0.703125] [adversarial loss: 1.571627, acc: 0.062500]\n",
      "2912: [discriminator loss: 0.542864, acc: 0.726562] [adversarial loss: 0.864188, acc: 0.453125]\n",
      "2913: [discriminator loss: 0.560738, acc: 0.664062] [adversarial loss: 1.955812, acc: 0.078125]\n",
      "2914: [discriminator loss: 0.561620, acc: 0.742188] [adversarial loss: 0.934642, acc: 0.359375]\n",
      "2915: [discriminator loss: 0.508878, acc: 0.703125] [adversarial loss: 1.710791, acc: 0.078125]\n",
      "2916: [discriminator loss: 0.485436, acc: 0.773438] [adversarial loss: 1.079704, acc: 0.250000]\n",
      "2917: [discriminator loss: 0.437419, acc: 0.820312] [adversarial loss: 1.347423, acc: 0.171875]\n",
      "2918: [discriminator loss: 0.493019, acc: 0.718750] [adversarial loss: 1.075134, acc: 0.328125]\n",
      "2919: [discriminator loss: 0.536821, acc: 0.742188] [adversarial loss: 1.384895, acc: 0.156250]\n",
      "2920: [discriminator loss: 0.503939, acc: 0.734375] [adversarial loss: 1.124970, acc: 0.312500]\n",
      "2921: [discriminator loss: 0.528992, acc: 0.742188] [adversarial loss: 1.942101, acc: 0.078125]\n",
      "2922: [discriminator loss: 0.521854, acc: 0.750000] [adversarial loss: 0.863579, acc: 0.453125]\n",
      "2923: [discriminator loss: 0.527439, acc: 0.718750] [adversarial loss: 1.899648, acc: 0.140625]\n",
      "2924: [discriminator loss: 0.574253, acc: 0.703125] [adversarial loss: 1.004858, acc: 0.437500]\n",
      "2925: [discriminator loss: 0.438227, acc: 0.804688] [adversarial loss: 1.474841, acc: 0.203125]\n",
      "2926: [discriminator loss: 0.521020, acc: 0.757812] [adversarial loss: 1.452520, acc: 0.140625]\n",
      "2927: [discriminator loss: 0.535885, acc: 0.757812] [adversarial loss: 1.166420, acc: 0.343750]\n",
      "2928: [discriminator loss: 0.458760, acc: 0.781250] [adversarial loss: 1.216353, acc: 0.281250]\n",
      "2929: [discriminator loss: 0.437325, acc: 0.812500] [adversarial loss: 1.160226, acc: 0.296875]\n",
      "2930: [discriminator loss: 0.464459, acc: 0.843750] [adversarial loss: 1.517384, acc: 0.156250]\n",
      "2931: [discriminator loss: 0.614859, acc: 0.671875] [adversarial loss: 1.352590, acc: 0.203125]\n",
      "2932: [discriminator loss: 0.549892, acc: 0.718750] [adversarial loss: 1.115384, acc: 0.312500]\n",
      "2933: [discriminator loss: 0.531788, acc: 0.757812] [adversarial loss: 1.775095, acc: 0.125000]\n",
      "2934: [discriminator loss: 0.489367, acc: 0.757812] [adversarial loss: 0.926708, acc: 0.453125]\n",
      "2935: [discriminator loss: 0.571891, acc: 0.687500] [adversarial loss: 1.779279, acc: 0.062500]\n",
      "2936: [discriminator loss: 0.529631, acc: 0.734375] [adversarial loss: 1.027992, acc: 0.390625]\n",
      "2937: [discriminator loss: 0.500473, acc: 0.812500] [adversarial loss: 1.377625, acc: 0.109375]\n",
      "2938: [discriminator loss: 0.428775, acc: 0.828125] [adversarial loss: 1.009849, acc: 0.359375]\n",
      "2939: [discriminator loss: 0.481341, acc: 0.781250] [adversarial loss: 1.719912, acc: 0.078125]\n",
      "2940: [discriminator loss: 0.487859, acc: 0.734375] [adversarial loss: 0.734616, acc: 0.625000]\n",
      "2941: [discriminator loss: 0.590082, acc: 0.687500] [adversarial loss: 1.843755, acc: 0.062500]\n",
      "2942: [discriminator loss: 0.589208, acc: 0.679688] [adversarial loss: 0.957964, acc: 0.359375]\n",
      "2943: [discriminator loss: 0.559295, acc: 0.718750] [adversarial loss: 1.403182, acc: 0.171875]\n",
      "2944: [discriminator loss: 0.458840, acc: 0.773438] [adversarial loss: 1.335095, acc: 0.156250]\n",
      "2945: [discriminator loss: 0.470125, acc: 0.750000] [adversarial loss: 1.381791, acc: 0.140625]\n",
      "2946: [discriminator loss: 0.442737, acc: 0.781250] [adversarial loss: 1.338581, acc: 0.156250]\n",
      "2947: [discriminator loss: 0.492207, acc: 0.757812] [adversarial loss: 1.370665, acc: 0.203125]\n",
      "2948: [discriminator loss: 0.501946, acc: 0.742188] [adversarial loss: 1.214714, acc: 0.281250]\n",
      "2949: [discriminator loss: 0.506924, acc: 0.750000] [adversarial loss: 1.669484, acc: 0.093750]\n",
      "2950: [discriminator loss: 0.454579, acc: 0.781250] [adversarial loss: 1.134971, acc: 0.312500]\n",
      "2951: [discriminator loss: 0.590116, acc: 0.695312] [adversarial loss: 1.720136, acc: 0.187500]\n",
      "2952: [discriminator loss: 0.597244, acc: 0.703125] [adversarial loss: 0.744532, acc: 0.562500]\n",
      "2953: [discriminator loss: 0.550855, acc: 0.695312] [adversarial loss: 1.566636, acc: 0.078125]\n",
      "2954: [discriminator loss: 0.477760, acc: 0.773438] [adversarial loss: 1.110681, acc: 0.312500]\n",
      "2955: [discriminator loss: 0.550135, acc: 0.726562] [adversarial loss: 1.700224, acc: 0.062500]\n",
      "2956: [discriminator loss: 0.467923, acc: 0.781250] [adversarial loss: 1.224888, acc: 0.265625]\n",
      "2957: [discriminator loss: 0.504501, acc: 0.710938] [adversarial loss: 1.384362, acc: 0.250000]\n",
      "2958: [discriminator loss: 0.512555, acc: 0.734375] [adversarial loss: 1.148129, acc: 0.296875]\n",
      "2959: [discriminator loss: 0.543191, acc: 0.718750] [adversarial loss: 1.612389, acc: 0.109375]\n",
      "2960: [discriminator loss: 0.490673, acc: 0.765625] [adversarial loss: 1.009777, acc: 0.359375]\n",
      "2961: [discriminator loss: 0.543526, acc: 0.750000] [adversarial loss: 1.651702, acc: 0.093750]\n",
      "2962: [discriminator loss: 0.503304, acc: 0.765625] [adversarial loss: 1.054919, acc: 0.375000]\n",
      "2963: [discriminator loss: 0.577264, acc: 0.664062] [adversarial loss: 1.726864, acc: 0.031250]\n",
      "2964: [discriminator loss: 0.542094, acc: 0.750000] [adversarial loss: 1.283744, acc: 0.265625]\n",
      "2965: [discriminator loss: 0.424198, acc: 0.843750] [adversarial loss: 1.546782, acc: 0.187500]\n",
      "2966: [discriminator loss: 0.480713, acc: 0.726562] [adversarial loss: 1.073783, acc: 0.359375]\n",
      "2967: [discriminator loss: 0.450202, acc: 0.804688] [adversarial loss: 1.532058, acc: 0.109375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2968: [discriminator loss: 0.486417, acc: 0.757812] [adversarial loss: 1.070711, acc: 0.390625]\n",
      "2969: [discriminator loss: 0.509466, acc: 0.734375] [adversarial loss: 1.629635, acc: 0.109375]\n",
      "2970: [discriminator loss: 0.585558, acc: 0.664062] [adversarial loss: 1.236716, acc: 0.281250]\n",
      "2971: [discriminator loss: 0.513696, acc: 0.765625] [adversarial loss: 1.032655, acc: 0.406250]\n",
      "2972: [discriminator loss: 0.597796, acc: 0.734375] [adversarial loss: 1.373550, acc: 0.203125]\n",
      "2973: [discriminator loss: 0.593598, acc: 0.703125] [adversarial loss: 1.294225, acc: 0.234375]\n",
      "2974: [discriminator loss: 0.489693, acc: 0.781250] [adversarial loss: 1.247315, acc: 0.250000]\n",
      "2975: [discriminator loss: 0.467828, acc: 0.757812] [adversarial loss: 1.106574, acc: 0.234375]\n",
      "2976: [discriminator loss: 0.480492, acc: 0.789062] [adversarial loss: 1.185675, acc: 0.187500]\n",
      "2977: [discriminator loss: 0.474383, acc: 0.757812] [adversarial loss: 1.426274, acc: 0.171875]\n",
      "2978: [discriminator loss: 0.543875, acc: 0.750000] [adversarial loss: 1.342617, acc: 0.171875]\n",
      "2979: [discriminator loss: 0.453285, acc: 0.757812] [adversarial loss: 1.328191, acc: 0.218750]\n",
      "2980: [discriminator loss: 0.526501, acc: 0.773438] [adversarial loss: 1.452876, acc: 0.156250]\n",
      "2981: [discriminator loss: 0.504340, acc: 0.765625] [adversarial loss: 1.400217, acc: 0.218750]\n",
      "2982: [discriminator loss: 0.478146, acc: 0.765625] [adversarial loss: 1.698110, acc: 0.062500]\n",
      "2983: [discriminator loss: 0.502261, acc: 0.734375] [adversarial loss: 0.896137, acc: 0.375000]\n",
      "2984: [discriminator loss: 0.543920, acc: 0.726562] [adversarial loss: 2.214618, acc: 0.062500]\n",
      "2985: [discriminator loss: 0.631763, acc: 0.703125] [adversarial loss: 0.663947, acc: 0.593750]\n",
      "2986: [discriminator loss: 0.580436, acc: 0.695312] [adversarial loss: 1.554515, acc: 0.125000]\n",
      "2987: [discriminator loss: 0.454791, acc: 0.781250] [adversarial loss: 1.127667, acc: 0.203125]\n",
      "2988: [discriminator loss: 0.486515, acc: 0.781250] [adversarial loss: 1.358480, acc: 0.203125]\n",
      "2989: [discriminator loss: 0.537510, acc: 0.718750] [adversarial loss: 1.263309, acc: 0.109375]\n",
      "2990: [discriminator loss: 0.480830, acc: 0.773438] [adversarial loss: 1.354708, acc: 0.156250]\n",
      "2991: [discriminator loss: 0.557228, acc: 0.718750] [adversarial loss: 1.554765, acc: 0.140625]\n",
      "2992: [discriminator loss: 0.464282, acc: 0.773438] [adversarial loss: 1.302054, acc: 0.234375]\n",
      "2993: [discriminator loss: 0.463573, acc: 0.757812] [adversarial loss: 1.273941, acc: 0.187500]\n",
      "2994: [discriminator loss: 0.519095, acc: 0.710938] [adversarial loss: 1.378664, acc: 0.156250]\n",
      "2995: [discriminator loss: 0.436942, acc: 0.804688] [adversarial loss: 1.372674, acc: 0.203125]\n",
      "2996: [discriminator loss: 0.547139, acc: 0.687500] [adversarial loss: 1.220103, acc: 0.234375]\n",
      "2997: [discriminator loss: 0.527417, acc: 0.757812] [adversarial loss: 1.125402, acc: 0.250000]\n",
      "2998: [discriminator loss: 0.446234, acc: 0.804688] [adversarial loss: 1.509572, acc: 0.218750]\n",
      "2999: [discriminator loss: 0.449149, acc: 0.804688] [adversarial loss: 1.265597, acc: 0.203125]\n"
     ]
    }
   ],
   "source": [
    "models = (generator, discriminator, adversarial)\n",
    "params = (batch_size, latent_size, train_steps, model_name)\n",
    "generator, list_images = train(models, x_train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated images at different steps of learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 500 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 1500 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 2000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAADMCAYAAAACsC2UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deXhURbbAf70kZE9ISNgFRGSVMAiKG4sgi4DiME8ejAijiOIbRJ/C6AwqOgqOiAsMiuP6XEZBZATZRWFERBlZBIFBBQSEQAhbAkk66e56f1zr2B2y9Zow1u/7+jO0t++tc++tqlPnnDrHppTCYDAEh72mG2AwnMuYDmQwhIDpQAZDCJgOZDCEgOlABkMImA5kMISAs7L/abPZArZx2+1Wn/R6vUE2KXCUUraqjglGlnBjs9moym1wrshSGfod8Hg8tVoWm81qnt1ul/e1oudT0XMxM5DBEAJh70A2m016tsGftLQ0YmJiiImJoU6dOsTGxhIbG1vTzQo7Xq83qhpIsGRnZ5Odnc3MmTOx2+0ycwZCpSpcoNjtdlJTUwE4efLkOXETo8mpU6fk7/bt2/P73/8egNtuu61K1e5c4lwYQO+55x7GjRsHWO+tx+MJ6jxGhTMYQsBW2cgX6AIvPT2dt956C4BevXrJSHTixAk5prCwkOPHjwMwbtw4Nm7cCFS8eKsO0Vx4+46uwbS5b9++ACxduhSXywVAZmYmhYWF+py1euFdHRwOBwButzuqsuhnM2XKFH788UfAus8xMTEAXHfddfzXf/0XAN26dZNZ58orr+Srr76q9NwVPZewdqDmzZuzc+dOAOLi4qr1G3393/72t6xevRqAw4cPB3LZiL90devWZdGiRQA0btyY5ORkAIYOHcqnn34a0Ll2794NQIsWLUTFrVOnjjzMaHegjh07AnD06FFycnLCck69rnO5XGGXJSMjgylTpgDQr18/4uPjAWjQoIF03LLWzvIGPbfbzYQJEwB44YUXqryuscIZDBEgrDOQzWZj1apVAFx99dXV+o3b7Zbf6pHio48+on///tW+bqRH7eTkZB5++GEAbr/9dpKSkgA4cuQI3bp1A+CHH36o8jzx8fHs378fsGY1PQMlJCTIfYjmDPTee+9xww03AJCXl0fDhg11G0I6r1aZSkpKwiZL3bp1Afjuu+/IyMgo9xh9bxcvXsyRI0cA6Nq1K4MGDZJjSkpKAEhKSqK0tLQ6lwYqfi5htcIppejduzdgWZmysrIASyetU6cOAGfOnJEp3uFwyPedOnUSM6K25NUWCgoK+MMf/gBYcvXr1w+ArKws0amnT59e5XmKioro0aMHADfeeCOZmZkAeDyeoEyowaLvf/fu3eW6KSkppKWlAf5r1kCx2+1ht8LZ7XaaNWsGWO+PVqE9Ho+sI0eMGMGyZcvO+m1iYqIsCRISEuS3gXSeStsWlrMYDL9QwjoD+bJ9+3a2b98OwKeffipqz80338yFF14IWGrD0KFDAfxG4IULF0aqWUGj1Zq8vDwZYZVSXHPNNUD1ZiCAHTt2AMhCGCzZo+kH0ipWQkKCfFdaWkrjxo0By4cXaHu0JuF7/nChlOLrr78GLBVaz6DXXXcd06ZNA342zpSlRYsW8u4ppUSFCxdhXQNVeqGfXrq0tDRmzpwJWOZDp9Pqw02aNJE1wSeffCIvZnWIxrpBt3/fvn00bdpUX1fa+fHHH/sdG+gLqM/v9XojLou2XOXm5pKYmAhY67m9e/cC8Nxzz5Geng5UbKFyOp3SaVq3bk2TJk0AWLVqlcheWFgYEVn00iApKYlDhw4BUFxcXO6xLpdLOtyhQ4dkkAgUY4UzGCJAxFS4suhR6dSpU+I76devn1hUvF4v33//PWBNzbUN3X49emu++eabCo8N5vzRQBtpfGUpKSmRmXXEiBFs2rQJsPwuesHtdrt54IEHAMsh3LJlS8BSQfVsdM0115CbmxvR9uvzFxYWVjjztG/fHsAv1rBt27Zhb0vUOpBGKSUm7oyMDFn7uFwurrrqKsCyVtVWfB3EHo/HL77tXMBmszFr1iz5t+4ciYmJPPjggwD86le/Ij8/H4A+ffrw6quvApbs2uFrt9s5duwYAFu3bmXu3LkAbNq0SRyakeb06dPlfj98+HBeeeUV+feCBQsARKZwYlQ4gyEEojYD6ZmmXr16DBkyBPAPsfjnP//J0aNHo9WcoPFVexwOxzkXRX3//fdz/fXXA/5WKYfDITNHXl6e+F0mTpwo1jqv18t3330HwIcffijqXNl7EK0tGikpKWLF3blzJ+vXrz/rmPvuu89vxg03UetA9evXB2DMmDHidY+Li5O/33vvPV8PdrSaVW20tdCXoqKicyJ035eEhARR22JiYmRgi42NFRU6IyODVq1aAf6Brq+++irjx4+v8hqRfH6JiYncdNNNADzwwAOcd955wNnxbmvWrAEsq6COYtDRCeHEqHAGQwhEbQY6efIkYFmA9ELc4/Gwbds2wAo7r40zj0arKTk5OeLzKCkpkRn0XOGFF17gzjvvBKwZSKukp06dIjs7G7CsW1u3bgXg3XfflTCmmuTFF18E4He/+51oKkopmXl8DU/79++noKAAsJzyKSkpgLVMuOuuuwA4cOBAeBqmlKrwA6hwfTp06KA6dOigXC6X8mXUqFFq1KhRyuFwBH3uymQIlyzJyckqOTlZFRQUSNs9Ho/KzMxUmZmZYbtP0ZAlMTFRJSYmqpKSElVaWiqflStXqpUrV6qxY8eqmJgYFRMTUytkmThxonK73crtdvu9O263W+Xm5qrc3Fw1adIk1aVLF9WlSxd1zz33qEOHDqlDhw6V+xu3262WLFkSFlmMCmcwhEDYVThtyWnatClTp04FYMuWLTz22GOAf5yU1+sV/0Gwe9KjhVYvfdtpt9vFKtWpU6dqbWmoDZw5cwaAzp07i6oG0KFDBwAGDRoUtmjlcOBrICgqKhL/0/vvv8+7774LWO+Y3rbw+OOP+6bWkt/bbDZ5Pzt37hyWFGxhjYVLSUlh165dgGWu9m1geVasVatWBRTzVhHRiIXTYfDbt2+XNZDX6xWv+IYNG3juuecAWL9+fYUe8qqI9vZ0PTA4nU4mT54MWC9gOAiXLA6HQzrH0aNH2bx5M3C2w113jqSkJFmzulwuWedNnTqVgQMHym91tEJ1Bm8TC2cwRICQVTiHw8HFF18MWDtJdXSv1+v1y/zoix4dRo8eHerlo4LdbmfGjBkArF69WhKh3HTTTTKz7tq1S3Z35uTkSFxfbbbStW3bVtpfXFzME088UcMtKh+Px8NHH30EwODBg/nyyy8rPA44K7xKb7rbsmWL7HSeN29eWJYNQXcgHZA4aNAgSc6wf/9+GjRoAFgqjzYlpqSkSGMdDoesG3Qoem2nb9++kiylf//+LF68GLB2brZp0wawsrzoLdEJCQliKq7NbNmyRf4+fvx42Nah4d4PBIgz9+OPPw54zaIdqc8884xESejA2VAxKpzBEAJBzUB2u10cUr6xUgUFBRLWXlpaKuqB1+v1s4roBB21PY5ML0pfeOEFUcnatGnDQw89BFhprfTIWLduXYkO7tmzJ/Xq1QNq5yyrE6H4zhQ333xz2M6vZ+JwopcDbdq0kWeRm5tb5WzkdDol2UhCQoK8c0ePHpX3MxQ1O6gO5PV6ZVt2XFycvGipqanl5uCy2Wwi6MqVKyXHWm1HvwipqamiBqSlpTFy5EjAkst3wNBbh9etWyem1trIunXrzvpu7dq1YTm33W6PyBYP/S717NmTN954A4C9e/fywQcfAPDyyy+Ltc1ut3PFFVcAluXNd0u3Ns/v3btXnmkoQcxGhTMYQiBoP5COZ/vhhx8kPVNZa5tPrjNZsN5www0cPHgwtFaXIVK+E50XYN68eTLjZmVlyazje++KiorEqXfbbbcF7ZyLtB+obt26kloZft6UlpaWFjYjQiQzkyYnJ0scW0JCQrlxcWX/1hQXF4sPac6cOcybN0+3s8rrVvRcgrbCaUfh5MmT+etf/wpYKo3uRAUFBeJkvPnmm8X0W9sjDnzRL1q/fv1EX160aJFYcOrXry97TZ5++umI7HgMN08//bT8rZSS7fXhfC6RfMYFBQWSVOTYsWPyXMpuK9GD9+HDh+UZJSYmyk5Vu91erY5TFUaFMxhCIORQnvj4eJlGi4uLZWFWp04dsUBF2pkYzfCXSBMpWfQInZ6e7rfBTPvqwkk0U3QF0p7Y2FgJXQrUAlzRc4laXrhIYjpQ7eSXIItR4QyGEDAd6D8YU6828kQ9L1xNEUy63XOdmpL3l9RpzQxkMIRApUYEg8FQOWYGMhhCwHQggyEETAcyGELAdCCDIQRMBzIYQsB0IIMhBEwHMhhCoNJIhHM90M8XI0v0qSlZyksrECph31BnqBydJ0IpFVLqWEP1sNvtsms4Ly9P8jJEOkWxUeEMhhAwM1AEaNOmjcxAHo+Hf//73zXcov9M7Ha75OP44osvZKv9unXr6NWrV1TaYDpQiGh92+l0ctFFFwFW/VCdJ2Lz5s2mA4UZvQO6ffv2UnjrvPPOk2fx1FNPRU1tNiqcwRAK0apQF8lPNKq6lffp2bOnysnJUTk5OaqwsNCvEpqu+jZs2LBaIYvNZlM2m03Z7XYVFxen4uLilN1ul+/Pledis9nUnDlz1Jw5c9SZM2eU1+tVXq9Xud1utWXLFrVly5aIyFNh+2u6A4XjAdZUB7LZbGro0KFq6NCh6tChQ9JplFLq2LFj6tixY2rbtm01LktsbKxyuVxnldesCI/Ho7799lv17bffqvr16yu73a7sdnuteC6jR4+WTuP1elVxcbEqLi5WCxYsUPHx8So+Pj4i72lF7TcqnMEQAlHJyhMbG8vdd98NQH5+Pm+99RZgpb7SSdivvfZann/+eaB6mSJ9UTXofNTWtjp16rBixQrASt6uv9+3bx/nn3++bmeV54uELCNHjpR80oFSUFDAvn37ACtD65NPPgmEls3Tl+rKohMorl69miuvvFK+1znIO3XqxI8//lidUwVFRbKEtQNlZWVJetuePXv6nsfvOH3zjx07Jom/4+Pj2b17NwBdu3aVlLPVoaY6UNlUxp07dwbgyy+/lP9XWFgoRceqQyRkSUxMlBctJiZGOrLu5OW0QY7xer1i0SopKZGyiLriQWWEUxb9Dr377rtSyMxmszFkyBAAli1bJu10OBx+uQrDQUWyGBXOYAiBsPiB9Gh7yy23VMuBpZOzN2rUyO97Xe1t586dNGvWDAitgnIkuOCCCyR5+ty5cxk8eDBgJdvXvh/fmcnpdNK8eXOAGqvibbPZ5NqNGjWSnN8ZGRlSJMDtdvPZZ58Blk/lggsuAPArDn369GnGjx8PwMyZMyXJezTQM2JCQoJfmJSeaeLj4yVsx+l00qJFCwB2794dtlmoPELuQDabTeqyPPLIIyE3CKBJkyZSij0zMzMgdS5S6CJikydPFrXB7XbLQ3vqqafo2rXrWb+LjY2ldevWQM11IIfDIWrkvn37pFLGkiVL+PbbbwE4ePCgxI95vV6GDRsGwIwZM2SdmpqayuHDh4GaKxyWk5MjfyulpLK4b4riiy++WBLQr1u3Tmof5eXlhb09RoUzGEIgZCNCmzZtmD59OmBZ0sourCujpKRESoIkJSX5JQHXCen37dsnI3hF6lw0jAi/+c1vAHjjjTdk1nn22Wdl9r3rrrtENfWlqKiIxo0bA1ZR4qqIhCwNGzaUUbhOnToMGDAAOFu90e9CvXr1aNKkCQCzZ8+WvydNmsTcuXOrfd1IyBITEyMlHuvXry+lVHJycli2bBlgGTi06nn55ZdLiNX1118vxaJLS0sD2upQkSxBq3CXXHIJAMuXL5eK3WU7j37h3W43GzZsACwL3LXXXivf699edNFFjBs3DoA+ffqQlpYGQIsWLbj11lsBeOmll4JtbsjoYlu+NZDuvvtu6UBlK1Nr2YcPH16tjhNJvF6vBF3GxcXx2muvAdbL5VNJQQaq2bNn06NHD8CSV6vmgXSeSFFaWsqOHTsAS9XXzyIrK0tK2E+dOlWOGT16tHS4W2+9lVdffRXwr1AeCkaFMxhCICgVzm6388033wDQtm1b+d7r9cqItnfvXhmxXn/9db777jvgbGeitgKNHTtWKkWnpKTQqlUrOX7lypUAMsKUJdIqnM1mE1UzMTGxWrmftdoTaDnLSMjiW/g3ISFB2n/mzBmZHRcsWMDtt98OWCq0PuaTTz6hT58+gVxOiNRz0VrL4cOHxaLo8Xjk/Rk/frxoBKdOnRLjzkcffcRjjz0GWIWFA7HwhtWRarfbKSoqAn6uhwmWevbRRx8BcNNNNwVUrdnpdLJ69WoAunTpIh3L6/Vyxx13ABWrcJHuQA6HQ5y/vs5Hr9frp7bqBzJ06FCpHh0okZIlOzsbqFh18ZVFKSVe/WbNmgW9LTpSsjRo0ACAHTt2SJtPnjzJpEmTAKtzPProo4DlSNWdZvPmzeK4P3z4cFg6kFHhDIYQCMqIoJTi888/B6BHjx5iCdm6dSsTJkwACLjgrtvtpnv37oClqi1duhSwFo3a9l9TJUo8Hg9bt24FrE1cviO1DpGpW7cuy5cvB2DVqlVRb2NVfP311wDceeedzJgxA7DUZ62q2e12ubelpaUyY9XE/a4KbdDZtGmTtHPFihX89re/BeDCCy8Up7bL5eKrr76S32p54uPjxdcYCkGbsevXrw/Arl27pFGDBw8WFSEU5+fjjz/O/fffL/9+7rnnAPjf//3fco+Phhlb19aMiYkRE/v+/ft5+OGHARg1apQ4WIuLi4OuVB3NuL527drxxRdfAJYbQcvocrnEChpKB4qULNoUrSNXwBrktEp28uRJ/vCHPwDWe6qDYU+ePClrpu+//14Gv+pgVDiDIQKE7EgdNGgQ7dq1A2DAgAFif7/77rsDTik0aNAgAObPn+/nlGzYsCGAhJGUJdKjttPpLFcWt9stI/jTTz/N4sWLAUsdCnRLhiaaM1BcXBwff/wxYPmEfNoglquNGzcGff5IyDJkyBAWLFigf1vhcTq0p06dOmKR27BhA1OnTgXgs88+8zNyVaUxRHQ7gw4gnTt3rsRc5ebmMnLkSGlsVaSnp5ObmwtYli7drlWrVon5uqYiEYqKisQq6EtJSYnEkr300ktikWzXrh233HJLUNeKZgfq1KmTXwfxtShqK2tiYmKtsMLpgNzXX39dnLw/XQOwBjPdfl85fDtZcXGx7NmaPHkyb7/9NmBtP9Fuh1//+tfSmXwHTaPCGQwRICzbGbT/pnHjxpLCKT09XULf//Wvf/mpNHpK7dOnDwsXLvT7DqyZ5vHHHwfg4YcfrjFLkI6n8p19lFLMnj0bgO+++04cyaNGjZKYN7vdLiposKpcpLDZbPKMLrzwQrm3J06cEHmTk5NF5uXLl9OvX7+aaexPNG3alPfffx+Ajh07SpuPHTsmhoAVK1aIAev48eNiqXvkkUckX1xRUZH8PXjwYNF4Lr30UubPnw9YM1lAW2jCmfABUE6nUzmdTpWUlKRiYmLk06VLF9WlSxe1bdu2KpNaLFy4sMaTVwBq/vz5av78+UopK9GGx+NRDRo0kEQocXFxau3atWrt2rWqqKhI5efnq/z8fHXmzJlalYgDUA6HQzkcDrV8+XJJyKGUUoWFhaqwsFAtX75c5eXlqby8POXxeOQYl8sVdOKXcMlit9tVVlaWysrKUhMnTlSpqakqNTVV9ezZU+5zRe3r06ePyFVYWKg2b96sNm/erF588UW1e/dutXv3bvXggw+q2NhYFRsbG7AsRoUzGEIgoklFfBdz2sdQ2T58vUO1ImtbRURq4a0dbQkJCRw5cgSwQlu0oWTevHlcccUVgOWo1PF+L774YrUMJ+URCVlsNps4gjt06CAq0I8//sjVV18NQL9+/cQg8ve//1020SmluOqqqwBrc1ogRNMg8tO59HXl70svvVTCy5xOp2zfePTRR+XvuLg45syZA/xsPClLRbJENLWv1iUfffRRv86kH2B+fr5sc7juuusiuvU2GHzj/HSnuf766yV7UEZGhsjYqlUr2TMUyewwEHhERuPGjcXVAEhH6dKlizyXNWvWSMxhYmKiOIu9Xm+FL1VtQ98Tm80mAae+u1ALCgokQmb9+vVcfPHFgPUeBrtWNSqcwRACEZ2BtGVt2LBhftPrn/70JwCeeOKJWhlrpfFVN/UMpNN2abQzLjs7O+IzjybQe5adnS0zTXFxscwoI0aM4M477wSsnb/a3+ZwOPzi/Tp27AhY0cy1+XlplFISSta8eXMKCwsBaxmhVevTp0/z5ptvAv5xgIES0Q504403AkjhI7A82zo5X21/GLpDaNOnRqttx48fl+xBtVHN0ab01157Te51XFwcnTp1AmDOnDkysOmgTLCei3Yiut3uWilbVWgV9NChQ9JpBg4cKDuId+3aFZbiW0aFMxhCIGIz0B133MELL7wg/9YjYK9evYKOVI42OrdYfHy8tNnlcolq53a7a+0smpSURO/evQFYu3at7Co9deqUqNZOp1Osbb4opcQCOW/ePObNmyffn2vk5eWJ0eSSSy6R5UO4Sj+G3YytX67S0lJRD9xut5hAfVP+hotom0sjSaiy6LVLx44d2bNnD2Dp/tqNUNbLrp9RTEyMDBLhGuBqy3PR72RKSgonT54EAh8MTCycwRABwq7C9e3bF7Bs69qvM2HCBAlBN0QWPaMcOHBAQvorG231/9Mz1H8iekaNRHqxsKtwWoVITU2V6RIiqz/XFlUhHPzSZLHb7epcWFsZFc5giACVzkAGg6FyzAxkMISA6UAGQwiYDmQwhIDpQAZDCJgOZDCEgOlABkMImA5kMIRApaE8/0kebyNL9KktsuhgUq/X67ft2zfpYlWprEwkgsEQAUwHMvzH4/F48Hg8Z8VjOhwOHA6HFN0Khohu6TYYagK73S45LFwuV7mR5r7b1kPZXGdmIIMhBCI2A8XGxsoo4PV6ue222wD47//+b8m0//nnn3PXXXcBVlaY/5TA1pqqpBcK8fHx1K1bF7C2QeuceEVFRefMFnydROXIkSNiFGjZsmVE9zqFZT+Qtmb8z//8DzNnzgSsTlNRFlJf9PXT09P99g8FQrStPbo6X6dOnejWrRtg5RcYM2YMAP/4xz/kmHvvvZcffvih2ueOlCy6MG+dOnVYtGgRYGUp9U14WR6FhYU88cQTAPz5z38O6JrRfi46E4/D4ZAOFBcXF5b8B8YKZzBEglAz5zscDjV9+nQ1ffr08kstKKXcbrdUAdi8ebM6c+aMOnPmjN8xaWlpAWf/159wVQGo7mfChAlqwoQJqrCwsFx5Dx06pJYuXaqWLl2qxo8fr+Lj41V8fHxUZYmNjVW9evVSvXr1Unv37pVqC7oqQ2XoShQej0cppVRpaakqLS1V6enptfa5jB071k8G/b4FU1UiEFlCXgPFxcVJLZm33npLTILvvPOO5L0+ePCg3zTaunVrwCoWqzNI6uyR5wL33XcfgCTpK0tqaqrkWu7atSsffvghYOUpiNZ6onfv3ixZsgTwr9KmlBJVZ9u2bSLD3/72N7Zv3w5YKtC9994r59HqUEJCghTprS3omka+KdQArr32WiCyqQTAqHAGQ0iEPAO5XC7uuecewD+3WFxcnMwqesTTvPzyy4A1MuoRsOwxtZU333xT6mmWRcu+c+dOSZ9bUlLCP//5TwAuu+wyDh06FJV2tm/fXmb3xMREaduXX34pCRfLhq/oY5KSkqRCIPxcpSInJyfi7Q6Ul156CfAvpbNnzx7WrFkTleuHPSuPVhfsdrs8IN9rdOjQgX/961+A1cm++uorAKkKHQwqwtYem83Gpk2bAKRjaPRLt2zZMnnpBg4cyAMPPABY90GbUa+++mo+//zzSq8VLll8E6YnJiZKptHKnrd+CdPS0kSda9CggcjYtGnTgDpRpJ/LT9c467v4+Hi/UjlZWVmAVUVeW0fj4+OZO3cuYOXJrkrVq0gWo8IZDCEQdkeq9v0kJyf7JbJLTk4GYPHixX5Fe6dMmRLuJoSdY8eOiZOxLLrg8JNPPiny/u1vf/Pzgenit9EqfwL+6plW5ar7G4/H41ehTpdw0QV9awtlDU86lbFvsax69epJUeW0tDTRkHwLh+lqIUERbhOjLtbaqFEjlZycrJKTk9WVV16pDh8+rA4fPuxnJv3LX/4SdAFb30+kzKXa/FwWt9ut3G632rJli2rZsqVq2bKl+vOf/yzflz22adOmqmnTprXO9FvRp0+fPuro0aPq6NGjyu12q2nTpqlp06YFXDQ5UrLod8aX0tJSP3eBLj68cuXKcp+LUko1bNhQNWzYMCRZjApnMIRA2I0IWj3r37+/1Ny86qqrSEhIkGN0yE5mZmZYrG+RWqz6xoPpBbbH4/FTdXT+6czMzHLPkZqaKnU5q0M0Ft7lYbfbueyyywB4/fXXJV7x3//+N1deeSXwczW+6hIpWbSK5lvDNi4uzk910yVctm7dSsuWLf2+0+iyLcOGDavymhXJEvY1kLZ+FBcXc8011wD+Jsbi4mJ5UG63W/5fVTsCawJtPZs6daoMAH379qVDhw6A9UB813O+aGtdIJ2nJmjTpg1gdRpdiNjpdIq1bfDgwWLBqw2MHz/er+PoCtxliwTryoEHDhwQ536DBg1kbWqz2cSc73A4gnZwGxXOYAiBiG1nWLVqlYwKviEv27Ztk5qjl156KR988AFgjdSRDrsIlgcffNBvFm3bti1AhdHm27Zt4+uvv45K26qLDnlxOp2isuTl5bF48WLA35+ybt067r77bgAOHz5cK5zcusLejBkz5DuPx8MNN9xw1rHa4gvwzTffSJjP5Zdfzu233w5YvjF93B133CHW1EAJ+xpI43Q62bt3L2A5svRDyM3N5ZtvvgEsb/n7778PwLRp04KOs4rmuqF+/fqi3vjGmMHPamiLFi3Yv39/UOePlCydO3cGYPXq1aLSlN3KoM3Ca9asYeTIkXKMXueVlJQENMiFKotuX0FBgd8aWrdh586dEvPmdru57rrrAKtD7Nu3D4AePZow3IIAABB1SURBVHrwyiuvyPe+g7k+z2effUb37t2DksWocAZDCERsBho5ciTPP/88YKk6b7/9NmCFg2in5HnnnUdaWhoAGzZs4Ne//jUQuMMuGjOQNhasWbOGSy+9tLw2yGL72Wef5cEHHwzqOpGSRauUbdq0EWuUx+MR1c7r9YrDcf/+/axatUp+l5eXB1gWOd8QmaoIVRY9U+bn558122u0ZnP8+HF5l7RMPtfw++9PbRMHc7du3dixY0el7YyaFU43csqUKfKgjh8/LlaRgQMHioB//OMfmThxImCZuufPnw9YlbxrEzabTaxVZWPhNL45x7TKU1uIiYmRQWvVqlVcdNFFgFXyULd5ypQprF+/HoB+/foxaNAgAMaOHStRCX//+99lC3401qv6BR8yZIi8J926dfPr9HodmpaW5medq4rS0lLeeustAFlqBINR4QyGEAi7CqejXXNycmRRnZ+fT6NGjQD8VIDY2FhZbNevX19GtXbt2ok6UR0ipfZoZ+I777zj5/vRs6zT6fRz6j311FOAZRAJ1OmoCacsup179uyRiOSNGzeKerZv3z4J+z948KD8Lj09nWXLlgFw4YUXygI+Pz9f7kl15IvUc9FyZWRkiJrXvn175syZA1hGq4pmowMHDgDW1hJfmasiairc1KlT9QVlF+qwYcPK1Z1LS0t58803AWuXp74xn3/+Oenp6eFuWkAkJiaybt06AFJSUti8eTNgRVFccMEFAJx//vmiThQWFkpgbCDrhEii1eYGDRpIO1u3bk1ubi4ACxculAHMd33QtGlTWTNdcMEFopLGx8f7WfNqCj3Q5uXlyfps3759kvRk0qRJ4iqJiYmRoNG3335bomPC5bg3KpzBEAJhn4EGDBgAWCPa4cOHAfjqq6/Epu8ThYvT6ZT8CL7ExcXJ4jDaOcn01P9///d/ovbYbDYZsVq3bi1qjG+C8u7du9eamQes2LxPPvkE+DlfGli+lVmzZgH+G8l8Zbn22mu5/vrrAcsSpo/xer288cYbgOVXOnr0aHSEqSaDBw8GLNn1+3bmzBmeeeYZwDKUhDtkLOwdSFs0GjRoIOue3//+93Kzi4qKJH5p0KBB9OzZE/CfUk+cOFFjsXHaO33RRRfJQygpKZGOnpWV5Re/p3Os1bbIgxtvvFHUYF/1LCMjQ5yk69evF0tpo0aNRO257777/Lz5vutknV+uOjn/oonD4eCKK64ArE6vB95Fixbx7LPPApGJtzQqnMEQAmG3wukF9o4dO2R081XbyjqzfEdzrQKNGDGChQsXVvua4bT2aBXuscceY9y4cYClUupFuK8s//jHP/jNb35T7XZWh3DJUrduXbGkde3aVe6z2+0Wh++uXbska2r//v3LDfHxer0cOXIEsIw7OvTqnXfeiZos1WXbtm2AZQTRju9OnToFZNGtiAplidTOxwEDBqiSkhJVUlJy1k5AjdfrVQUFBaqgoEDl5+er5cuXq+XLl6tu3brV2M5Hvdtx586dfskFfZkzZ46aM2dO0PcmWrLoT+vWrdX+/fvV/v37lcfjqTLBotfrVUuWLFFLlixRHTt2VA6HQzkcDpWcnFzjslT2qVu3rqpbt67at2+fmjVrlpo1a5ZyOBwRfS5GhTMYQiEao0NKSoo6ceKEOnHihPJ4PCo/P1/l5+eroUOHqvT0dJWenh7wfnvfTyRGuiZNmkheAJfLJfvqhw8frpKSklRSUtI5MwP5fjp16qQOHDigDhw4oPLy8pTL5RL5Vq5cqVauXKnq1KlzTshS0ScrK0tmzUg/l4gFk0YTFQFd22azSfxYVlaWZNzJzc2NaBxYJGSpKX4JshgVzmAIATMD1TKMLLUTMwMZDBHAdCCDIQQqVeEMBkPlmBnIYAgB04EMhhAwHchgCAHTgQyGEDAdyGAIAdOBDIYQMB3IYAiBSrd0n+thFr4YWaLPL0EWMwMZDCFgOpDBEAKmAxkMIWA6kKFcdJ44nWgEzq4nZDAdyGAICdOBfuHY7XbsdjuxsbE0adKEJk2asGHDBk6dOsWpU6dYvHgxWVlZZGVlVVij55dMjexITU1NBcJXF/WXYC71JZyyPP3004CVzre8NMsFBQWSjdTlcgWUajnasnTs2BGAMWPG8Lvf/Q6wioXpTKybNm0K+tzGjG0wRICozEA2m02KuCYnJ0uR4S5dukhuaa/XG3Q16JqagRITE3nuuecASxadL3rmzJm8/PLLum0BnTOasthsNilX+eSTT3LllVfK9xq32y35y3W5l+oSCVnS0tKk6nbHjh2ldlGTJk3OKu1YlqlTp/KnP/0pkMsJFclSVSRCwC+AfonGjBkjpdKbNGkiU//hw4d59913AatGqk6NO27cOEkdVZuJiYmhd+/eACxbtswvZbEeAHSRsXMBXWRq3rx5/OpXvwKsgUHLVVhYKBWva4rmzZvTqlUrAObPny8poHX1jOpy3333Se1aUx/IYKgFhFWFi42NlYphvuUxfFFKiYpQWloqCeg3bdrEJZdcAgReEyjSao/NZpPR7pNPPpHFtsPhkJG6pKSE3bt3A9C7d2+pjRQo0VThGjduTJMmTQC49957ZQZq2rSpn89HV78uLCwM6PyhyqLbcOLECXmf3G63zPTHjh2T96eoqEiO2bVrl9QEmjFjBuedd56cU/u1dIL9UGUJa32g0tJSqf2jS9aXxVe/1sIDZGdny2/ee++9cDYraLQ6+uyzz4olR1sQwSorqB/Eli1bpN3Bdp5ooStQjBgxgm+//Rawip3p5+F0OuU51aRarQen4uJiefGdTid79uwBrHds165dcqzvZKDbHxcXJ2VE4efBINAOVBFGhTMYQiCsM5BSiqFDhwLQsGFDmfJ79eoli8Cbb75ZRort27czZMgQwBopdJXrPXv2sHHjxnA2LSgWL14MQL9+/eQ7t9stqtrKlSulSO+kSZNqtPBudbHZbJx//vkA3H///WK5OnLkiJSC9Hq9MoKfPn1aivRGG/2edO7cmQ8++ACwKulpI87x48crVPf1b32fHYRfOwh7iUdNTk6O/K2FB5g+fbr8bbPZuPrqqwHrZdTJ3Bs3blzjHSgjI4OuXbsCVjt1WfcHHnhA2uZyufjrX/8KWJW8MzMz5fjamm8vPj6e119/HbDK2WtrVFxcHBs2bAAs1Vqrr7Uh+uDgwYOi3n/wwQf06dMHqFzV1+3WazxNuMs8GhXOYAiBiM1A1UEpJVWf7Xa7TMdLly6tyWYBVonKjIwM+bf2Pbz//vsS2nLxxReLOuR0OqWob21EGwhycnJISUk56//v3buX/v37A1aV6xkzZgBWqcjaMJseOHAAgGnTplFaWgpYfqBjx44BlhVOtzM1NVW0B+1oBcu6G25ZarQDpaSkSCVv+Hl6DTYiIZzExsb6OUnr1asHIDVFwSofr02tSik6deoU9XZWl08//RTgrM6jLVSjR4+W71wuF40bNwYsU7FWh2pDR/rwww9lrTxt2jQZGL744gtuvPFGwKo+ruultmjRQn7rcrnC3h6jwhkMIVCj9YHOnDnjN8XqCt/aylVdIuV87NWrFwCzZ8+W8Byv1yvWqoSEBFlsK6UYOHAgYBlEAnUGayIlix59tQ8IrNlFO4h9F9dNmzblww8/BGDBggU8/vjjQO1zcPfr108MVEopsSjOnz9f/Dxer5fbbrsNsN63ihz8VREVR2p10c5I384DgXecSKPN0u3atRM1pnv37rJuS0pK8lNrSkpKgPBbekIlJibGz2mtmThxYrltTU5O5u233waseL9gB4NIs2LFCukQnTt3FllKSkokfm/8+PFiho+JiZFB+vvvvw9LG4wKZzCEQrQrKANq9uzZavbs2cqXe++9N+wVlCMly/Dhw9Xw4cNVQUGBtN/lcqns7GyVnZ0dkWrQwchis9mUzWZTGzdu9LvXBQUFqqCgQHXv3l1lZmaqzMxM1ahRI9W8eXPVvHlzNWPGDNWsWTPVrFkz9ZOKVeOyBCKvw+FQTqdTOZ1O1bx5c1VaWqpKS0uV1+tVS5cuVUuXLg1YroraH/Y1kFYVGjRoQG5uLuBv/WjYsCF79+4FLCvWZ599BsBVV10V6KWEaO8H6tatG2BZe3zRFh9fS12ghEsWp9PJ5MmTAXjooYf8LGnHjx8HYO3ataICXXbZZaIC2e12sdoNGDAgGDH0taL6XMqzFtrtdlasWAFAz5495ZgWLVqIabw6mB2pBkMECLsRQfd+h8NB06ZNAWtfuh7pHnroIbGWlJSUMHjw4HA3IeLoCGbf7RgAzZo1A0KbgcJFYmIit956K+AfjnPixAm2bt0KwCuvvCKxiA0bNpQZNC4ujssvv1x+Wxv8P8HicDgk/s13O8aYMWOYMmUKQEjyhb0DaSfoyZMnZYvwm2++KebSZs2aiSd51KhRnDx5MtxNiDja+Vt2C/GPP/4YletX56V2u93ygrz44osSy7d//35xMrZq1Yo//vGPgGXeXrZsGQDt27f3M3efC9jtdokc8U1WExMTw6pVqwC45JJLZJAYPXq0RFvk5+cHf91QGm0w/NKJmB/orrvu4uGHHwYsB5xWdYqLi+nRoweARP+ea+iZp2ykso7GjrQ/qzoqR1FREcuXLweskJdbbrkFsOLHdPzea6+9JtqAy+USo0ibNm3EWTxo0CBxqtZG9DM4//zzZQfANddcIwlQ/vKXv0gSG99cFVlZWTzyyCMAzJo1SzbpVXT+igh7B9IXnDhxosSJ2e12eejdunXj66+/Dvdlo0pFSUN27NgR5ZZUjNfrlYQhDz30ELNnzwYsq5oOwNSdB6w1k9767Kuazps3T9TvgoKCqLS9uthsNmlrTk6OZA/q27evvHter1fWpgUFBcTHxwOWBfiOO+4AYNGiRXKvXC6X32+rGqyMCmcwhEDYZyCtqvkmJYefF9jaAnQuU16S9dLSUk6fPl0DrakapZRYol577bVyjzl58iRjx44FrB3BOsYvJiaGQYMGAdYGttoQKR8XFwdYFra2bdsC0KFDB/F7nTp1ilGjRgHWjlqtWnu9XjGOuN1umV2uuOIKyVrqcDgC2oEb9g7km3TDF53Q7lw2iWrKU0FLS0v9zPPnGkopsdT5qna+a6Pa0HliYmIYNmwYYOV50+p0RkaGLB98rZRJSUmiesbHx8sgV1RUxJw5cwAr3k/v9yopKQnoHTUqnMEQAmGfgfSUCj/PNmfOnJHQkP8EtJPRl9jYWFEV9IL0XEPPQM8//zw33XQTYKX8rQnHsMPh8IsU1++S1+sVY0fr1q1lyeCbb1AfB5bfa8GCBQA888wzkrfQ7XbLMU6n02/WDYSIOVJ37dolevSCBQuC3odRG9FbzvPy8khMTASsrd5aDTjXmTRpEmvXrgUQU3i0qWgLhcfjkf1Je/bskXXPgQMHxCk/efJkMUtXR+0MtvOAUeEMhpAIezS2XtQlJSVJNHZKSoo4uSKx2aymon7r1q1LdnY2YFUu0CNZKIaSaMtSEVp7qI4vpCIiLYsuDgb+7YyEoaoiWaKypdvpdEbUglNbXrpwUB1Z7Ha7Ohesmb+E52JUOIMhBCqdgQwGQ+WYGchgCAHTgQyGEDAdyGAIAdOBDIYQMB3IYAgB04EMhhD4f+JO7tOBrd6NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 2500 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After 3000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images(list_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the generator after the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "images = generator.predict(noise_input)\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
