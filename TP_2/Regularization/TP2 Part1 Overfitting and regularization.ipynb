{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 Part1: Over-fitting and regularization\n",
    "Date : 04/11  \n",
    "Duration: 1h30  \n",
    "@Carl Rlgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Over-fitting: how to deals with it ?](#Over-fitting:-how-to-deal-with-it-?)\n",
    "    1. [Some ideas](#Some-ideas)\n",
    "    1. [Example: Ordinary Least Squares](#Example:-Ordinary-Least-Squares)\n",
    "1. [Regularization, Penalization with Linear Models](#Regularization,-Penalization-with-Linear-Models)\n",
    "    1. [Ridge Regression - l2 penalization](#Ridge-Regression---l2-penalization)\n",
    "    1. [Lasso Regression - l1 penalization](#Lasso-Regression---l1-penalization)\n",
    "    1. [Elastic-net Regression - l1/l2 penalization](#Elastic-net-Regression---l1/l2-penalization)\n",
    "1. [Bootstrapping, Bagging, Ensemble](#Bootstrapping,-Bagging,-Ensemble)\n",
    "1. [Biblio](#Biblio)\n",
    "\n",
    "\n",
    "## Over-fitting: how to deal with it ?\n",
    "\n",
    "Overfitting may occur when a statistical model describes random errors or noise instead of the underlying variable relationships. An overfitted model will generally have poor predictive performance as it may exaggerate minor fluctuations in the data and misinterpret others. The overfitting phenomenon has three main explanations:\n",
    "- excessively complex model\n",
    "- multicollinearity\n",
    "- high dimensionality\n",
    "\n",
    "A learning algorithm is calibrated using a set of training samples. When the algorithm overfits on the training set, the performance on training samples will improve while the performance on test sample set will decline. \n",
    "<img src=\"img/overfitting.png\" alt=\"Overfitting illustration\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "### Some ideas\n",
    "#### First idea: penalization\n",
    "\n",
    "Regularization is a method that applies to objective functions (loss function here) in ill-posed optimization problems. The aim is to constrain the optimization in order to avoid overfitting.  \n",
    "To do so, we use  a penalty technique. It adds a term, called a penalty function, to the objective function consisting in a penalty parameter multiplied by a measure of violation of the constraints, often a norm (for example for euclidean norm $\\mathcal{l}_2$-norm or Euclidean norm  for Ridge).  \n",
    "\n",
    "A way to estimate this hyper parameter is to split our data in three part : a training set, testing set and validation set. The first is used to calibrate a model, the second to estimate penalty parameters and the last to evaluate prediction performance of the thus built model.  \n",
    "\n",
    "#### Second idea: better sampling\n",
    "\n",
    "Sample methods can be used to train both model's parameters and hyper-parameters (from regularization for example). There are plenty of methods to avoid over-fitting by sampling; here are detailed cross validation, bootstrapping, bagging (and ensemble which is not really a sample method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Ordinary Least Squares\n",
    "Linear regression models the output (or target) variable y $\\in R$ as a linear combination of the P-dimensional input x $\\in R^P$.  \n",
    "Let $y$ be the N-dimensional vector of outputs in the training set and $X$ be the $N\\times(P+1)$ $X$ matrix with each row an input vector (with a 1 in the first position). The linear regression will predict $y$ vector given $X$ matrix\n",
    "using the parameter vector (or weight) $\\beta \\in R^{P+1}$ like following:  \n",
    "$$\\begin{aligned}\n",
    "y &= f(\\beta) + \\varepsilon\\\\\n",
    " &= X\\beta + \\varepsilon\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\varepsilon \\in  R^N$ are the residuals (or errors) of the prediction. The $\\beta$ parameters minimize the loss function $L(\\beta)$ i.e. the error measured on the data. This error is the Mean Squared Errors (MSE).  \n",
    "$$\\begin{aligned}\n",
    "\\text{OLS}(\\beta) &= L(\\beta) \\\\\n",
    "&= ||y - X\\beta||^2_2 \\\\\n",
    "&= \\sum_{i=1}^N (y_i - x_i^T \\beta)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Quick question: recall the estimator formula of a standard OLS:_  \n",
    "_Answer:_  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def load_extended(dataset):\n",
    "    X = dataset.data\n",
    "    X = MinMaxScaler().fit_transform(dataset.data)\n",
    "    X = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
    "    return X, dataset.target\n",
    "\n",
    "dataset = load_boston()\n",
    "X, y = load_extended(dataset)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Scatter plot labels according to input indexed 12 from training set:_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Print input and label shapes:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How many features $P$ for this model ? How many obervations $N$?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Import LinearRegression model, fit it to training data, then compute its prediction accuracy both on trainind and testing set:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = r2_score(y_train, lr_train)\n",
    "r2_test = r2_score(y_test, lr_test)\n",
    "\n",
    "print('Training set R2 score: %.3f' % r2_train)\n",
    "print('Testing set R2 score:  %.3f' % r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Print the coefficients and the intercept of the regression:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization, Penalization with Linear Models\n",
    "To avoid overfitting, which create complex weight vectors due to noise or spurious correlations within predictors, adding a penalty to the learning reduce bias ie the capacity of the learning algorithm. The basis of regression regularization methods such as LASSO and ridge regression comes from bias-variance decomposition, but won't be detailed here.  \n",
    "Thus, usual loss function $L(\\beta)$ is combined with the penalty function $\\omega(\\beta)$ which gives the new objective function:   \n",
    "$$\\text{Penalized}(\\beta) = L(\\beta) + \\lambda \\omega(\\beta)$$\n",
    "\n",
    "### Ridge Regression - l2 penalization\n",
    "For Ridge regression the penalty function $\\omega(\\beta)$ is the euclidean norm of the coefficients, as following:    \n",
    "$$\\text{Ridge}(\\beta) = ||y - X\\beta||^2_2 + \\lambda ||\\beta||^2_2$$\n",
    "Increasing $\\lambda$ forces coefficients to move towards zero, which decreases training set performance, but might help generalization. The more $\\lambda$ is close to zero, the more the model resembles linear regression.\n",
    "\n",
    "Notice that $\\lambda$ is alpha parameter in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Import Ridge from sklearn.linear_model and fit to train sample:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set R2 score: %.3f' % ridge.score(X_train, y_train))\n",
    "print('Testing set R2 score:  %.3f' % ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Fit a Ridge with $\\alpha$ parameter at 10. Then, print ridge score on training and testing sets:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What do you note ?_  \n",
    "_Answer:_  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "***\n",
    "_Fit a Ridge with $\\alpha$=0.1. Then, print ridge score on training and testing sets:_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset.feature_names) # Number of variables to show\n",
    "plt.title('Ridge coefficients')\n",
    "plt.plot(ridge.coef_[:N], 'o', label='Ridge alpha=1')\n",
    "plt.plot(ridge10.coef_[:N], 'o', label='Ridge alpha=10')\n",
    "plt.plot(ridge01.coef_[:N], 'o', label='Ridge alpha=0.1')\n",
    "plt.plot(lr.coef_[:N], 'o', label='Linear Regression')\n",
    "plt.xticks(range(len(dataset.feature_names)), dataset.feature_names, rotation=60) \n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How can you interprete the above graph ?_  \n",
    "_Answer:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Lasso Regression - l1 penalization\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator (popularized in 1996) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability.  \n",
    "Lasso regression uses the $\\mathcal{l}_1$-norm as penalty function. This constraint will reduce bias of the learning algorithm.  Adding this constraint forces the coefficients to be small by shrinking them toward zero. This penalty forces some coefficients to be exactly zero, providing a feature selection property. The objective function to minimize becomes:\n",
    "$$Lasso(\\beta) = ||y - X\\beta||^2_2 + \\lambda ||\\beta||_1$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Import Lasso from sklearn.linear_model and fit to train sample. Then, print LASSO scores on train and test sets, as well as the number of features used:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What can you say?_  \n",
    "_Answer:_  \n",
    ".  \n",
    ".  \n",
    ".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LASSO with $\\alpha=0.01$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso01 = Lasso(alpha=0.01).fit(X_train, y_train)\n",
    "print('Training set R2 score: %.3f' % lasso01.score(X_train, y_train))\n",
    "print('Testing set R2 score:  %.3f' % lasso01.score(X_test, y_test))\n",
    "print('Number of features used: %d' % sum(lasso01.coef_ != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lasso.coef_[:N], 'o', label='Lasso alpha=1')\n",
    "plt.plot(lasso01.coef_[:N], 'o', label='Lasso alpha=0.01')\n",
    "plt.plot(ridge01.coef_[:N], 'o', label='Ridge alpha=0.1')\n",
    "plt.xticks(range(len(dataset.feature_names)), dataset.feature_names, rotation=60) \n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How can you interprete the above graph ?_  \n",
    "_Answer:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Elastic-net Regression - l1/l2 penalization\n",
    "The Elastic-net model combines both $\\mathcal{l}_1$ and $\\mathcal{l}_2$ penalties with $\\alpha$ as the global penalty and $p$ as the l1/l2 ratio. The loss function is :\n",
    "$$Enet(\\beta) = ||y - X\\beta||^2_2 + \\alpha (p||\\beta||_1 + (1-p)||\\beta||^2_2)$$\n",
    "\n",
    "<img src=\"img/lasso.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "_Import ElasticNet from sklearn.linear_model and fit to train sample. Then, print its scores on train and test sets:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with $\\alpha=0.1$ and l1_ratio $=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Compare these scores with usual linear regression._  \n",
    "_Answer:_  \n",
    ".  \n",
    ".  \n",
    ".  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question arises: how to determine optimaly hyper-parameters like $\\alpha$ or $\\mathcal{l}_1$ ratio ?\n",
    "***\n",
    "# Sample Methods\n",
    "\n",
    "Sample methods can used in order to avoid overfitting. Here are presented several methods, including cross validation which can be used both in overfitting and regularizaion.\n",
    "\n",
    "### Cross validation.\n",
    "It is a resampling procedure used to evaluate quality of models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. For example when k is 10 we call it 10-fold cross-validation.  \n",
    "The general procedure is as follows:  \n",
    "- Shuffle the data set randomly\n",
    "- Split the data set into k groups\n",
    "- For each group:  \n",
    "    - Take the group as test data set  \n",
    "    - Take the remaining groups as a training data set  \n",
    "    - Fit a model on the training set and evaluate it on the test set  \n",
    "    - Retain the evaluation score and discard the model\n",
    "- Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "<img src=\"img/crossvalidation.png\" alt=\"Cross validation scheme\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_We will use GridSearchCV and specify three parameters:\n",
    "the model considered, the hyper-parameters and the number of folds. GridSearchCV proceed the cross validation on all the hyper-parameters and return the best one._  \n",
    "\n",
    "_Import GridSearchCV from sklearn.model_selection._  \n",
    "_Call a Ridge model._  \n",
    "_Construct a dictionnary named alphas including several alpha values to be tested._  \n",
    "_Initialize a 5-folds cross validation named ridgeCV, with ridge model, alphas and cv attributes._\n",
    "_Eventually, fit ridgeCV on training sample._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = {'alpha': [1e-10, 1e-6, 1e-3, 0.01, 0.1, 1, 5, 10, 20]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Print best_params_ and best_score_ of ridgeCV:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our little set of alphas, 0.1 gives the best score. But is it robust?  \n",
    "Bellow you can find a little piece of code shearching the best alpha among 50 possiblilities via 5-foldes cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create an array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-3, 1, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=5)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.plot(alpha_space, ridge_scores)\n",
    "std_error = ridge_scores_std / np.sqrt(10)\n",
    "plt.fill_between(alpha_space, ridge_scores + std_error, ridge_scores - std_error, alpha=0.2)\n",
    "plt.ylabel('CV Score +/- Std Error')\n",
    "plt.xlabel('Alpha')\n",
    "plt.axhline(np.max(ridge_scores), linestyle='--', color='.5')\n",
    "plt.xlim([alpha_space[0], alpha_space[-1]])\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping, Bagging, Ensemble\n",
    "\n",
    "__Ensemble__ relies on the idea that we can lower variance by using the wisdom of the crowd, for example averaging the score across 10 or 20 predictors, as opposed to relying on only one. This has shown to improve accuracy and so its recommended that you explore ensemble techniques. One of the most popular ones is Random Forests, which rely on a specified amount of Decision Trees to create predictions.  \n",
    "\n",
    "__Boosting__ refers to any Ensemble method that can combine several weak learners into a strong learner and is used to reduce bias and variance. It does this through a weighted majority vote (classification) or a weighted sum (regression). Ada boost and Gradient boost are two popular methods.  \n",
    "\n",
    "__Bootstrapping__ is a sampling technique with replacement. This ends up leaving some data unselected (on average 63% are sampled), while the remaining 37% of the training instances that are not sampled are called out-of-bag instances. Since the predictor never sees the out of bag instances during training, it can be evaluated on these instances without the need for a separate validation set or cross validation.  \n",
    "\n",
    "<img src=\"img/bagging.png\" alt=\"Bagging and bootstrapping schemes\" style=\"width: 600px;\"/>\n",
    "\n",
    "__Bagging__ otherwise known as bootstrap aggregating, is used to reduce variance which helps avoid overfitting. The idea is that once you have your sample from bootstrapping, you can then build a series of models. This ensemble of models, will carry votes with equal weight and you’re able to use that average.  \n",
    "\n",
    "\n",
    "<img src=\"img/bagging2.png\" alt=\"Bagging and bootstrapping tables\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_diabetes().data\n",
    "y = load_diabetes().target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print('R2 score without bagging: ')\n",
    "print('Training set: %.3f' % lr.score(X_train, y_train))\n",
    "print('Testing set:  %.3f' % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Implement a bagging algorithm for K (to be chosen) OLS models._  \n",
    "_Then, compare bagging results with the OLS model:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblio\n",
    "\n",
    "• \"Notes on Regularized Least Squares\", Rifkin & Lippert  \n",
    "• \"Generalized Linear Models\", Scikit-learn, Section 1.1; learn.org/stable/modules/linear_model.html  \n",
    "• \"Regularization Path For Generalized linear Models by Coordinate Descent,\" Friedman, Hastie & Tibshirani, J Stat Softw, 2010.  \n",
    "• \"An Interior-Point Method for Large-Scale L1-Regularized Least Squares\", S.J.Kim, K.Koh, M.Lustig, S.Boyd and D.Gorinevsky, in IEEE Journal of Selected Topics in Signal Processing, 2007.  \n",
    "• \"Bagging predictors\", Breiman, Leo (1996) Machine Learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
